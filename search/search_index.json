{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":""},{"location":"#turn-ai-complexity-into-business-advantage","title":"Turn AI Complexity Into Business Advantage","text":""},{"location":"#independent-ai-consultant-google-developer-expert-helping-businesses-build-high-impact-ai-solutions","title":"Independent AI Consultant &amp; Google Developer Expert helping businesses build high-impact AI solutions","text":"<p>With over a decade of experience in machine learning systems and a proven track record of transforming ideas into measurable results.</p> <p> Book a Consultation  Explore My Services</p>"},{"location":"#im-muhammad-your-ai-expert","title":"\ud83d\udc4b I'm Muhammad \u2014 Your AI Expert","text":"<p>I'm an independent consultant, advisor, and Google Developer Expert with over a decade of hands-on experience in building and scaling machine learning systems. I partner with businesses like yours to turn complex AI challenges into clear, actionable strategies that deliver tangible results\u2014boosting efficiency, unlocking new opportunities, and driving growth.</p> <p>Whether you're navigating the ever-evolving landscape of Generative AI or trying to unlock the full potential of LLMs, I can guide you from ideation to execution\u2014with a focus on measurable results.</p> <p>Before diving into consulting, I earned a Ph.D. in Applied Machine Learning, specializing in wearable sensors and biomedical signal processing. You can browse my academic research and publications on Google Scholar.</p> <p>I'm also deeply committed to open-source and community-driven learning. I created localGPT, one of the first open-source Retrieval-Augmented Generation (RAG) projects for secure, private document interaction\u2014now with over 20k GitHub \u2b50.</p> <p>On my YouTube channel with 196k+ subscribers, I regularly share practical insights on LLMs, Generative AI, Agents, and more. Join the conversation on Discord, where 4,500+ members exchange ideas, ask questions, and build cool stuff together.</p> <p>Ready to bring AI into your business in a meaningful way?</p> <p> Book a Consultation</p>"},{"location":"#consulting","title":"Consulting","text":"<p>I help startups and enterprise teams build high-impact machine learning systems, adapt quickly to emerging AI tools, and implement strategies that drive tangible business outcomes.</p> <p>Here's how I can empower your organization:</p>"},{"location":"#strategic-consulting","title":"Strategic Consulting","text":"<p>Navigate the AI landscape confidently. We'll co-create a custom roadmap ensuring your AI investments align directly with strategic goals, whether it's market leadership, product innovation, or operational excellence. Ideal for: Product strategy, R&amp;D, AI integration Timeline: 4\u20138 weeks</p>"},{"location":"#process-optimization","title":"Process Optimization","text":"<p>Identify bottlenecks and unlock significant cost savings. I'll help you implement targeted AI solutions to automate key workflows and dramatically boost productivity. Ideal for: Ops, customer support, internal tools Timeline: 2\u20134 weeks</p>"},{"location":"#ai-team-training","title":"AI Team Training","text":"<p>Transform your team's capabilities with practical, hands-on AI skills. We'll cover prompt engineering, RAG, LLM integration, and more\u2014tailored to your stack\u2014so your team can build and deploy with confidence. Ideal for: Technical teams and engineers Timeline: 1\u20135 days</p>"},{"location":"#leadership-development","title":"Leadership Development","text":"<p>Equip your leaders to steer your organization through the AI revolution. Through 1:1 coaching and group sessions, we'll build the strategic mindset needed to foster an AI-first culture and gain a competitive edge. Ideal for: Founders, CTOs, product leaders Timeline: Custom</p> <p>Every engagement starts with a discovery phase where I dig into your systems, goals, and blockers\u2014so we can design a high-impact, personalized solution.</p>"},{"location":"#new-ai-team-training-programs","title":"New: AI Team Training Programs","text":"<p>Want to build lasting AI expertise within your organization?</p> <p>My custom training programs equip your team with practical skills to:</p> <ul> <li>Master prompt engineering: Get consistent, high-quality results from LLMs.</li> <li>Implement advanced RAG: Build powerful, context-aware AI applications.</li> <li>Integrate AI for developers: Seamlessly embed AI into your existing products and pipelines.</li> <li>Champion AI ethics &amp; governance: Build responsible, trustworthy AI systems.</li> </ul> <p>Each session is customized based on your team's technical background and the specific challenges your business is facing, ensuring immediate applicability and impact.</p> <p> Learn More About My Services</p>"},{"location":"#rag-beyond-basics-course","title":"RAG Beyond Basics Course","text":"<p>Learn how to build powerful chat-with-documents applications using advanced Retrieval-Augmented Generation (RAG) techniques and the latest LLMs. This comprehensive course takes you from the basics to advanced implementation of RAG systems. Explore the Course.</p>"},{"location":"#featured-youtube-videos","title":"Featured YouTube Videos","text":"<p>I create educational and informative videos on my YouTube channel. Here are some of my most popular videos:</p> <p> See All Videos</p>"},{"location":"#connect-with-me","title":"Connect With Me","text":"<ul> <li> <p> Book a Consultation</p> <p>Interested in working together? Schedule a free discovery call to discuss your needs and how I can help you achieve your goals.</p> <p> Book a Consultation</p> </li> <li> <p> Follow Me</p> <p>Stay connected and get the latest updates by following me on  YouTube,  Twitter, and  LinkedIn.</p> </li> </ul>"},{"location":"404/","title":"404 - Page Not Found","text":"<p>Sorry, the page you were looking for doesn't exist.</p>"},{"location":"404/#what-might-have-happened","title":"What might have happened?","text":"<ul> <li>The page might have been moved or deleted</li> <li>You might have followed a broken link</li> <li>There might be a typo in the URL</li> </ul>"},{"location":"404/#what-can-you-do-now","title":"What can you do now?","text":"<ul> <li>Go back to the homepage</li> <li>Use the search function at the top of the page</li> <li>Check out some of my popular content:</li> <li>RAG Beyond Basics Course</li> <li>Strategic AI Consulting Services</li> <li>Expert AI Video Library</li> </ul> <p>If you believe this is an error, please contact me via email or reach out on X. </p>"},{"location":"book-a-call/","title":"Let's Turn Your AI Ambitions Into Real-World Results","text":"<p>Struggling with AI implementation, scaling machine learning systems, or making sense of where to begin? With over a decade of experience helping startups and enterprises alike, I'll help you cut through the noise and create a clear, strategic path forward.</p>"},{"location":"book-a-call/#what-youll-get-from-our-45-minute-consultation","title":"What You'll Get From Our 45-Minute Consultation","text":"<p>This isn't a sales call. It's a focused session designed to unlock clarity, fast.</p> <ol> <li>Current State Assessment \u2014 Understand where you are and what's getting in your way</li> <li>Opportunity Discovery \u2014 Identify the most impactful areas where AI can move the needle</li> <li>Custom Roadmap \u2014 Walk away with a tailored game plan and clear next steps</li> <li>Expert Insights \u2014 Get candid answers to your questions about strategy, tools, or implementation</li> </ol>"},{"location":"book-a-call/#book-instantly-no-back-and-forth","title":"Book Instantly. No Back-and-Forth.","text":"<p> Schedule Your Call \u2192</p>"},{"location":"book-a-call/#make-the-most-of-our-time-together","title":"Make the Most of Our Time Together","text":"<p>To ensure we hit the ground running:</p> <ul> <li>Come with a brief summary of your current challenges or goals</li> <li>Have 1\u20133 key questions ready that you'd like clarity on</li> <li>Think about what success would look like in the next 3\u20136 months</li> <li>Optional: Share any relevant docs or links that help explain your context</li> </ul>"},{"location":"book-a-call/#what-happens-after-our-call","title":"What Happens After Our Call","text":"<p>You'll receive:</p> <ul> <li>A summary of key insights from our discussion</li> <li>Tailored recommendations to guide your next steps</li> <li>A prioritized action plan to help you build momentum quickly</li> <li>Any relevant tools or resources that match your situation</li> </ul> <p>Whether or not we work together long-term, you'll leave the call with more clarity, direction, and a practical plan to move forward.</p> <p>\"Muhammad's guidance helped us implement an AI solution that reduced our processing time by 78% and dramatically improved customer satisfaction. His practical approach made all the difference.\" \u2014 Previous Client</p>"},{"location":"book-a-call/#ready-to-move-forward","title":"Ready To Move Forward?","text":"<p> Book Your Call Now</p>"},{"location":"rag-beyond-basics/","title":"RAG Beyond Basics: Master AI-Powered Document Intelligence","text":""},{"location":"rag-beyond-basics/#transform-how-you-extract-value-from-documents-build-professional-grade-rag-systems-that-deliver-precise-contextual-answers-at-scale","title":"Transform How You Extract Value From Documents: Build Professional-Grade RAG Systems That Deliver Precise, Contextual Answers At Scale","text":"<p>\u2b50 Master Both The \"How\" AND The \"Why\" Behind Advanced RAG Systems Through Hands-On Implementation \u2b50</p>"},{"location":"rag-beyond-basics/#why-this-course-is-a-game-changer","title":"\ud83d\udc40 Why This Course Is A Game-Changer","text":"<p>Retrieval-Augmented Generation (RAG) is revolutionizing how businesses interact with their document repositories. Instead of spending hours searching through files, imagine instantly extracting precise insights through natural conversation\u2014unlocking document intelligence that drives real business decisions. \ud83d\udcc8\ud83d\udcda</p> <p>This comprehensive course bridges theory with hands-on implementation, taking you beyond basic RAG tutorials into professional-grade system architecture. You'll master both commercial APIs and fully private, on-premise solutions, giving you the flexibility to build systems that meet any security or customization requirement.</p> <p> Enroll Now</p>"},{"location":"rag-beyond-basics/#who-will-get-transformative-value","title":"\ud83d\udc69\u200d\ud83d\udcbb Who Will Get Transformative Value","text":"<ul> <li>\u2705 SaaS Founders &amp; Product Leaders: Transform document-heavy workflows into competitive advantages and create AI features users will pay premium prices for</li> <li>\u2705 ML &amp; AI Engineers: Skip months of trial-and-error by implementing battle-tested RAG architectures that overcome common challenges</li> <li>\u2705 Technical Leaders &amp; Architects: Make informed decisions about AI infrastructure integration and optimize for performance, cost, and security</li> <li>\u2705 Enterprise Innovation Teams: Build proof-of-concepts that demonstrate immediate business value from your untapped document repositories</li> </ul> <p>The only prerequisite is basic Python knowledge\u2014everything else is covered step-by-step. Familiarity with LangChain or Streamlit is helpful but not essential.</p>"},{"location":"rag-beyond-basics/#the-complete-rag-system-architecture-well-build-together","title":"\ud83d\udee0 The Complete RAG System Architecture We'll Build Together","text":"<p>We'll progress systematically from fundamental concepts to advanced implementation, constructing a production-ready RAG system that handles real-world document complexity with impressive accuracy.</p> <p>Here's a structured breakdown:</p>"},{"location":"rag-beyond-basics/#lesson-1-getting-started-with-rag","title":"Lesson 1: Getting Started with RAG","text":"<ul> <li>Introduction: What is RAG?</li> <li>Setup your virtual environment &amp; API keys.</li> <li>Building blocks of basic RAG applications.</li> </ul>"},{"location":"rag-beyond-basics/#lesson-2-building-your-first-rag-pipeline-code-time","title":"Lesson 2: Building Your First RAG Pipeline (Code Time!)","text":"<ul> <li>Hands-on coding your initial RAG pipeline.</li> <li>Chatting seamlessly with PDF documents.</li> </ul>"},{"location":"rag-beyond-basics/#lesson-3-advanced-query-techniques-re-ranking","title":"Lesson 3: Advanced Query Techniques &amp; Re-ranking","text":"<ul> <li>Supercharge retrieval accuracy with Query Expansion.</li> <li>Refine results precisely using re-ranking techniques (GPT-4, ColBERT, Cohere).</li> </ul>"},{"location":"rag-beyond-basics/#lesson-4-hypothetical-document-embeddings-retrieval-ensembles","title":"Lesson 4: Hypothetical Document Embeddings &amp; Retrieval Ensembles","text":"<ul> <li>Generate targeted \"Hypothetical Documents\" to enrich your retrieval.</li> <li>Leverage ensemble retrieval methods (combining semantic &amp; keyword-based searches).</li> </ul>"},{"location":"rag-beyond-basics/#lesson-5-hierarchical-chunking-parent-document-retrieval","title":"Lesson 5: Hierarchical Chunking &amp; Parent Document Retrieval","text":"<ul> <li>Implement Hierarchical Chunking to extract richer contexts.</li> <li>Explore advanced parent-document retrieval methods for accurate answers.</li> </ul>"},{"location":"rag-beyond-basics/#lesson-6-creating-interactive-gui-applications","title":"Lesson 6: Creating Interactive GUI Applications","text":"<ul> <li>Build a beautiful, intuitive frontend with Streamlit.</li> <li>Deploy a fully working GUI RAG application you can proudly showcase.</li> </ul>"},{"location":"rag-beyond-basics/#learning-architecture-theory-implementation-application","title":"\ud83d\udcda Learning Architecture: Theory + Implementation + Application","text":"<p>Each module follows a proven learning pattern that maximizes both understanding and practical skill development:</p> <ol> <li>Conceptual Foundation: Clear explanation of why specific techniques matter and how they solve real problems</li> <li>Implementation Deep-Dive: Hands-on coding with step-by-step guidance and complete access to source code</li> <li>Real-World Application: Apply what you've learned to actual document sets and see immediate results</li> </ol> <p>By the final module, you'll have built a production-ready RAG system that you fully understand and can confidently customize for any domain-specific challenge.</p>"},{"location":"rag-beyond-basics/#course-curriculum-overview","title":"\ud83d\udcd6 Course Curriculum Overview","text":"<ul> <li>What is RAG?</li> <li>Setup \u2013 Virtual Environment and API Keys</li> <li>Building Blocks of RAG Applications</li> <li>RAG Pipeline Implementation</li> <li>Query Expansion Techniques</li> <li>Re-ranking with Multiple Models</li> <li>Hypothetical Document Embeddings</li> <li>Ensemble Retrieval Methods</li> <li>Hierarchical Chunking Strategies</li> <li>Parent Document Retrieval</li> <li>Creating Interactive GUI Applications</li> <li>Deployment Options</li> </ul> <p> Get Access Now </p>"},{"location":"services/","title":"Strategic AI Consulting Services","text":""},{"location":"services/#unlock-the-power-of-ai-for-your-business","title":"Unlock the Power of AI for Your Business","text":"<p>Whether you're just starting to explore AI or looking to scale advanced capabilities, I offer customized solutions that align with your goals and deliver real business impact.</p> <ul> <li>Strategic AI Roadmapping \u2013 We'll collaboratively build clear implementation plans, ensuring your AI investments directly fuel your business objectives and drive measurable growth.</li> <li>Process Optimization \u2013 Pinpoint and eliminate inefficiencies using targeted AI solutions, significantly boosting productivity and reducing operational costs.</li> <li>AI Team Training &amp; Development \u2013 Elevate your internal AI capability through hands-on workshops, empowering your team to build, deploy, and manage AI solutions effectively.</li> <li>Leadership &amp; Strategy Coaching \u2013 Equip your executives with the strategic insights and AI fluency needed to lead confidently and gain a competitive advantage in the age of AI.</li> <li>Technical Implementation Guidance \u2013 Navigate complex AI rollouts successfully with expert, step-by-step guidance and hands-on support for your technical teams.</li> <li>Ongoing Innovation Support \u2013 Stay ahead of the curve. My ongoing advisory services help you scale solutions, iterate effectively, and adapt to the rapidly evolving AI landscape.</li> </ul> <p>Schedule a Consultation</p>"},{"location":"services/#how-i-work-a-simple-proven-process","title":"How I Work: A Simple, Proven Process","text":"<p>I follow a clear, outcome-focused process that keeps your goals front and center\u2014from first conversation to long-term results.</p> <ol> <li> <p>Understand Your Landscape We start with a deep dive into your tech stack, workflows, and goals\u2014so we can tailor everything to your specific needs.</p> </li> <li> <p>Find the Right Opportunities I help you identify the AI use cases that offer the biggest wins and flag any roadblocks early on.</p> </li> <li> <p>Design a Smart Solution We co-create a practical, scalable plan using the best-fit tools and techniques\u2014no fluff.</p> </li> <li> <p>Build and Deliver With hands-on support and knowledge sharing, we move from prototype to production smoothly and confidently.</p> </li> <li> <p>Refine and Scale Once live, we iterate, optimize, and expand\u2014so your solution continues to grow with your business.</p> </li> </ol>"},{"location":"services/#practical-ai-training-that-sticks","title":"Practical AI Training That Sticks","text":"<p>Empower your teams with targeted AI skills through training programs customized for different roles and technical levels:</p> <ul> <li>Executive AI Literacy \u2013 Empower leaders to make strategic AI decisions that drive competitive advantage and foster an AI-first culture.</li> <li>Prompt Engineering Mastery \u2013 Teach teams to craft effective prompts, enabling them to harness the full power of LLMs for consistent, high-quality outputs.</li> <li>RAG Implementation Workshop \u2013 Equip developers with the practical skills to build, fine-tune, and deploy robust RAG systems for enhanced information retrieval.</li> <li>AI Integration for Developers \u2013 Guide your developers in seamlessly embedding AI capabilities into existing products and workflows, creating smarter applications.</li> <li>AI Ethics &amp; Governance \u2013 Build trust and mitigate risk by implementing frameworks for responsible, fair, and transparent AI systems.</li> </ul> <p>Workshops range from focused half-day intensives to comprehensive multi-week programs with projects, mentoring, and real use cases, ensuring skills are not just learned, but applied.</p>"},{"location":"services/#learn-rag-end-to-end-rag-beyond-basics-course","title":"Learn RAG, End-to-End: 'RAG Beyond Basics' Course","text":"<p>If you're serious about building real-world AI solutions, my flagship course, RAG Beyond Basics, is designed to take you from foundational concepts to full-scale, production-ready applications.</p> <p>This course is ideal for:</p> <ul> <li> <p>Engineers who want to master LLM-based search and retrieval systems</p> </li> <li> <p>Product builders looking to integrate RAG into their SaaS tools</p> </li> <li> <p>Innovation teams aiming to unlock value from internal document repositories</p> </li> </ul> <p>You'll go far beyond simple demos\u2014learning query expansion, re-ranking, ensemble retrieval, chunking strategies, and how to ship a polished, interactive app.</p> <p>Explore the Course \u2192</p>"},{"location":"services/#engagement-models-that-fit-your-needs","title":"Engagement Models That Fit Your Needs","text":"<p>Every organization is different\u2014so I offer flexible ways to work together depending on your goals, timeline, and internal capabilities.</p> <ul> <li>Strategic Assessments provide a focused deep dive into your systems, identifying key AI opportunities and delivering a clear, actionable roadmap.</li> <li>Implementation Support offers hands-on execution, helping your team bring complex AI solutions to life with expert guidance.</li> <li>Advisory Retainers keep me on-call as your trusted AI partner, providing monthly strategic input and fast answers when you need them.</li> <li>AI Team Training ranges from quick workshops to multi-day deep dives, tailored to your team's skill level and real-world needs.</li> <li>Leadership Development is available for executives and teams seeking to sharpen their AI thinking and build long-term capability.</li> <li>Strategy Intensives are designed for fast-moving teams that need clarity, prioritization, and direction in just a day or two.</li> </ul>"},{"location":"services/#what-about-pricing","title":"What About Pricing?","text":"<p>Every project is different\u2014so I offer flexible pricing tailored to your goals, team size, and project complexity.</p> <p>Whether you're looking for a quick strategic boost, hands-on implementation, or ongoing advisory support, we can find a model that works for your budget.</p> <p>Let's start with a discovery call and build a plan that fits.</p> <p> Contact Me |  Schedule a Consultation</p>"},{"location":"youtube/","title":"Expert AI Video Library","text":"<p>Access my curated collection of in-depth technical tutorials, strategic insights, and practical implementation guides that have helped over 193,000 professionals master AI, LLMs, and Generative AI. Each video distills complex concepts into actionable knowledge you can apply immediately.</p> <p> Join 193K+ Subscribers</p>"},{"location":"youtube/#retrieval-augmented-generation-tutorials","title":"Retrieval Augmented Generation Tutorials","text":"RAG Local LightRAG: A GraphRAG Alternative but Fully Local with Ollama <p>In this video, we explore how to set up and run LightRAG\u2014a retrieval augmented generation (RAG) system that combines knowledge graphs with embedding-based retrieval\u2014locally using OLLAMA. .</p> Agents RAG with reasoning models <p>This video explores how reasoning models can help your RAG pipelines. Reasoning model can help with reranking of the returned context without the need for external reranker models. </p> Agents Graph RAG: Improving RAG with Knowledge Graphs <p>Discover Microsoft\u2019s groundbreaking GraphRAG, an open-source system combining knowledge graphs with Retrieval Augmented Generation to improve query-focused summarization. I\u2019ll guide you through setting it up on your local machine, demonstrate its functions, and evaluate its cost implications. </p> LLM Training Multimodal RAG with Vision Models <p>Learn how to create an end to end multimodal RAG pipeline without the need of parsing your docs, chunking, embedding etc. This technique enables you to feed image of your docs (PDF pages) and create a multi-vector represetnation with techniques like ColPali and direclty feed your returned pages in to a Vision Language Model.</p>"},{"location":"examples/youtube-embed/","title":"YouTube Video Embedding Examples","text":""},{"location":"examples/youtube-embed/#method-1-using-html-iframe","title":"Method 1: Using HTML iframe","text":"<p>You can embed a YouTube video directly using an HTML iframe. This is the most common method:</p> <pre><code>&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VIDEO_ID\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre> <p>Replace <code>VIDEO_ID</code> with your actual YouTube video ID. For example, if your YouTube URL is <code>https://www.youtube.com/watch?v=dQw4w9WgXcQ</code>, the video ID is <code>dQw4w9WgXcQ</code>.</p> <p>Here's a live example:</p>"},{"location":"examples/youtube-embed/#method-2-using-material-for-mkdocs-admonitions","title":"Method 2: Using Material for MkDocs Admonitions","text":"<p>You can combine admonitions with iframes for a nicer presentation:</p> <pre><code>!!! video \"Video Title\"\n    &lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VIDEO_ID\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n</code></pre> <p>Here's how it looks:</p> <p>Sample YouTube Video</p> <p></p>"},{"location":"examples/youtube-embed/#method-3-responsive-embedding","title":"Method 3: Responsive Embedding","text":"<p>For responsive videos that adjust to screen size, you can use this CSS approach:</p> <pre><code>&lt;div class=\"video-wrapper\"&gt;\n  &lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VIDEO_ID\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n&lt;/div&gt;\n</code></pre> <p>With this CSS in your <code>docs/stylesheets/extra.css</code> file:</p> <pre><code>.video-wrapper {\n  position: relative;\n  padding-bottom: 56.25%; /* 16:9 aspect ratio */\n  height: 0;\n  overflow: hidden;\n  max-width: 100%;\n}\n\n.video-wrapper iframe {\n  position: absolute;\n  top: 0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\n</code></pre>"},{"location":"examples/youtube-embed/#method-4-using-a-plugin","title":"Method 4: Using a Plugin","text":"<p>If you prefer using a plugin, you can install the <code>mkdocs-video</code> plugin. Add it to your <code>requirements-doc.txt</code> file:</p> <pre><code>mkdocs-video\n</code></pre> <p>Then add it to your <code>mkdocs.yml</code> file:</p> <pre><code>plugins:\n  - search\n  - mkdocs-video\n</code></pre> <p>And use it in your Markdown:</p> <pre><code>![type:video](https://www.youtube.com/embed/VIDEO_ID)\n</code></pre>"},{"location":"examples/youtube-embed/#getting-the-youtube-video-id","title":"Getting the YouTube Video ID","text":"<p>The video ID is the part of the YouTube URL after <code>v=</code>. For example:</p> <ul> <li>Full URL: <code>https://www.youtube.com/watch?v=dQw4w9WgXcQ</code></li> <li>Video ID: <code>dQw4w9WgXcQ</code></li> </ul> <p>You can also get the embed URL directly from YouTube by: 1. Going to the video on YouTube 2. Clicking \"Share\" 3. Clicking \"Embed\" 4. Copying the iframe code provided </p>"},{"location":"writing/","title":"Writing and Insights","text":"<p>I write about a mix of consulting, open source, personal work, and applying llms. I won't email you more than twice a month, not every post I write is worth sharing but I'll do my best to share the most interesting stuff including my own writing, thoughts, and experiences.</p> <p>If you find this content valuable, consider subscribing to my newsletter for regular updates on AI, technology trends, and insights into building impactful AI solutions.</p> <p> Subscribe to my Newsletter  Follow me on X</p> <p>For posts about RAG (Retrieval-Augmented Generation) or LLMs (Large Language Models), check out the category labels in the sidebar. Here are some of my best posts on these topics:</p>","tags":["consulting","AI","open source","writing","RAG"]},{"location":"writing/#rag-and-llm-expertise","title":"RAG and LLM Expertise","text":"<ul> <li>Agents and Architecture: Understanding the components of LLM Agents</li> <li>Model Control Protocol - MPC An Introduction</li> </ul>","tags":["consulting","AI","open source","writing","RAG"]},{"location":"writing/#talks-and-interviews","title":"Talks and Interviews","text":"<ul> <li>Forcing LLMs to Think with \"Think Tool\":Forget Chain-of-Thought\u2014This </li> <li>DIffusion LLMs: Diffusion LLMs Are Here!</li> <li>Reward Hacking: Reward Hacking Explained</li> </ul>","tags":["consulting","AI","open source","writing","RAG"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/","title":"Model Context Protocol (MCP) - A Quick Intro","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#introduction","title":"Introduction","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is an open, standardized protocol introduced by Anthropic that bridges AI models with external data sources, tools, and services. Think of MCP like a \"USB-C for AI applications\" \u2013 it provides a universal adapter for connecting AI assistants to various content repositories, business tools, code environments, and APIs. By defining a common interface (built on JSON-RPC 2.0) for communication, MCP enables large language models (LLMs) to invoke functions, retrieve data, or use predefined prompts from external systems in a consistent and secure way.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#purpose-and-problem-addressed","title":"Purpose and Problem Addressed","text":"<p>MCP was designed to solve a major integration challenge often called the \"M\u00d7N problem\" in AI development. Traditionally, integrating M different LLMs with N different tools or data sources required custom connectors for each combination \u2013 a combinatorial explosion of ad-hoc code. This meant AI systems were largely isolated from live data, trapped behind information silos unless developers painstakingly wired in each external API or database.</p> <p>MCP addresses this by providing one standardized \"language\" for all interactions. Developers can create one MCP-compliant interface for a data source or tool, and any MCP-enabled AI application can connect to it. This open standard replaces fragmented one-off integrations with a sustainable ecosystem of compatible clients and servers.</p> <p>The result is a simpler, more scalable way to give AI assistants access to the fresh, relevant context they need \u2013 whether it\u2019s company documents, live databases, or web results.</p> <p>MCP is two-way and secure: it enables LLMs to query data and perform actions, while allowing organizations to keep data access controlled. The protocol supports OAuth 2.1 for authentication, enabling safe and secure enterprise use.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#comparison-mcp-vs-simpler-function-calling-apis","title":"Comparison: MCP vs. Simpler Function-Calling APIs","text":"Feature Function-Calling (e.g. OpenAI/Google) MCP (Anthropic) Scope Individual functions defined per-model Unified interface for data, tools, and prompts Standardization Proprietary formats (OpenAI, Google) Open JSON-RPC-based protocol Interoperability Model- and vendor-specific Any LLM can talk to any MCP-compliant server State Management Stateless (single call-response) Persistent sessions and complex workflows Security Developer-defined Built-in auth flows (OAuth 2.1), user approval model Extensibility Requires custom code per function Plug-and-play with reusable components","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#mermaid-function-calling-vs-mcp-workflow","title":"Mermaid: Function-Calling vs. MCP Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant LLM_API as LLM (function-calling)\n    participant App as Developer App\n    participant API as External API\n    User-&gt;&gt;LLM_API: Ask a question\n    LLM_API--&gt;&gt;App: Emit function call\n    App-&gt;&gt;API: Call API\n    API--&gt;&gt;App: API Response\n    App--&gt;&gt;LLM_API: Return result\n    LLM_API--&gt;&gt;User: Final answer</code></pre> <pre><code>sequenceDiagram\n    participant User\n    participant AI_Client as MCP Client\n    participant MCPServer as MCP Server\n    participant API as External API\n    User-&gt;&gt;AI_Client: Ask a question\n    AI_Client-&gt;&gt;MCPServer: tools/call\n    MCPServer-&gt;&gt;API: API Call\n    API--&gt;&gt;MCPServer: Response\n    MCPServer--&gt;&gt;AI_Client: Result\n    AI_Client--&gt;&gt;User: Final answer</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#core-components-of-mcp","title":"Core Components of MCP","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#server-side-primitives","title":"Server-Side Primitives","text":"<ul> <li>Resources \u2013 Static or dynamic data sources exposed via URI (e.g. <code>file://</code>, <code>db://</code>).</li> <li>Tools \u2013 Executable functions the AI can call with structured inputs/outputs.</li> <li>Prompts \u2013 Predefined prompt templates or workflows returned as message sequences.</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#client-side-primitives","title":"Client-Side Primitives","text":"<ul> <li>Roots \u2013 Contextual entry points (e.g. current workspace, folder, URL scope).</li> <li>Sampling \u2013 Allows servers to call back to the model via the client (used for chaining, agents).</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#mermaid-mcp-architecture","title":"Mermaid: MCP Architecture","text":"<pre><code>flowchart LR\n    subgraph Host[AI Host Application]\n        LLM\n        Client[MCP Client]\n        LLM --&gt; Client\n    end\n    subgraph Server1[MCP Server: Files]\n        Tools1\n        Resources1\n        Prompts1\n    end\n    subgraph Server2[MCP Server: Web API]\n        Tools2\n        Resources2\n    end\n    Client -- JSON-RPC --&gt; Server1\n    Client -- JSON-RPC --&gt; Server2</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#python-tutorial-building-an-mcp-server","title":"Python Tutorial: Building an MCP Server","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-1-install-sdk","title":"Step 1: Install SDK","text":"<pre><code>pip install mcp\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-2-minimal-server","title":"Step 2: Minimal Server","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nmcp = FastMCP(\"Demo Server\")\nmcp.serve()\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-3-add-a-tool","title":"Step 3: Add a Tool","text":"<pre><code>@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-4-add-a-resource","title":"Step 4: Add a Resource","text":"<pre><code>@mcp.resource(\"greet://{name}\")\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-5-add-a-prompt","title":"Step 5: Add a Prompt","text":"<pre><code>from mcp.server.fastmcp.prompts import base\n\n@mcp.prompt()\ndef solve_math(problem: str) -&gt; list[base.Message]:\n    return [\n        base.SystemMessage(\"You are a math assistant.\"),\n        base.UserMessage(f\"Solve: {problem}\"),\n        base.AssistantMessage(\"Let me show the steps...\")\n    ]\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-6-run-and-test","title":"Step 6: Run and Test","text":"<pre><code>python server.py\nmcp dev server.py\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#advanced-use-cases","title":"Advanced Use Cases","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#state-management","title":"State Management","text":"<ul> <li>Servers can hold state across requests (e.g. database session).</li> <li>Maintain open connections or cache results.</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#agent-frameworks","title":"Agent Frameworks","text":"<ul> <li>LangChain supports converting MCP Tools into agent tools.</li> <li>Compatible with Claude, ChatGPT (via Agents SDK), etc.</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<pre><code>sequenceDiagram\n    participant User\n    participant AI as Client\n    participant RAGServer as Server\n    participant DB as Document Store\n    User-&gt;&gt;AI: Ask question\n    AI-&gt;&gt;RAGServer: tools/call search\n    RAGServer-&gt;&gt;DB: Query\n    DB--&gt;&gt;RAGServer: Top documents\n    RAGServer--&gt;&gt;AI: Text/resources\n    AI--&gt;&gt;User: Informed answer</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#multi-modal-extensions","title":"Multi-Modal Extensions","text":"<ul> <li>Resources can serve images, audio, etc.</li> <li>Tools can control browsers, devices, cloud services.</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#security-governance","title":"Security &amp; Governance","text":"<ul> <li>Human approval is built-in for risky actions.</li> <li>OAuth 2.1 support for secure resource access.</li> </ul>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/model-context-protocol-mcp---a-quick-intro/#conclusion","title":"Conclusion","text":"<p>MCP represents a new, open foundation for connecting AI to external tools and context. It standardizes how LLMs retrieve data, call functions, and use reusable prompts, making AI integration modular, scalable, and interoperable.</p> <p>Anthropic\u2019s open protocol is being adopted across the industry, enabling a future where AI assistants plug into the real world seamlessly \u2014 through one unified protocol.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/","title":"A Visual Guide to LLM Agents","text":"","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to Large Language Models</li> <li>From LLMs to Agents</li> <li>Core Components of LLM Agents</li> <li>Tools and Augmentation</li> <li>Agent Planning and Reasoning</li> <li>Agent Memory Systems</li> <li>Advanced Agent Architectures</li> <li>Multi-Agent Systems</li> <li>Building and Deploying Agents</li> <li>Future Directions</li> </ol>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#introduction-to-large-language-models","title":"Introduction to Large Language Models","text":"<p>Before diving into agents, we need to understand what Large Language Models (LLMs) are and how they function.</p> <p>LLMs are sophisticated neural networks trained on vast amounts of text data to understand and generate human-like text. These models have evolved from simple statistical approaches to complex architectures based primarily on the Transformer architecture introduced in 2017.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#how-llms-work","title":"How LLMs Work","text":"<p>At their core, LLMs predict the next token (word or subword) in a sequence based on the context provided. The basic architecture consists of:</p> <pre><code>flowchart TD\n    A[Input Text] --&gt; B[Tokenization]\n    B --&gt; C[Embedding Layer]\n    C --&gt; D[Transformer Layers]\n    D --&gt; E[Output Layer]\n    E --&gt; F[Generated Text]\n\n    style A fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style B fill:#e6f3ff,stroke:#333,stroke-width:1px\n    style C fill:#e6f3ff,stroke:#333,stroke-width:1px\n    style D fill:#cce5ff,stroke:#333,stroke-width:1px\n    style E fill:#e6f3ff,stroke:#333,stroke-width:1px\n    style F fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> <ul> <li>Tokenization: Converting input text into tokens</li> <li>Embedding Layer: Transforming tokens into vector representations</li> <li>Transformer Layers: Processing these vectors through attention mechanisms</li> <li>Output Layer: Generating probability distributions for the next token</li> </ul> <p>Most modern LLMs use the transformer architecture, which employs self-attention mechanisms to weigh the importance of different words in context.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#capabilities-and-limitations-of-traditional-llms","title":"Capabilities and Limitations of Traditional LLMs","text":"<p>Capabilities: - Text generation across various domains - Understanding context and nuance - Adapting to different writing styles - Performing various language tasks without task-specific training</p> <p>Limitations: - No ability to access or verify external information beyond training data - No capability to take actions in the world - Limited understanding of temporal context (when events occurred) - No persistent memory between sessions - No ability to use tools or APIs - Risk of hallucinations (generating false information)</p> <p>These limitations highlight why moving from passive LLMs to active agents is necessary for more complex applications.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#from-llms-to-agents","title":"From LLMs to Agents","text":"<p>The transition from passive language models to active agents is fundamental to understanding LLM agents.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#what-defines-an-agent","title":"What Defines an Agent?","text":"<p>As defined by Russell &amp; Norvig, \"an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\"</p> <p>This definition introduces two critical components missing in standard LLMs: 1. Perception: The ability to sense the environment 2. Action: The ability to affect the environment</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#the-agent-framework","title":"The Agent Framework","text":"<p>An agent-based framework adapts this definition to work with LLMs:</p> <pre><code>flowchart LR\n    subgraph LLM[\"Traditional LLM\"]\n        A[Input Prompt] --&gt; B[Text Generation]\n        B --&gt; C[Output Text]\n    end\n\n    subgraph Agent[\"LLM Agent\"]\n        D[Environment] --&gt; E[Perception]\n        E --&gt; F[Reasoning/Planning]\n        F --&gt; G[Tool Use]\n        G --&gt; H[Actions]\n        H --&gt; D\n        I[Memory] --&gt; F\n        F --&gt; I\n    end\n\n    LLM --&gt; Agent\n\n    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px\n    style Agent fill:#e6f3ff,stroke:#333,stroke-width:2px</code></pre> <ul> <li>Environment: The context in which the agent operates (could be a chat interface, document, or digital environment)</li> <li>Perception: Input methods like prompts, document content, or API responses</li> <li>Reasoning: Internal processing using the LLM to decide what to do</li> <li>Action: Outputs that affect the environment (generating text, calling functions, using tools)</li> <li>Memory: Retaining information across interactions</li> </ul> <p>This framework transforms a passive text prediction system into an entity that can intelligently interact with its surroundings.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#core-components-of-llm-agents","title":"Core Components of LLM Agents","text":"<p>Let's explore the essential components that make up an LLM agent:</p> <pre><code>flowchart LR\n    %% Central node with dashed border\n    LLM[\"Large Language\\nModel\"] \n\n    %% Main components with their subgraphs\n    subgraph Tools[\" \"]\n        T[Tool Use] --&gt; T1[Function Calling]\n        T --&gt; T2[API Integration]\n        T --&gt; T3[Database Access]\n    end\n\n    subgraph Reasoning[\" \"]\n        R[Reasoning] --&gt; R1[Task Decomposition]\n        R --&gt; R2[Chain-of-Thought]\n        R --&gt; R3[Decision-Making]\n    end\n\n    subgraph Perception[\" \"]\n        P[Perception] --&gt; P1[Text Inputs]\n        P --&gt; P2[Document Understanding]\n        P --&gt; P3[Multimodal Inputs]\n    end\n\n    subgraph Memory[\" \"]\n        M[Memory] --&gt; M1[Short-Term]\n        M --&gt; M2[Long-Term]\n        M --&gt; M3[Episodic &amp; Semantic]\n    end\n\n    %% Connect LLM to main components\n    LLM --&gt; T\n    LLM --&gt; R\n    LLM --&gt; P\n    LLM --&gt; M\n\n    %% Style nodes\n    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px,stroke-dasharray:5 5\n\n    %% Tool styles\n    style T fill:#fd7e14,stroke:#333,stroke-width:2px\n    style T1 fill:#ffe8cc,stroke:#333,stroke-width:1px\n    style T2 fill:#ffe8cc,stroke:#333,stroke-width:1px\n    style T3 fill:#ffe8cc,stroke:#333,stroke-width:1px\n\n    %% Reasoning styles\n    style R fill:#40c057,stroke:#333,stroke-width:2px\n    style R1 fill:#d3f9d8,stroke:#333,stroke-width:1px\n    style R2 fill:#d3f9d8,stroke:#333,stroke-width:1px\n    style R3 fill:#d3f9d8,stroke:#333,stroke-width:1px\n\n    %% Perception styles\n    style P fill:#4dabf7,stroke:#333,stroke-width:2px\n    style P1 fill:#d0ebff,stroke:#333,stroke-width:1px\n    style P2 fill:#d0ebff,stroke:#333,stroke-width:1px\n    style P3 fill:#d0ebff,stroke:#333,stroke-width:1px\n\n    %% Memory styles\n    style M fill:#ae3ec9,stroke:#333,stroke-width:2px\n    style M1 fill:#f3d9fa,stroke:#333,stroke-width:1px\n    style M2 fill:#f3d9fa,stroke:#333,stroke-width:1px\n    style M3 fill:#f3d9fa,stroke:#333,stroke-width:1px\n\n</code></pre>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#environment-perception","title":"Environment Perception","text":"<p>Agents need to understand their environment through various inputs:</p> <ul> <li>Text Input: The most basic form of perception through prompts</li> <li>Document Understanding: Processing and understanding documents</li> <li>Structured Data: Working with databases, APIs, and structured information</li> <li>Multimodal Input: Processing images, audio, or other data types (in advanced agents)</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#planning-and-reasoning","title":"Planning and Reasoning","text":"<p>An agent must plan its actions and reason about the best course of action:</p> <ul> <li>Task Decomposition: Breaking complex tasks into manageable steps</li> <li>Chain-of-Thought: Working through problems step-by-step</li> <li>Decision-Making: Evaluating options and selecting the best course of action</li> <li>Self-Reflection: Evaluating its own reasoning and outputs</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#tool-usage-and-integration","title":"Tool Usage and Integration","text":"<p>A defining characteristic of LLM agents is their ability to use tools:</p> <ul> <li>Function Calling: Identifying when to call an external function</li> <li>API Integration: Connecting to external services through APIs</li> <li>Code Execution: Running code to perform calculations or manipulate data</li> <li>Database Access: Retrieving or storing information in databases</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#memory-systems","title":"Memory Systems","text":"<p>Agents require memory to maintain context and learn from past interactions:</p> <pre><code>flowchart TB\n    %% Short-term Memory Section\n    subgraph ShortTerm[\"Short-Term Memory\"]\n        direction LR\n        CH[\"Conversation History\"] --- CD[\"Current Session Data\"] --- AT[\"Active Task State\"]\n    end\n\n    %% Long-term Memory Section\n    subgraph LongTerm[\"Long-Term Memory\"]\n        direction LR\n        VD[\"Vector Database\"] --- KG[\"Knowledge Graph\"] --- DD[\"Document Store\"] --- UP[\"User Profiles\"]\n    end\n\n    %% Memory Types Section\n    subgraph MemTypes[\"Memory Types\"]\n        direction LR\n        EM[\"Episodic Memory&lt;br/&gt;(Specific Interactions)\"] --- SM[\"Semantic Memory&lt;br/&gt;(General Knowledge)\"] --- PM[\"Procedural Memory&lt;br/&gt;(How to Perform Tasks)\"]\n    end\n\n    %% Memory Retrieval Section\n    subgraph Retrieval[\"Memory Retrieval\"]\n        direction LR\n        SS[\"Semantic Search\"] --- TF[\"Temporal Filtering\"] --- SR[\"Relevance Ranking\"] --- CR[\"Contextual Retrieval\"]\n    end\n\n    %% Decision Making\n    DM[\"Agent Decision Making\"]\n\n    %% Connections\n    ShortTerm --&gt; MemTypes\n    LongTerm --&gt; MemTypes\n    MemTypes --&gt; Retrieval\n    Retrieval --&gt; DM\n\n    %% Styling\n    style ShortTerm fill:#fff7e6,stroke:#333,stroke-width:1px\n    style LongTerm fill:#e6ffe6,stroke:#333,stroke-width:1px\n    style MemTypes fill:#e6f7ff,stroke:#333,stroke-width:1px\n    style Retrieval fill:#ffe6e6,stroke:#333,stroke-width:1px\n    style DM fill:#f0f0ff,stroke:#333,stroke-width:1px\n\n    %% Node Styles\n    classDef default fill:#fff,stroke:#333,stroke-width:1px</code></pre> <ul> <li>Short-Term Memory: Recent conversation history</li> <li>Long-Term Memory: Persistent information stored across sessions</li> <li>Episodic Memory: Specific sequences of interactions</li> <li>Semantic Memory: General knowledge and facts</li> </ul> <p>These core components transform an LLM into an agent capable of complex, goal-oriented behavior.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#tools-and-augmentation","title":"Tools and Augmentation","text":"<p>Tools and augmentation techniques enhance the capabilities of LLM agents beyond their built-in knowledge.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#types-of-tools","title":"Types of Tools","text":"<p>Modern LLM agents can leverage various tools:</p> <ul> <li>Search Tools: Accessing up-to-date information from the internet</li> <li>Calculators: Performing precise mathematical operations</li> <li>Knowledge Bases: Accessing specific domain knowledge</li> <li>Code Interpreters: Executing and debugging code</li> <li>Database Interfaces: Querying and manipulating structured data</li> <li>API Connectors: Interacting with external services and platforms</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>RAG is a powerful technique that combines retrieval of information with text generation:</p> <pre><code>flowchart TD\n    Q[Query/Question] --&gt; E[Embedding Model]\n    E --&gt; VS[Vector Search]\n\n    subgraph Indexing[\"Document Indexing (Pre-processing)\"]\n        D[Documents] --&gt; DC[Document Chunking]\n        DC --&gt; DE[Document Embedding]\n        DE --&gt; VDB[Vector Database]\n    end\n\n    VS --&gt; VDB\n    VDB --&gt; RD[Retrieved Documents]\n\n    Q --&gt; P[Prompt Construction]\n    RD --&gt; P\n\n    P --&gt; LLM[Large Language Model]\n    LLM --&gt; A[Augmented Response]\n\n    style Indexing fill:#e6f3ff,stroke:#333,stroke-width:1px\n    style Q fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style P fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style LLM fill:#cce5ff,stroke:#333,stroke-width:2px\n    style A fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> <ol> <li>Indexing: Documents are processed, chunked, and stored in a vector database</li> <li>Retrieval: When a query is received, relevant documents are retrieved</li> <li>Augmentation: Retrieved content is added to the prompt</li> <li>Generation: The LLM generates a response based on both the query and the retrieved information</li> </ol> <p>RAG enhances accuracy by grounding responses in specific knowledge sources, reducing hallucinations and improving factual accuracy.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#function-and-api-integration","title":"Function and API Integration","text":"<p>Function calling allows agents to interact with the world:</p> <pre><code>sequenceDiagram\n    participant User\n    participant LLM\n    participant Function\n\n    User-&gt;&gt;LLM: Query (e.g., \"What's the weather in New York?\")\n\n    LLM-&gt;&gt;LLM: Recognize need for external data\n\n    LLM-&gt;&gt;Function: Call function&lt;br/&gt;(get_weather, {location: \"New York\"})\n    Function-&gt;&gt;LLM: Return data (Temperature: 72\u00b0F, Condition: Sunny)\n\n    LLM-&gt;&gt;User: Generate response with function data&lt;br/&gt;\"The current weather in New York is sunny with a temperature of 72\u00b0F.\"\n\n    note over LLM,Function: Modern LLMs can determine when to&lt;br/&gt;call functions and structure the&lt;br/&gt;appropriate parameters</code></pre> <ol> <li>Function Definition: Functions are defined with names, descriptions, and parameter specifications</li> <li>Function Detection: The LLM detects when a function should be called based on the user's query</li> <li>Parameter Generation: The LLM generates the appropriate parameters</li> <li>Function Execution: The function is executed, and results are returned</li> <li>Response Integration: The LLM incorporates the function results into its response</li> </ol> <p>This capability enables agents to perform actions like checking the weather, booking appointments, or processing payments.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#agent-planning-and-reasoning","title":"Agent Planning and Reasoning","text":"<p>Effective planning and reasoning are crucial for complex tasks.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#prompt-engineering-for-agents","title":"Prompt Engineering for Agents","text":"<p>Agent prompts typically include:</p> <ul> <li>System Instructions: Defining the agent's role and capabilities</li> <li>Available Tools: Descriptions of tools the agent can use</li> <li>Constraints: Limitations on the agent's actions</li> <li>Output Format: How the agent should structure its responses</li> <li>Examples: Demonstrations of expected behavior</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#chain-of-thought-cot-reasoning","title":"Chain-of-Thought (CoT) Reasoning","text":"<p>CoT enables an agent to work through problems step-by-step:</p> <ol> <li>Problem Analysis: Understanding the task and breaking it down</li> <li>Intermediate Steps: Working through each step logically</li> <li>Reflection: Checking the reasoning at each step</li> <li>Solution: Arriving at the final answer based on the steps</li> </ol> <p>This approach significantly improves performance on complex reasoning tasks.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#react-framework","title":"ReAct Framework","text":"<p>ReAct (Reasoning + Acting) interleaves thinking and action:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tools as External Tools\n\n    User-&gt;&gt;Agent: Task or Query\n\n    loop Until task completion\n        Agent-&gt;&gt;Agent: Thought: Reasoning about next step\n        Agent-&gt;&gt;Agent: Action: Decide which tool to use\n        Agent-&gt;&gt;Tools: Call appropriate tool\n        Tools-&gt;&gt;Agent: Observation: Return result\n        Agent-&gt;&gt;Agent: Thought: Process observation\n    end\n\n    Agent-&gt;&gt;User: Final response\n\n    note over Agent: ReAct interleaves reasoning (thoughts)&lt;br/&gt;with actions and observations in a cycle</code></pre> <ol> <li>Reasoning: The agent thinks about what it needs to do</li> <li>Action: The agent takes action using available tools</li> <li>Observation: The agent observes the results of its action</li> <li>Continued Reasoning: The agent incorporates observations into its reasoning</li> </ol> <p>This cycle continues until the task is complete, enabling dynamic, adaptive problem-solving.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#agent-memory-systems","title":"Agent Memory Systems","text":"<p>Memory systems enable agents to maintain context and learn from past interactions.</p> <pre><code>flowchart TD\n    subgraph SM[\"Short-Term Memory\"]\n        SM1[\"Conversation History\"]\n        SM2[\"Current Session Data\"]\n        SM3[\"Active Task State\"]\n    end\n\n    subgraph LM[\"Long-Term Memory\"]\n        LM1[\"Vector Database\"]\n        LM2[\"Knowledge Graph\"]\n        LM3[\"Document Store\"]\n        LM4[\"User Profiles\"]\n    end\n\n    subgraph MT[\"Memory Types\"]\n        MT1[\"Episodic Memory&lt;br/&gt;(Specific Interactions)\"]\n        MT2[\"Semantic Memory&lt;br/&gt;(General Knowledge)\"]\n        MT3[\"Procedural Memory&lt;br/&gt;(How to Perform Tasks)\"]\n    end\n\n    subgraph MR[\"Memory Retrieval\"]\n        MR1[\"Semantic Search\"]\n        MR2[\"Temporal Filtering\"]\n        MR3[\"Relevance Ranking\"]\n        MR4[\"Contextual Retrieval\"]\n    end\n\n    SM --&gt; MT\n    LM --&gt; MT\n    MT --&gt; MR\n    MR --&gt; A[Agent Decision Making]\n\n    style SM fill:#ffffcc,stroke:#333,stroke-width:1px\n    style LM fill:#ccffcc,stroke:#333,stroke-width:1px\n    style MT fill:#ccffff,stroke:#333,stroke-width:1px\n    style MR fill:#ffcccc,stroke:#333,stroke-width:1px</code></pre>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#short-term-context","title":"Short-Term Context","text":"<p>Short-term or working memory includes:</p> <ul> <li>Conversation History: The recent exchanges between user and agent</li> <li>Current Session Data: Information gathered during the current interaction</li> <li>Active Task State: The current progress on the task being performed</li> </ul> <p>These elements are typically handled through context window management.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#long-term-memory-storage","title":"Long-Term Memory Storage","text":"<p>Long-term memory enables persistent information storage:</p> <ul> <li>Vector Databases: Storing semantic representations of past conversations</li> <li>Knowledge Graphs: Structured representations of entities and relationships</li> <li>Document Stores: Persistent storage of important information</li> <li>User Profiles: Preferences and patterns specific to individual users</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#episodic-vs-semantic-memory","title":"Episodic vs. Semantic Memory","text":"<p>Agents can implement different types of memory:</p> <ul> <li>Episodic Memory: Specific sequences of interactions (e.g., \"Last time we discussed home renovation options\")</li> <li>Semantic Memory: General knowledge and facts (e.g., \"The user prefers minimalist design\")</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#memory-retrieval-strategies","title":"Memory Retrieval Strategies","text":"<p>Effective retrieval is critical for using stored information:</p> <ul> <li>Semantic Search: Finding relevant information based on meaning</li> <li>Temporal Filtering: Retrieving information based on when it was stored</li> <li>Relevance Ranking: Prioritizing the most important information</li> <li>Contextual Retrieval: Finding information relevant to the current context</li> </ul> <p>A well-designed memory system allows agents to build on past interactions and provide more personalized experiences.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#advanced-agent-architectures","title":"Advanced Agent Architectures","text":"<p>As agents become more sophisticated, their architectures evolve to handle more complex tasks.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#task-decomposition","title":"Task Decomposition","text":"<p>Complex task handling requires sophisticated decomposition:</p> <ol> <li>Goal Analysis: Understanding the overall objective</li> <li>Subtask Identification: Breaking down the goal into manageable parts</li> <li>Dependency Mapping: Determining the order of subtasks</li> <li>Resource Allocation: Assigning appropriate tools to each subtask</li> </ol> <p>This approach enables agents to tackle problems too complex to solve all at once.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#self-reflection-and-self-correction","title":"Self-Reflection and Self-Correction","text":"<p>Advanced agents can evaluate and improve their own outputs:</p> <ol> <li>Output Generation: Producing an initial response</li> <li>Self-Critique: Identifying potential issues or improvements</li> <li>Refinement: Revising the response based on self-critique</li> <li>Verification: Checking the improved response against requirements</li> </ol> <p>This recursive improvement process enhances accuracy and quality.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#verification-of-outputs","title":"Verification of Outputs","text":"<p>Ensuring reliability through verification:</p> <ul> <li>Fact-Checking: Verifying factual claims against reliable sources</li> <li>Consistency Checks: Ensuring internal consistency in responses</li> <li>Hallucination Detection: Identifying when the agent is generating unfounded information</li> <li>Confidence Scoring: Assessing the reliability of different parts of a response</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#meta-prompting-and-prompt-chaining","title":"Meta-Prompting and Prompt Chaining","text":"<p>Sophisticated prompting techniques:</p> <ul> <li>Meta-Prompting: Using the LLM to generate or refine its own prompts</li> <li>Prompt Chaining: Connecting multiple prompts in sequence to handle complex workflows</li> <li>Adaptive Prompting: Modifying prompts based on user responses or task progress</li> </ul> <p>These techniques allow for more flexible and powerful agent behaviors.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#multi-agent-systems","title":"Multi-Agent Systems","text":"<p>Multiple specialized agents can collaborate to solve complex problems.</p> <pre><code>flowchart TD\n    U[User] --&gt; C[Coordinator Agent]\n\n    C --&gt; P[Planner Agent]\n    C --&gt; R[Researcher Agent]\n    C --&gt; E[Expert Agent]\n    C --&gt; CR[Critic Agent]\n\n    P --&gt; C\n    R --&gt; C\n    E --&gt; C\n    CR --&gt; C\n\n    C --&gt; U\n\n    subgraph Communication\n        P &lt;-.-&gt; R\n        R &lt;-.-&gt; E\n        E &lt;-.-&gt; CR\n        P &lt;-.-&gt; CR\n    end\n\n    style U fill:#f9f9f9,stroke:#333,stroke-width:1px\n    style C fill:#ffcc99,stroke:#333,stroke-width:2px\n    style P fill:#ccffcc,stroke:#333,stroke-width:1px\n    style R fill:#ccffcc,stroke:#333,stroke-width:1px\n    style E fill:#ccffcc,stroke:#333,stroke-width:1px\n    style CR fill:#ccffcc,stroke:#333,stroke-width:1px\n    style Communication opacity:0.2</code></pre>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#agent-collaboration-models","title":"Agent Collaboration Models","text":"<p>Different models for agent collaboration:</p> <ul> <li>Hierarchical: Supervisor agents coordinate specialized worker agents</li> <li>Peer-to-Peer: Agents communicate directly with each other</li> <li>Market-Based: Agents bid for tasks based on their capabilities</li> <li>Consensus-Based: Agents work together to reach agreement on solutions</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#specialized-agent-roles","title":"Specialized Agent Roles","text":"<p>Multi-agent systems often feature specialized roles:</p> <ul> <li>Planner: Designs overall strategy and breaks down tasks</li> <li>Researcher: Gathers information from various sources</li> <li>Expert: Provides domain-specific knowledge and analysis</li> <li>Critic: Evaluates and improves outputs</li> <li>Coordinator: Manages communication between agents</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#communication-protocols","title":"Communication Protocols","text":"<p>Effective inter-agent communication requires:</p> <ul> <li>Message Formats: Structured formats for exchanging information</li> <li>Dialogue Management: Tracking conversation state between agents</li> <li>Knowledge Sharing: Methods for sharing relevant information</li> <li>Conflict Resolution: Mechanisms for resolving disagreements</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#consensus-mechanisms","title":"Consensus Mechanisms","text":"<p>When agents must agree on a course of action:</p> <ul> <li>Voting: Simple majority or weighted voting schemes</li> <li>Debate: Agents present arguments and counter-arguments</li> <li>Evidence Evaluation: Assessing the quality of evidence presented</li> <li>Meta-Evaluation: Using another agent to evaluate competing proposals</li> </ul> <p>Multi-agent systems enable more complex problem-solving than any single agent could achieve alone.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#building-and-deploying-agents","title":"Building and Deploying Agents","text":"<p>Practical considerations for implementing LLM agents.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#frameworks-and-libraries","title":"Frameworks and Libraries","text":"<p>Popular tools for building agents:</p> <ul> <li>LangChain: Framework for building language model applications</li> <li>LlamaIndex: Tools for connecting LLMs to external data</li> <li>AutoGPT: Autonomous AI agent framework</li> <li>Microsoft Semantic Kernel: Framework for integrating AI with traditional programming</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Assessing agent performance:</p> <ul> <li>Task Completion Rate: How often the agent successfully completes tasks</li> <li>Efficiency: Number of steps or time required to complete tasks</li> <li>Accuracy: Correctness of information and actions</li> <li>User Satisfaction: User ratings and feedback</li> <li>Hallucination Rate: Frequency of unfounded claims</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#safety-considerations","title":"Safety Considerations","text":"<p>Important safety measures:</p> <ul> <li>Action Limitations: Restricting potentially harmful actions</li> <li>User Confirmation: Requiring approval for significant actions</li> <li>Monitoring: Tracking agent behavior for unexpected patterns</li> <li>Transparency: Making reasoning and sources clear to users</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#deployment-patterns","title":"Deployment Patterns","text":"<p>Common approaches to deployment:</p> <ul> <li>Serverless Functions: Deploying components as cloud functions</li> <li>Containerization: Packaging agents and dependencies in containers</li> <li>API Services: Exposing agent capabilities through APIs</li> <li>Edge Deployment: Running lightweight agents on edge devices</li> </ul> <p>Careful attention to these aspects ensures agents that are effective, reliable, and safe.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#future-directions","title":"Future Directions","text":"<p>The field of LLM agents is rapidly evolving. Here are some emerging trends and challenges:</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#current-limitations","title":"Current Limitations","text":"<p>Areas needing improvement:</p> <ul> <li>Reasoning Abilities: Enhancing logical and causal reasoning</li> <li>Tool Creation: Enabling agents to create new tools as needed</li> <li>True Autonomy: Reducing the need for human oversight</li> <li>Cross-Domain Knowledge: Applying knowledge across different domains</li> <li>Efficiency: Reducing computational requirements</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#research-frontiers","title":"Research Frontiers","text":"<p>Exciting areas of research:</p> <ul> <li>Embodied Agents: Connecting language models to robotic systems</li> <li>Multi-Modal Agents: Integrating text, vision, audio, and other modalities</li> <li>Continual Learning: Agents that learn and improve through interaction</li> <li>Collective Intelligence: Emergent capabilities from agent collaboration</li> <li>Neural-Symbolic Approaches: Combining neural networks with symbolic reasoning</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#potential-applications","title":"Potential Applications","text":"<p>Promising applications for advanced agents:</p> <ul> <li>Personalized Education: Tutors adapted to individual learning styles</li> <li>Scientific Discovery: Agents that generate and test hypotheses</li> <li>Healthcare Assistance: Diagnostic and treatment planning support</li> <li>Creative Collaboration: Partners for writing, design, and other creative tasks</li> <li>Autonomous Systems: Self-directed systems that adapt to changing conditions</li> </ul>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#ethical-considerations","title":"Ethical Considerations","text":"<p>Important ethical questions:</p> <ul> <li>Transparency: Ensuring users understand agent capabilities and limitations</li> <li>Accountability: Determining responsibility for agent actions</li> <li>Privacy: Protecting sensitive information used by agents</li> <li>Bias: Addressing biases in training data and reasoning</li> <li>Human Augmentation: Enhancing rather than replacing human capabilities</li> </ul> <p>The future of LLM agents will depend on thoughtful approaches to these challenges and opportunities.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/28/a-visual-guide-to-llm-agents/#conclusion","title":"Conclusion","text":"<p>LLM agents represent a significant evolution in artificial intelligence, transforming passive language models into active, capable assistants. By combining the language understanding of LLMs with the ability to perceive, reason, and act, these agents can solve increasingly complex problems and provide more valuable assistance.</p> <p>As the technology continues to develop, we can expect agents to become more autonomous, capable, and integrated into our daily lives and work. The journey from simple language models to sophisticated agents is just beginning, with many exciting possibilities on the horizon.</p> <p>The most successful approaches will likely be those that thoughtfully combine the strengths of artificial and human intelligence, creating systems that augment human capabilities rather than simply attempting to replace them.</p>","tags":["agents","llm","architecture","rag"]},{"location":"writing/2025/03/31/introduction/","title":"Introduction","text":"","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is an open, standardized protocol introduced by Anthropic that bridges AI models with external data sources, tools, and services. Think of MCP like a \"USB-C for AI applications\" \u2013 it provides a universal adapter for connecting AI assistants to various content repositories, business tools, code environments, and APIs. By defining a common interface (built on JSON-RPC 2.0) for communication, MCP enables large language models (LLMs) to invoke functions, retrieve data, or use predefined prompts from external systems in a consistent and secure way.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#purpose-and-problem-addressed","title":"Purpose and Problem Addressed","text":"<p>MCP was designed to solve a major integration challenge often called the \"M\u00d7N problem\" in AI development. Traditionally, integrating M different LLMs with N different tools or data sources required custom connectors for each combination \u2013 a combinatorial explosion of ad-hoc code. This meant AI systems were largely isolated from live data, trapped behind information silos unless developers painstakingly wired in each external API or database. MCP addresses this by providing one standardized \"language\" for all interactions.</p> <p>In practice, developers can create one MCP-compliant interface for a data source or tool, and any MCP-enabled AI application can connect to it. This open standard thus replaces fragmented one-off integrations with a sustainable ecosystem of compatible clients and servers. The result is a simpler, more scalable way to give AI assistants access to the fresh, relevant context they need \u2013 whether it's company documents, live databases, or web results.</p> <p>Importantly, MCP is two-way and secure: it enables LLMs to query data and perform actions, while allowing organizations to keep data access controlled (the latest spec even supports OAuth 2.1 for authentication).</p> <p>In summary, MCP's purpose is to make AI integrations interoperable, secure, and context-rich, turning isolated LLMs into truly context-aware systems.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#comparison-model-context-protocol-mcp-vs-simpler-function-calling-apis","title":"Comparison: Model Context Protocol (MCP) vs. Simpler Function-Calling APIs","text":"<p>MCP represents a more comprehensive and standardized approach to AI-tool integration compared to the simpler function-calling mechanisms provided by vendors like OpenAI or Google. Below we contrast MCP with typical LLM function-calling or plugin approaches:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#scope-of-integration","title":"Scope of Integration","text":"<p>Traditional function calling (e.g. OpenAI's function call API) lets a developer define a few functions that an LLM can call during a single chat session. While useful, this is relatively limited in scope \u2013 each function integration is custom and tied to one specific model or platform. In contrast, MCP covers a broader scope: it standardizes not just function calls, but also data retrieval (resources) and reusable prompt workflows across any number of tools. An MCP server can expose multiple capabilities (data, tools, prompts) at once to any compatible AI client. This turns the one-off \"tool use\" into a universal interface for many integrations at scale.</p> <p>For example, via MCP a single AI assistant could simultaneously connect to a code repository, a database, and a web browser tool \u2013 all through one protocol \u2013 whereas vanilla function calling would require baking in each integration separately.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#standardization-and-interoperability","title":"Standardization and Interoperability","text":"<p>OpenAI's function calls and ChatGPT plugins are proprietary approaches \u2013 they work primarily within OpenAI's ecosystem (or Google's, for their functions) and follow vendor-specific formats. Each ChatGPT plugin essentially required its own mini-integration defined by an OpenAPI spec, and only certain platforms (like ChatGPT or Bing) could use those plugins.</p> <p>By contrast, MCP is an open standard from the ground up. It's vendor-agnostic and designed for broad adoption: any LLM provider or tool builder can implement MCP's JSON-RPC interface. Think of ChatGPT plugins as specialized tools in a closed toolbox, whereas MCP is an open toolkit that any AI platform or developer can utilize. This standardization means an MCP-compliant server (say for Google Drive, Slack, etc.) can be used by Anthropic's Claude, OpenAI's ChatGPT (which recently announced support), or a local open-source LLM alike. MCP's interoperability has gained momentum \u2013 even OpenAI and Microsoft have added support for it, signaling industry convergence on open standards.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#state-management-and-persistence","title":"State Management and Persistence","text":"<p>Simple function-calling is typically stateless beyond the immediate request. The LLM calls a function and gets a result, but there is no persistent connection \u2013 each tool use is a one-shot call embedded in the prompt/response cycle. MCP, on the other hand, establishes a persistent client\u2013server connection. An MCP server can maintain state across multiple calls (e.g. keep a database connection open, remember prior queries, cache results, etc.). This opens the door to more complex, multi-turn interactions with tools.</p> <p>For instance, an MCP server could handle a session with an AI agent, allowing the agent to iteratively query data, refine results, or perform a sequence of actions with continuity. The client-server architecture means there's an ongoing \"conversation\" between the AI (client) and the tool backend (server), rather than just isolated function calls. Additionally, MCP's model of Resources (described later) allows large data to be loaded or referenced as needed, instead of stuffing everything into a single prompt window. In short, MCP is built for long-lived, context-rich interactions with external systems, unlike the transient function calls of simpler APIs.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#safety-and-permissions","title":"Safety and Permissions","text":"<p>With basic function calling, developers must implement any approval or safety checks themselves in the loop. MCP formalizes some of this. By design, MCP assumes a human or host application is in the loop for potentially risky actions \u2013 for example, tools calls are intended to be approved by a user, and the protocol supports permissioning and authentication at a systemic level. The updated MCP spec even includes standardized OAuth 2.1 flows for granting access to protected resources. This level of built-in security and consent is beyond what the raw function-call interfaces provide, making MCP better suited for enterprise and sensitive integrations.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#multi-model-flexibility","title":"Multi-Model Flexibility","text":"<p>Because MCP decouples tools from any specific model, it gives developers flexibility to switch LLM providers or models without redoing integrations. For example, an application could use Anthropic's Claude today and swap to another MCP-compatible model tomorrow, and all the same MCP servers (tools) would still work. In contrast, OpenAI's function calling is tightly coupled to using OpenAI's models; there's no guarantee those function definitions would port to Google's PaLM or others. MCP's standardized interface acts as a neutral layer between models and tools.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#interaction-flow-traditional-function-call-vs-mcp","title":"Interaction Flow: Traditional Function Call vs. MCP","text":"<p>To illustrate the difference, let's compare the flow of an AI using an external weather API via traditional function-calling versus using MCP:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#function-calling-workflow-eg-openai-api","title":"Function-Calling Workflow (e.g. OpenAI API)","text":"<pre><code>sequenceDiagram\n    participant User\n    participant LLM_API as LLM (with function-calling)\n    participant DevApp as Developer App\n    participant WeatherAPI as Weather API\n    User-&gt;&gt;LLM_API: \"What is the weather in LA tomorrow?\"\n    note right of LLM_API: LLM decides a function is needed&lt;br/&gt;from provided definitions\n    LLM_API--&gt;&gt;DevApp: Function call output (e.g. `get_weather(location=\"LA\")`)\n    DevApp-&gt;&gt;WeatherAPI: Invoke weather API (with location=\"LA\")\n    WeatherAPI--&gt;&gt;DevApp: Returns weather data\n    DevApp--&gt;&gt;LLM_API: Provide function result (weather data)\n    LLM_API--&gt;&gt;User: Final answer using the data</code></pre> <p>In the above function-calling flow, the developer had to predefine a get_weather function in the prompt and intercept the model's output to call the API. The interaction with the external service (Weather API) is not standardized \u2013 it's custom code in the developer's app, and the connection is not persistent (just a single call). The LLM itself doesn't maintain a direct connection to tools; it relies on the developer to mediate every call.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#mcp-workflow-clientserver-with-weather-tool","title":"MCP Workflow (Client\u2013Server with Weather Tool)","text":"<pre><code>sequenceDiagram\n    participant User\n    participant AI_Client as AI App (MCP Client)\n    participant MCPServer as Weather MCP Server\n    participant WeatherAPI as Weather API\n    User-&gt;&gt;AI_Client: \"What is the weather in LA tomorrow?\"\n    AI_Client-&gt;&gt;MCPServer: (via MCP) Request tool \"get-forecast\" with {\"location\":\"LA\"}\n    MCPServer-&gt;&gt;WeatherAPI: Fetch forecast for LA\n    WeatherAPI--&gt;&gt;MCPServer: Weather data\n    MCPServer--&gt;&gt;AI_Client: Result of get-forecast (data or summary)\n    AI_Client--&gt;&gt;User: Final answer incorporating weather info</code></pre> <p>In the MCP flow, the AI application (client) has an established connection to a Weather MCP server that exposes a get-forecast tool. The LLM (e.g. Claude or ChatGPT with MCP support) can directly trigger that tool via the MCP client, without the developer writing custom code for this API call. The MCP server handles communicating with the Weather API and returns the result in a standardized format. The AI model receives the weather info through MCP and can respond to the user.</p> <p>Notably, this happens through a consistent protocol \u2013 any other MCP-compatible weather service would work the same way. Also, the connection can be two-way: if the Weather server had multiple actions or needed additional queries, the conversation over MCP could continue beyond a one-shot call. This demonstrates how MCP generalizes and extends the idea of function calling into a full client\u2013server integration pattern.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#core-components-of-mcp","title":"Core Components of MCP","text":"<p>MCP defines a set of core components (primitives) that structure how clients and servers interact and what they can do. There are three server-side primitives (which servers provide) and two client-side primitives (which clients provide). Understanding these components is key to using MCP effectively:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#server-side-primitives","title":"Server-Side Primitives","text":"<p>These are capabilities that an MCP Server exposes to add context or functionality for the LLM. An MCP server can implement any or all of these:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#resources","title":"Resources","text":"<p>Resources are pieces of data or content that the server makes available for the AI to read and include in its context. A resource might be a file's contents, a database record, an email, an image, etc. \u2013 anything that could be useful as additional information for the model. Each resource is identified by a URI (like file://docs/report.pdf or db://customers/123) and can be fetched via the protocol.</p> <p>Importantly, resources are typically read-only context: the AI doesn't execute them, it uses them as reference material. For example, a \"Docs Server\" might expose a document's text as a resource so the AI can quote or summarize it. Resources are usually application-controlled, meaning the client or user decides which resources to pull in (to avoid flooding the model with irrelevant data). In practice, an AI interface might let a user pick a file (resource) to share with the assistant. If truly on-demand model-driven access is needed instead, that's where tools come in. Resources help inject structured data into the LLM's prompt context in a standardized way.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#tools","title":"Tools","text":"<p>Tools are executable functions that the server can perform on request. These are analogous to the \"functions\" in function-calling, but defined in a standard JSON schema format and invoked via MCP endpoints. Tools allow the AI to perform actions or fetch calculated information \u2013 for example, run a database query, call an external API, execute a computation, or even control a web browser.</p> <p>Each tool has a name, a description, and a JSON schema for its input parameters. The server lists available tools, and the AI (through the client) can call them by name with the required args. Tools are model-controlled with a human in the loop. This means the design assumes the AI can decide when to use a tool (e.g. the LLM's reasoning says \"I should use the database_query tool now\"), but the user or client must approve the action for safety.</p> <p>When invoked, the server executes the underlying function and returns the result (or error) to the client. Tools can be simple (e.g. a calculator) or very powerful (e.g. a tool that can send an email or modify data). Unlike resources, tools can have side effects or dynamic outputs \u2013 they may change external state or retrieve live data. This makes them essential for building AI agents that act, not just observe.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#prompts","title":"Prompts","text":"<p>Prompts in MCP are reusable prompt templates or workflows that servers provide. A Prompt primitive is essentially a predefined way to interact with the model, which could involve multiple steps or a structured input. Servers define prompts to standardize common interactions \u2013 for example, a prompt template to \"Summarize document X in style Y\", or a multi-turn workflow to \"Debug an error by asking these follow-up questions\".</p> <p>Each prompt is identified by a name and can accept input arguments (e.g. the document to summarize, the style to use). The client can query the server for available prompts, which might be presented to the user (for instance, an IDE could show a list of MCP prompt actions like \"Explain this code\" powered by the server). When a prompt is selected, the server can then guide the interaction: it might inject certain instructions to the model, include relevant resources automatically, or even orchestrate a chain of LLM calls.</p> <p>Prompts are typically user-controlled, meaning the user explicitly triggers those workflows (like choosing a predefined query or action). Under the hood, a prompt might use the other primitives \u2013 e.g. it could fetch some resource or call a tool as part of its process \u2013 to produce the final result. Prompts let developers encapsulate complex behaviors or multi-step conversations behind a single command, making them easy to reuse.</p> <p>In summary, on the server side: Resources = data context, Tools = actions/functions, and Prompts = preset conversational patterns. These primitives \"speak\" JSON-RPC \u2013 e.g. there are standard methods like resources/list, resources/read, tools/list, tools/call, prompts/list, etc., which the client can call to discover and use these primitives. By separating them, MCP makes clear what the AI is intending to do \u2013 whether it's just reading data, executing a function, or following a guided script, each has a defined protocol.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#client-side-primitives","title":"Client-Side Primitives","text":"<p>These are features that an MCP Client (host application) provides, which servers can leverage. There are two main client-side primitives:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#roots","title":"Roots","text":"<p>A Root is essentially a boundary or entry point that the client suggests to the server for where to focus. When an MCP connection starts, the client can send one or more \"root URIs\" to the server. This informs the server about the relevant workspace or scope.</p> <p>For instance, if a developer is working in /home/user/myproject/, the client might set that as a root for a Filesystem server \u2013 indicating the server should consider that directory as the project context (and not roam outside it). Or a root could be a specific URL or database name that the server should use as the primary endpoint. Roots thus provide guidance and organization, especially when multiple data sources are in use.</p> <p>They do not hard-enforce access limitations (the server could technically go beyond, unless the server itself restricts it), but they serve as a contract of what the current context is. This helps keep interactions focused and secure \u2013 the server knows what subset of data it should operate on, and the client knows the server won't unexpectedly access unrelated data. Not all scenarios require roots, but they are very useful in development tools (for setting project scope) and similar contexts.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#sampling","title":"Sampling","text":"<p>Sampling is a powerful client-side feature that allows the server to ask the client's LLM to generate a completion. In simpler terms, it lets the server turn around and say: \"I need the AI to complete/answer this sub-task for me.\" This might sound unusual, but it enables advanced workflows like agents that can reason recursively.</p> <p>For example, imagine a complex tool that, in the middle of its function, realizes it needs an LLM's help to parse something or make a decision \u2013 the server can send a sampling request to the client, providing a prompt, and the client will invoke the LLM to get a result, then return it to the server. The server can then proceed using that result. All of this happens under the hood of the protocol, with the crucial caveat that the user (or host application) should approve any such additional LLM call.</p> <p>The design is meant for \"agents calling themselves,\" but with human oversight to avoid infinite loops or undesired actions. Sampling requests include a formatted message (or conversation) that the server wants the model to continue, and possibly preferences for which model or how to balance speed vs. accuracy.</p> <p>This feature essentially lets the server compose intelligence: an MCP server could chain multiple LLM calls or do intermediate reasoning by leveraging the client's model. It's an advanced capability (not all clients support it yet, and it should be used judiciously), but it opens the door to sophisticated agent behaviors. With Sampling, MCP isn't just one LLM interacting with tools \u2013 it could be tools temporarily invoking the LLM in return, enabling nested AI reasoning.</p> <p>These five components \u2013 Resources, Tools, Prompts, Roots, and Sampling \u2013 form the core of MCP's design. The protocol is essentially a set of JSON-RPC methods that implement these primitives in a standardized way. By mixing and matching them, one can achieve various integration patterns. For example, a given MCP server might only implement Tools (if it purely offers actions like a Calculator server), or implement Resources + Prompts (if it mainly offers data and some templated queries on that data), etc. Clients similarly may or may not support Sampling or Roots depending on the application. The upshot is that MCP gives a common structure to describe \"what an AI can do\" in a given context: read this data, use these tools, follow these templates.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#architecture-clientserver-design","title":"Architecture: Client\u2013Server Design","text":"<p>At a high level, MCP follows a classic client\u2013server architecture tailored to AI needs. The design involves a Host application that incorporates an MCP client, which connects to one or more external MCP servers. Let's break down the pieces:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#mcp-host-and-client","title":"MCP Host and Client","text":"<p>The host is the main AI application or agent environment. For example, Claude Desktop (Anthropic's chat app) is an MCP host, as are certain IDE extensions, chat UIs, or agent orchestration frameworks. The host contains the MCP client component, which is responsible for managing the connection to servers and mediating between the LLM and those servers.</p> <p>The client knows how to speak MCP (it implements the protocol on the client side: sending requests like tools/call to servers, and handling incoming requests like sampling/createMessage from servers). It maintains a 1:1 connection with each server it uses. In practice, a host might spawn multiple client connections if it's using multiple servers at once (for example, connect to a GitHub server and a Slack server concurrently).</p> <p>The LLM itself (the model) is usually integrated into the host \u2013 for instance, Claude (the model) running locally or via API is what generates the responses, and the MCP client is feeding it information and tool results.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#mcp-server","title":"MCP Server","text":"<p>An MCP server is a lightweight program or process that exposes a specific set of capabilities (the primitives we discussed) via the MCP protocol. Each server typically corresponds to a particular domain or service. For example, one server might interface with a file system (exposing files as resources, and perhaps a prompt for searching files), another might interface with Google Drive, another with GitHub, a database, or a web browser, etc.</p> <p>The server runs separately from the AI model \u2013 it could be on the same machine or a remote one \u2013 and communicates with the client over a communication channel. Servers declare what capabilities they have during initialization (for instance, \"I support resources and tools, but not prompts\") so the client knows how to interact.</p> <p>Importantly, servers are stateless with respect to the protocol (they handle requests as they come) but can maintain internal state or connections (e.g. keep a DB session). They do not directly talk to the LLM; instead, all interaction is through the structured protocol messages.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#communication-transports-json-rpc","title":"Communication (Transports &amp; JSON-RPC)","text":"<p>MCP's communication is built on JSON-RPC 2.0 as the message format. This means every action (like listing tools or calling a resource) is a JSON request with a method name and params, and responses are JSON objects with results or errors. The protocol supports both requests (with responses) and notifications (one-way messages) in both directions \u2013 so the server can call methods on the client (like sampling/createMessage) and the client can call methods on the server (like tools/call).</p> <p>The underlying transport layer can vary. Two common transport modes are:</p> <p>Stdio (Standard I/O): Ideal for local setups, the client can launch the server as a subprocess and communicate via its stdin/stdout streams. This is simple and secure for local integrations \u2013 e.g. Claude Desktop can run an MCP server on your machine via stdio.</p> <p>HTTP (with SSE or WebSocket): For remote servers, MCP can run over HTTP. Initially, the spec used HTTP + Server-Sent Events (SSE) for server-to-client streaming, but an updated streamable HTTP transport was introduced (Mar 2025) to allow full bidirectional streaming over a single connection. In either case, the client might make HTTP POST requests for each JSON-RPC call and keep a channel open for streaming responses. WebSocket is another possible transport for continuous two-way communication.</p> <p>The transport is abstracted in MCP \u2013 developers don't usually worry about it beyond choosing one that fits their deployment. The key is that client and server establish a connection and exchange JSON-RPC messages. The first messages in a session are an initial handshake: the client sends an initialize request (with its MCP version and what features it supports, e.g. \"I support roots and sampling\") and the server responds with its own capabilities and version. Once initialization is done, they exchange an initialized notification and then the session is ready for general use.</p> <p>Either side can then send requests or notifications according to the protocol. For example, right after init, the client might call prompts/list or tools/list to discover what the server offers. From then on, the LLM (through the client) can start using those tools/resources.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#lifecycle","title":"Lifecycle","text":"<p>The MCP connection stays alive as long as needed. If the user closes the host app or a certain task is done, the client can send a shutdown message or simply disconnect the transport, ending the session. Servers and clients are expected to handle disconnects or errors gracefully (MCP defines standard error codes for invalid requests, internal errors, etc., similar to JSON-RPC's spec).</p> <p>Below is a diagram of the core MCP architecture with its main components and data flow:</p> <pre><code>flowchart LR\n    subgraph AI_Host[Host Application]\n        direction TB\n        AILLM[\"LLM (AI Assistant)\"]\n        MCPClient[\"MCP Client Component\"]\n        AILLM -- uses --&gt; MCPClient\n    end\n    subgraph MCP_Server1[\"MCP Server: Tool/Data Source 1\"]\n        direction TB\n        Tools1[\"Tools\"]\n        Resources1[\"Resources\"]\n        Prompts1[\"Prompts\"]\n    end\n    subgraph MCP_Server2[\"MCP Server: Tool/Data Source 2\"]\n        direction TB\n        Tools2[\"Tools\"]\n        Resources2[\"Resources\"]\n        %% Prompts2 not implemented in this one for illustration\n    end\n    %% Host connects to multiple servers\n    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server1\n    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server2\n    Resources1 -- fetch data --&gt; DataSource1[\"Local/Remote Data\\n(e.g. files, DB)\"]\n    Tools1 -- perform actions --&gt; DataSource1\n    Tools2 -- perform actions --&gt; Service2[\"External Service API\"]\n    Resources2 -- fetch data --&gt; Service2</code></pre> <p>In this diagram, the Host application (which contains the AI model and the MCP client library) maintains connections to two MCP servers. Server1 might be a local server (e.g. providing file system access \u2013 hence its tools/resources tie into local files or a database), and Server2 might be a remote service integration (e.g. a weather API server, which calls an external web service). The MCP client handles the JSON-RPC messaging to each server.</p> <p>When the LLM needs something, it formulates a request that the client sends to the appropriate server; when a server needs something (like performing a sampling request or notifying of new data), it sends that to the client. The primitives (Tools, Resources, Prompts) inside each server define what it can do.</p> <p>For example, if the user asks the AI \"Summarize the latest sales data\", the AI client might use a Prompts primitive from Server1 that knows how to retrieve sales records (as Resources) and then guide the model to summarize them. Meanwhile, if the user asks \"Also, what's the weather in LA?\", the AI client can call a Tool on Server2 to get that info. All of this happens through the uniform MCP interface.</p> <p>As the ecosystem grows, we might have dozens of servers (for Git, Jira, Gmail, etc.), but crucially, an AI app doesn't need to have custom code for each \u2013 it just speaks MCP to whichever servers are available.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#standardization","title":"Standardization","text":"<p>Because both client and server follow the MCP spec, any compatible client can talk to any server. This fosters an ecosystem where companies and the community are building a \"library\" of MCP servers for many common tools. Anthropic has open-sourced reference servers for Google Drive, Slack, Git, GitHub, Postgres, web browser automation, and more. Community contributors have added many others (at the time of writing, hundreds of MCP connectors exist for various services).</p> <p>On the client side, multiple products have added MCP support (Claude's apps, developer IDE plugins, agent frameworks like LangChain, etc.). This architecture and standardization are what turn MCP into that \"universal port\" for AI \u2013 it decouples the integration logic (server side) from the AI agent logic (client side), with a well-defined protocol in between.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#python-tutorial-building-and-using-mcp-step-by-step","title":"Python Tutorial: Building and Using MCP (Step-by-Step)","text":"<p>Now that we've covered concepts, let's get hands-on with a Python tutorial using MCP. We'll walk through creating a simple MCP server and demonstrate a client interacting with it. For this tutorial, we'll use the official MCP Python SDK (open-sourced by Anthropic) for convenience. (The SDK abstracts a lot of JSON-RPC boilerplate and lets us focus on defining our tools/resources). If you haven't already, you can install the SDK via pip:</p> <pre><code>pip install mcp\n</code></pre> <p>(The package name is mcp. This includes the server framework and some CLI tools.) Let's proceed in steps:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#1-setting-up-a-basic-mcp-server","title":"1. Setting Up a Basic MCP Server","text":"<p>First, we'll create a minimal MCP server script in Python. This server will expose a trivial functionality just to verify everything works. We'll use the SDK's FastMCP class to create a server instance and run it.</p> <pre><code># server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize an MCP server with a name\nmcp = FastMCP(\"Demo Server\")\n\n# Start the server (using stdio transport by default).\n# This call will block and wait for a client connection.\nmcp.serve()\n</code></pre> <p>In this snippet, we import FastMCP and instantiate it with a name \"Demo Server\". The FastMCP class is a high-level server that handles MCP protocol compliance and transport setup for us. By calling mcp.serve(), we tell it to begin listening for an MCP client. By default this uses the standard I/O transport (suitable if this script is launched by a host app). At this point, our server doesn't actually expose any tools or resources yet \u2013 it's an empty shell. But it can respond to basic protocol requests like initialize and will report that it has no capabilities.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#2-exposing-a-tool-function-on-the-server","title":"2. Exposing a Tool (Function) on the Server","text":"<p>Now we'll add a simple Tool to our server. The SDK provides a convenient decorator @mcp.tool to turn a Python function into an MCP tool. For demonstration, we'll add a basic arithmetic tool and then run the server.</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Demo Server\")\n\n# Define a tool using a decorator. This tool adds two numbers.\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers and return the result.\"\"\"\n    return a + b\n\nmcp.serve()\n</code></pre> <p>With the @mcp.tool() decorator, the SDK automatically registers our add function as a Tool primitive. Under the hood, it will assign it a name (defaulting to the function name \"add\" here) and generate a JSON schema for the input parameters a and b (both integers, in this case). It also captures the docstring as the tool's description, which is useful for the AI to understand what it does.</p> <p>When this server is running, an MCP client that connects can call tools/list and will see something like:</p> <pre><code>{\n  \"tools\": [\n    {\n      \"name\": \"add\",\n      \"description\": \"Add two numbers and return the result.\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"a\": {\"type\": \"integer\"},\n          \"b\": {\"type\": \"integer\"}\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>This is how the client discovers what functions are available. The inputSchema is derived from our function signature (using type hints, or it would default to accepting arbitrary JSON structure if not annotated). Now the add tool can be invoked via the MCP method tools/call with appropriate parameters.</p> <p>Let's test our server's tool quickly (in a real scenario, we'd connect an actual client, but here we'll simulate what a client would do for illustration). We can write a tiny MCP client snippet or use the SDK's CLI. For simplicity, imagine the client calls:</p> <pre><code># Pseudo-client code (for illustration only)\nfrom mcp import Client  # assume the SDK has a Client class for connecting\n\nclient = Client.connect_stdio(\"./server.py\")   # Launch server.py and connect via stdio\ntools = client.list_tools()                    # should list the \"add\" tool\nresult = client.call_tool(\"add\", {\"a\": 3, \"b\": 4})\nprint(result)  # expected output: 7\nclient.disconnect()\n</code></pre> <p>In reality, one might use the mcp CLI tool: for example, running mcp dev server.py will start the server and open an interactive interface (MCP Inspector) where you can manually invoke tools and see the results. Or, if using Claude Desktop, one could configure it to run this server and then simply ask Claude, \"What's 3+4?\" \u2013 the model could decide to use the add tool and would get the answer 7 from the server.</p> <p>The key takeaway is that adding a tool is as simple as defining a Python function with the SDK; MCP takes care of exposing it in a standardized way.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#3-exposing-a-resource-data-on-the-server","title":"3. Exposing a Resource (Data) on the Server","text":"<p>Let's extend our server to also provide a Resource. We'll make a resource that gives a friendly greeting. This will demonstrate how resources can supply text for the model's context. We use the @mcp.resource(\"uri_pattern\") decorator to register a resource handler.</p> <pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Demo Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers and return the result.\"\"\"\n    return a + b\n\n# Define a resource. The URI pattern {name} means the client can request e.g. \"greet://Alice\"\n@mcp.resource(\"greet://{name}\")\ndef get_greeting(name: str) -&gt; str:\n    \"\"\"Resource content: returns a greeting for the given name.\"\"\"\n    return f\"Hello, {name}! Welcome to MCP.\"\n\nmcp.serve()\n</code></pre> <p>Here we used @mcp.resource(\"greet://{name}\"). This tells the server that it can handle any resource URI of the form greet:// \u2013 the  will be passed as the name argument to our function get_greeting. So if the client later requests the resource greet://Alice, our function returns \"Hello, Alice! Welcome to MCP.\". The server will transmit that string as the content of the resource. We provided a description in the docstring as well. <p>The client's perspective: if it calls resources/list, it might not list all possible greetings (since {name} is a parameterized resource), but some servers do list resource \"directories\" or examples. In any case, the client can directly do a resources/read on a specific URI. For example, an AI client could be prompted (via system message) that greet://{name} resources are available for use. If the user asks \"Can you greet Alice?\", the AI could fetch greet://Alice from the server (the client would call resources/read with uri: \"greet://Alice\"). The server invokes get_greeting(\"Alice\") and returns the text. The LLM then sees that text and can respond to the user with it.</p> <p>Resources are a nice way to serve static or computed data that the model might include in its answer verbatim. They behave somewhat like HTTP GET endpoints (in REST analogy).</p> <p>Now our Demo Server has both a Tool (add) and a Resource (greet://{name}).</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#4-adding-a-prompt-reusable-template","title":"4. Adding a Prompt (Reusable Template)","text":"<p>For completeness, let's add a Prompt to our server. Suppose we want a standardized way for the AI to request a calculation and explanation. We'll create a prompt that, given a math problem, instructs the model to solve it step-by-step.</p> <pre><code>from mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base  # utility for prompt message objects\n\nmcp = FastMCP(\"Demo Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers and return the result.\"\"\"\n    return a + b\n\n@mcp.resource(\"greet://{name}\")\ndef get_greeting(name: str) -&gt; str:\n    \"\"\"Resource content: returns a greeting for the given name.\"\"\"\n    return f\"Hello, {name}! Welcome to MCP.\"\n\n@mcp.prompt()\ndef solve_math(problem: str) -&gt; list[base.Message]:\n    \"\"\"Prompt: guides the model to solve a math problem.\"\"\"\n    # This returns a conversation (list of messages) as a template\n    return [\n        base.SystemMessage(\"You are a helpful math assistant.\"),\n        base.UserMessage(f\"Please solve the following problem: {problem}\"),\n        base.AssistantMessage(\"Sure, let me break it down...\"),\n    ]\n\nmcp.serve()\n</code></pre> <p>We used @mcp.prompt() to define solve_math. In this case, our function returns a list of message objects (using the SDK's base.SystemMessage, base.UserMessage, etc., which likely correspond to the roles in a chat prompt). This defines a prompt template where the assistant is primed with a system role and an initial user request, and even an initial assistant response to indicate it will explain step-by-step. The specifics aren't too important \u2013 the idea is that the server can package this multi-turn template and offer it as a named prompt.</p> <p>The client will see this if it calls prompts/list:</p> <pre><code>{\n  \"prompts\": [\n    {\n      \"name\": \"solve_math\",\n      \"description\": \"Prompt: guides the model to solve a math problem.\",\n      \"arguments\": [\n        {\n          \"name\": \"problem\",\n          \"description\": \"\",\n          \"required\": true\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>The client (or user) can then invoke this prompt. For instance, a UI might show an option \"Solve a math problem\" which triggers prompts/execute (or a similar method) on the server with name: \"solve_math\" and problem: \"2+2*5\" as an argument. The server would return the prepared messages; the client would feed those to the LLM, and the LLM would continue the conversation from that context (producing a detailed solution). Essentially, the prompt primitive allows packaging expert instructions or workflows so the model can perform them on demand.</p> <p>We now have a server that demonstrates all three server-side primitives: - A Tool (add) - A Resource (greet://{name}) - A Prompt (solve_math)</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#5-running-the-server-and-client-interaction","title":"5. Running the Server and Client Interaction","text":"<p>To test our MCP server, we need an MCP client. In real-world usage, the \"client\" might be a GUI application (Claude Desktop) or an agent loop. Here, we can simulate a simple client or use the CLI. For brevity, let's illustrate using the CLI approach to interact with our server:</p> <pre><code># In one terminal, run the server (it will wait for client connection)\npython server.py\n</code></pre> <p>Now, in another terminal, we can use the mcp CLI (installed with the SDK) to connect:</p> <pre><code>mcp dev server.py\n</code></pre> <p>This should connect to the running server (or launch it if not already) and drop us into an interactive MCP Inspector. We could then try: - Listing tools: this should show the add tool. - Calling a tool: e.g. call add with {\"a\":5,\"b\":7} and expect result 12. - Reading a resource: e.g. request resource URI greet://Alice and see the text. - Executing a prompt: e.g. run prompt solve_math with argument \"12*3-4\" and watch the assistant's step-by-step solution.</p> <p>If all goes well, the server will log or display calls as it handles them. The SDK likely prints logs for each request. You'd see that our Python functions are triggered accordingly.</p> <p>For a programmatic approach, one could also use the Python SDK's client classes to do the same. For example:</p> <pre><code># Using Python to connect via stdio (hypothetical example)\nfrom mcp.client import StdioClientTransport, Client\n\n# Start the server process and connect transport (pseudo-code)\ntransport = StdioClientTransport(\"./server.py\")  # launch our server script\nclient = Client(transport)\n\n# Initialize the MCP connection\nclient.initialize()  # sends initialize handshake\n\n# List and call the 'add' tool\ntools = client.request(\"tools/list\", {})\nprint(tools)  # should include 'add'\nresult = client.request(\"tools/call\", {\"tool\": \"add\", \"params\": {\"a\": 2, \"b\": 3}})\nprint(result)  # should output {'result': 5}\n\nclient.close()\n</code></pre> <p>The actual API may differ, but the concept is that the client sends JSON-RPC requests like {\"method\": \"tools/call\", \"params\": {\"tool\": \"add\", \"params\": {\"a\":2,\"b\":3}}} and receives a response {\"result\": 5}. The SDK's Client class would wrap this in a nicer interface.</p> <p>Through this step-by-step exercise, we saw how to implement an MCP server in Python with minimal effort (just defining functions with decorators) and how a client might interact. In a real deployment, you might build much more complex servers \u2013 connecting to real databases, handling authentication, streaming large resources, etc. \u2013 but the pattern remains the same. Anthropic's SDK and spec provides a lot of guidance and examples for building robust servers (including lifecycle management, dependency injection, error handling, etc., as shown in their docs).</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#advanced-use-cases-and-workflows","title":"Advanced Use Cases and Workflows","text":"<p>With the fundamentals in place, let's discuss some advanced use cases of MCP and how it fits into larger AI systems:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#handling-complex-state-and-workflows","title":"Handling Complex State and Workflows","text":"<p>One powerful aspect of MCP's design is that a server can maintain and manage complex state over the course of a session. For example, consider an MCP server that interfaces with a user's email. Such a server might need to handle authentication, caching of email headers, incremental fetching of emails, etc.</p> <p>Using MCP, the server could manage an OAuth login flow (especially with the new OAuth 2.1 support in the protocol for secure client-server auth), store the user's token, and reuse it for subsequent requests \u2013 all internally. The AI client just calls high-level tools like list_emails or send_email, and the server handles the gritty details with its state.</p> <p>Another scenario is maintaining conversational or task state. Suppose you have an agent working on a coding task using multiple tools: a Git server, a documentation server, etc. The agent might ask the Git server to open a file (as a resource), edit it (via a tool), then ask for a diff. The Git MCP server could keep track of an open repo and the last opened file across those calls, rather than requiring the AI to specify everything each time.</p> <p>This is facilitated by MCP's session \u2013 since the connection is persistent, the server can hold onto objects in memory (like a database connection or a file handle) between calls. In our earlier Python example, the SDK even showed a concept of a lifespan context where you can initialize resources at server startup (e.g. connect to a DB) and clean up on shutdown. That means complex state (like a database pool or an in-memory cache) can be established once and reused, improving efficiency and making the server more capable.</p> <p>Chaining and Multi-step workflows: Using prompts and sampling, servers can also coordinate multi-step processes. For instance, a single MCP prompt might involve the server first fetching some data (resource), then formulating a series of messages (prompt template), possibly even calling an internal model (sampling) to pre-analyze something, and finally returning a polished prompt for the client's LLM to answer. This is a form of agent orchestration that can be hidden behind a single MCP call. Essentially, the server can implement a mini-agent or state machine, but expose it as a simple interface to the client. The separation of concerns here is neat: the server handles procedure and state, the client's LLM handles language generation.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#integration-with-agent-frameworks","title":"Integration with Agent Frameworks","text":"<p>MCP is quickly being adopted by AI agent frameworks and libraries, which is a testament to its flexibility. One notable integration is with LangChain, a popular framework for chaining LLM calls and tools. The LangChain team and community have built adapters to use MCP servers as LangChain tools.</p> <p>For example, rather than writing a custom tool wrapper for every API in LangChain, you can now spin up an MCP server (or use an existing one) and then use a converter that turns all of that server's Tools into LangChain Tool objects. This means LangChain-powered agents can immediately gain access to the huge library of MCP connectors (there are MCP servers for many services).</p> <p>Conversely, tools defined within LangChain could potentially be exposed via an MCP server to other systems. Beyond LangChain, other agent frameworks and IDE assistants are on board. The \"fast-agent\" project, for instance, is an MCP client that supports all features including Sampling and can coordinate multi-modal inputs.</p> <p>Developer tools like Cursor editor and Continue (VS Code extension) have added MCP support so that code assistants can use external tools uniformly. Even Microsoft's Guidance / Autogen frameworks and OpenAI's new Agents SDK are aligning with these standards.</p> <p>The fact that OpenAI itself announced MCP support in its Agents SDK shows that even proprietary agent ecosystems see value in interoperability. What does this mean for a developer? If you're building an AI agent, you could use MCP as the backbone for tool usage. Instead of hardcoding how to call each API, you let the agent discover available MCP servers and use them. The agent can remain focused on planning and high-level reasoning, delegating actual tool execution to MCP servers.</p> <p>This leads to more modular agent design \u2013 you can mix and match capabilities by just running different servers. Want to add calendar access to your agent? Just run a Calendar MCP server and connect it; no need to retrain the model or rewrite logic, because the model will see the new \"calendar tool\" via MCP and can start using it (assuming it's been instructed appropriately). This plug-and-play nature fits nicely with the vision of agents that can expand their toolsets dynamically.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#retrieval-augmented-generation-rag-with-mcp","title":"Retrieval-Augmented Generation (RAG) with MCP","text":"<p>Retrieval-Augmented Generation refers to the workflow where an LLM fetches relevant external information (from a document corpus or knowledge base) to augment its responses. MCP is an excellent vehicle for implementing RAG in a standardized way. Instead of custom retrieval code, one can create an MCP server that encapsulates the retrieval logic.</p> <p>For example, imagine you have a vector database of documents or an ElasticSearch index. You could build an MCP server KnowledgeBaseServer with a tool search(query: str) -&gt; list[str] (or which returns resource URIs). The AI model, when asked a question, can call this search tool via MCP to get relevant text snippets (as either direct text results or as resource references that it then reads). Those snippets then become part of the context for answering the question. Because MCP can handle streaming and large data via resources, even bigger documents can be fetched in chunks as needed.</p> <p>The advantage is consistency and reuse: any MCP-aware AI app could use that same server for retrieval. In fact, Anthropic's documentation includes a quickstart where Claude uses an MCP server to fetch weather info \u2013 which is essentially a form of retrieval (getting factual data) before answering. The pattern extends to any knowledge domain.</p> <p>Let's sketch an advanced RAG flow with MCP in a sequence diagram for clarity:</p> <pre><code>sequenceDiagram\n    participant User\n    participant AI_Client as AI (MCP Client)\n    participant KB_Server as KnowledgeBase MCP Server\n    participant DataStore as Document DB / Index\n    User-&gt;&gt;AI_Client: \"What were the key points of Project X's design?\"\n    AI_Client-&gt;&gt;KB_Server: tools/call: search({\"query\": \"Project X design key points\"})\n    KB_Server-&gt;&gt;DataStore: Perform semantic search for \"Project X design key points\"\n    DataStore--&gt;&gt;KB_Server: Returns top relevant doc snippets\n    Note over KB_Server: e.g. returns a Resource list or text results\n    KB_Server--&gt;&gt;AI_Client: Search results (as resources or text)\n    AI_Client-&gt;&gt;AI_Client: (LLM incorporates results into its prompt)\n    AI_Client--&gt;&gt;User: \"The key design points of Project X are ... (with details from docs)\"</code></pre> <p>In this RAG scenario, the heavy lifting of retrieval is done by the KB_Server. The AI client just knows that when it needs info, it can call the search tool. This decoupling means you could swap out the implementation (maybe use a different database or algorithm) by switching the server, without changing the AI's approach. Moreover, because MCP is model-agnostic, even if you used a different LLM tomorrow, it could perform the same retrieval process by using the server.</p> <p>MCP also helps maintain conversation flow in RAG. Traditional retrieval might not keep state of what's already fetched, but an MCP server could track which documents have been shown to the model (to avoid repetition or to follow up with more details as needed). It could also expose each document as a Resource with a URI, so that the model can say \"let me read resource doc://123\" in parts. All of this can be done with a clear protocol, rather than custom ad-hoc methods.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#rag-implementation-strategies-with-mcp","title":"RAG Implementation Strategies with MCP","text":"<p>There are several approaches to implementing RAG with MCP, each with its own advantages:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#1-tool-based-retrieval","title":"1. Tool-based Retrieval","text":"<p>The simplest approach is to expose a search tool that returns relevant content directly. The model asks a question, calls the search tool with appropriate keywords, and gets back content in the tool response:</p> <pre><code>@mcp.tool()\ndef search_knowledge_base(query: str, max_results: int = 3) -&gt; list[dict]:\n    \"\"\"Search the knowledge base for documents matching the query.\"\"\"\n    # Search logic using vector DB, keyword search, etc.\n    # Each result has the document content and metadata\n    return [\n        {\"content\": \"...\", \"source\": \"doc1.pdf\", \"relevance\": 0.92},\n        {\"content\": \"...\", \"source\": \"doc2.pdf\", \"relevance\": 0.87},\n        # ...\n    ]\n</code></pre> <p>This approach is straightforward but may have limitations with token context windows if the search returns large amounts of text.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#2-resource-based-retrieval","title":"2. Resource-based Retrieval","text":"<p>A more flexible approach is to expose documents as resources, so the model can request them by URI:</p> <pre><code>@mcp.tool()\ndef search_documents(query: str, max_results: int = 5) -&gt; list[str]:\n    \"\"\"Search documents and return resource URIs of relevant documents.\"\"\"\n    # Search logic using vector DB, etc.\n    # Return URIs instead of content\n    return [\"doc://123\", \"doc://456\", \"doc://789\"]\n\n@mcp.resource(\"doc://{id}\")\ndef get_document(id: str) -&gt; str:\n    \"\"\"Get the content of a document by ID.\"\"\"\n    # Retrieve document with the given ID\n    return \"Document content goes here...\"\n</code></pre> <p>This approach lets the model decide which documents to actually fetch after seeing search results, and it can request document content in chunks if needed.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#3-hybrid-chunking-and-streaming","title":"3. Hybrid: Chunking and Streaming","text":"<p>For very large documents, an MCP server can implement chunking and pagination:</p> <pre><code>@mcp.resource(\"doc://{id}/chunk/{chunk_number}\")\ndef get_document_chunk(id: str, chunk_number: int) -&gt; str:\n    \"\"\"Get a specific chunk of a document.\"\"\"\n    # Logic to fetch and return only that chunk\n    return f\"Content of chunk {chunk_number} from doc {id}\"\n\n@mcp.tool()\ndef get_document_metadata(id: str) -&gt; dict:\n    \"\"\"Get metadata about a document, including total chunks.\"\"\"\n    return {\n        \"title\": \"Example Document\",\n        \"total_chunks\": 15,\n        \"summary\": \"A brief overview of the document...\"\n    }\n</code></pre> <p>This allows the model to navigate large documents efficiently, requesting only the most relevant portions.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#4-semantic-routing","title":"4. Semantic Routing","text":"<p>An advanced MCP server could even implement semantic routing of queries to the right knowledge source:</p> <pre><code>@mcp.tool()\ndef route_query(query: str) -&gt; dict:\n    \"\"\"Route a query to the appropriate knowledge source.\"\"\"\n    # Analyze the query and determine which source to use\n    if \"financial\" in query.lower():\n        return {\n            \"recommended_source\": \"financial_db\",\n            \"query\": query,\n            \"confidence\": 0.85\n        }\n    elif \"technical\" in query.lower():\n        return {\n            \"recommended_source\": \"technical_docs\",\n            \"query\": query,\n            \"confidence\": 0.92\n        }\n    # ... other routing logic\n</code></pre> <p>This helps the model choose the right information source before retrieval.</p> <p>Each of these patterns can be mixed and matched to create sophisticated RAG systems, all communicating through the standardized MCP interface.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#beyond-text-multi-modal-and-other-extensions","title":"Beyond Text \u2013 Multi-Modal and Other Extensions","text":"<p>While our focus has been on text-based LLMs, MCP is not limited to text. The protocol can carry binary data (resources can be images or audio clips, with appropriate encoding). The Sampling primitive even has a provision for image generation or interpretation by specifying the content type (text or image) in the request.</p> <p>This means one could have an MCP server for, say, an image database, where images are exposed as resources (with mimeType) and the model could request them, or even an OCR tool as a Tool that returns text from an image. The modular design of MCP allows layering new capabilities.</p> <p>For example, Microsoft's recent contribution of a Playwright MCP server (for web browsing automation) shows how MCP can enable completely new tool modes: that server exposes browser actions like clicking, typing, and navigation as MCP tools. An AI agent using that server can browse the web like a user, but through standardized calls (browser_navigate, browser_click, etc.).</p> <p>This is far more complex than a basic function call, yet MCP handles it smoothly by formalizing those actions as tools with JSON schemas. The server even streams back page content as resources for the AI to read. This kind of advanced use case \u2013 essentially turning the AI into an automated agent in a real environment \u2013 is made feasible by MCP's architecture. And because it's standardized, improvements like better metadata (tool annotations, as added in MCP's update) benefit all tools across the board.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#multi-modal-server-examples","title":"Multi-Modal Server Examples","text":"<p>Here are some examples of what multi-modal MCP servers might look like:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#1-image-processing-server","title":"1. Image Processing Server","text":"<pre><code>from mcp.server.fastmcp import FastMCP\nimport base64\nfrom PIL import Image\nimport io\n\nmcp = FastMCP(\"Image Processing Server\")\n\n@mcp.resource(\"image://{id}\")\ndef get_image(id: str) -&gt; bytes:\n    \"\"\"Return image data as bytes with appropriate MIME type.\"\"\"\n    # Load image from storage\n    image_bytes = load_image_from_storage(id)\n    return image_bytes  # MCP can handle binary data with proper MIME type\n\n@mcp.tool()\ndef analyze_image(image_uri: str) -&gt; dict:\n    \"\"\"Analyze an image and return metadata and detected objects.\"\"\"\n    # Extract ID from URI\n    image_id = image_uri.replace(\"image://\", \"\")\n\n    # Get image data\n    image_data = load_image_from_storage(image_id)\n\n    # Run image analysis (just mock results for this example)\n    return {\n        \"resolution\": \"1920x1080\",\n        \"detected_objects\": [\"person\", \"car\", \"tree\"],\n        \"dominant_colors\": [\"blue\", \"green\"],\n        \"estimated_style\": \"photograph\"\n    }\n\n@mcp.tool()\ndef ocr_image(image_uri: str) -&gt; str:\n    \"\"\"Extract text from an image using OCR.\"\"\"\n    # Extract ID from URI\n    image_id = image_uri.replace(\"image://\", \"\")\n\n    # Perform OCR on the image\n    # (in a real implementation, would use an OCR library)\n    return \"Text extracted from the image would appear here.\"\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#2-audio-processing-server","title":"2. Audio Processing Server","text":"<pre><code>@mcp.resource(\"audio://{id}\")\ndef get_audio(id: str) -&gt; bytes:\n    \"\"\"Return audio data as bytes with appropriate MIME type.\"\"\"\n    # Load audio from storage\n    audio_bytes = load_audio_from_storage(id)\n    return audio_bytes\n\n@mcp.tool()\ndef transcribe_audio(audio_uri: str) -&gt; str:\n    \"\"\"Transcribe speech in audio to text.\"\"\"\n    # Extract ID from URI\n    audio_id = audio_uri.replace(\"audio://\", \"\")\n\n    # Get audio data and transcribe\n    # (in a real implementation, would use a speech-to-text service)\n    return \"Transcription of the audio would appear here.\"\n\n@mcp.tool()\ndef analyze_audio(audio_uri: str) -&gt; dict:\n    \"\"\"Analyze audio properties and content.\"\"\"\n    # Implementation details...\n    return {\n        \"duration_seconds\": 120,\n        \"language_detected\": \"English\",\n        \"speakers_detected\": 2,\n        \"music_detected\": False,\n        \"audio_quality\": \"High\"\n    }\n</code></pre> <p>These examples show how MCP can handle different types of media while maintaining the same protocol structure. The AI can request media as resources and then analyze them using tools, all through the standard MCP interface.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#workflow-orchestration-and-human-oversight","title":"Workflow Orchestration and Human Oversight","text":"<p>Finally, it's worth noting how MCP can fit into human-in-the-loop workflows. Because MCP clients (hosts) can intercept every tool invocation or resource request, a developer can implement policies like \"ask user for permission before executing destructive tools\" or log all data access for auditing. This is critical in enterprise settings.</p> <p>For instance, if an AI agent tries to call a \"delete_record\" tool on a database server, the MCP client could pop up a confirmation to the user. Or if a sampling request is made by a server (i.e., the server wants the AI to do something), the client can require a user click before letting it proceed. These controls ensure that even as we give AI agents more power via protocols like MCP, we maintain safety and governance.</p> <p>Moreover, MCP's open nature encourages community-driven best practices. Early adopters like Block (Square) highlighted that open protocols enable transparency and collaboration on making AI more helpful and less mechanical. We're likely to see shared schemas for common tasks, security frameworks, and audit tools develop around MCP as it matures.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#implementing-human-in-the-loop-controls","title":"Implementing Human-in-the-Loop Controls","text":"<p>Let's look at how a client might implement human approval for sensitive operations:</p> <pre><code>from mcp.client import MCPClient\n\nclass HumanOversightClient(MCPClient):\n    def __init__(self, transport, ui_interface):\n        super().__init__(transport)\n        self.ui = ui_interface  # Interface to show prompts to the user\n\n        # Define which tools require explicit approval\n        self.sensitive_tools = [\n            \"delete_record\",\n            \"send_email\",\n            \"execute_transaction\",\n            \"modify_permissions\"\n        ]\n\n        # Track usage and approvals\n        self.audit_log = []\n\n    async def call_tool(self, server_id, tool_name, params):\n        \"\"\"Override to add human approval for sensitive tools.\"\"\"\n        # Log the tool call attempt\n        self.audit_log.append({\n            \"timestamp\": time.time(),\n            \"action\": \"tool_call_attempt\",\n            \"server\": server_id,\n            \"tool\": tool_name,\n            \"params\": params\n        })\n\n        # Check if this tool requires approval\n        if tool_name in self.sensitive_tools:\n            # Format the request for human review\n            approval_text = f\"The AI wants to use tool '{tool_name}' with these parameters:\\n\"\n            approval_text += json.dumps(params, indent=2)\n\n            # Ask for user approval\n            approved = await self.ui.ask_for_approval(approval_text)\n\n            # Log the decision\n            self.audit_log.append({\n                \"timestamp\": time.time(),\n                \"action\": \"approval_decision\",\n                \"tool\": tool_name,\n                \"approved\": approved\n            })\n\n            if not approved:\n                # Return a standard rejection response\n                return {\n                    \"error\": \"User did not approve this operation\"\n                }\n\n        # If no approval needed or approval granted, proceed with the call\n        return await super().call_tool(server_id, tool_name, params)\n</code></pre> <p>This example demonstrates how a client can wrap standard MCP methods with approval flows. Similar patterns can be applied to resource access and sampling requests.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#enterprise-governance-patterns","title":"Enterprise Governance Patterns","text":"<p>For enterprise deployments, MCP enables several governance patterns:</p> <ol> <li> <p>Role-Based Access Control (RBAC): MCP servers can implement authentication and authorization, mapping user roles to allowed tools and resources.</p> </li> <li> <p>Audit Logging: All MCP interactions can be logged for compliance and review.</p> </li> <li> <p>Action Policies: Organizations can define policies about which tools require approval and from whom.</p> </li> <li> <p>Sandboxing: MCP servers can be deployed in controlled environments with limited access to backend systems.</p> </li> <li> <p>Rate Limiting: Servers can implement throttling to prevent abuse or unintended resource consumption.</p> </li> </ol> <p>These patterns can be implemented consistently across different MCP servers, creating a standardized governance framework for AI tool use.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#performance-considerations-and-best-practices","title":"Performance Considerations and Best Practices","text":"<p>When implementing MCP in production systems, several performance considerations and best practices should be kept in mind:</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#latency-management","title":"Latency Management","text":"<p>MCP introduces additional communication steps between the AI and external systems. To minimize latency:</p> <ol> <li>Co-locate servers and clients when possible to reduce network overhead</li> <li>Implement caching at the server level for frequently accessed resources</li> <li>Use streaming responses for large data transfers rather than waiting for complete results</li> <li>Balance chunking strategies - too many small requests can be inefficient, while too few large requests can block the UI</li> </ol> <pre><code># Example of an MCP server with caching\nfrom mcp.server.fastmcp import FastMCP\nimport functools\n\nmcp = FastMCP(\"Cached Server\")\n\n# Simple in-memory cache for demonstration\ncache = {}\n\ndef with_cache(ttl_seconds=300):\n    \"\"\"Decorator to cache function results.\"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a cache key from function name and arguments\n            key = f\"{func.__name__}:{str(args)}:{str(kwargs)}\"\n\n            # Check if result is in cache and not expired\n            if key in cache and (time.time() - cache[key]['time']) &lt; ttl_seconds:\n                return cache[key]['result']\n\n            # Call the function and cache the result\n            result = func(*args, **kwargs)\n            cache[key] = {\n                'result': result,\n                'time': time.time()\n            }\n            return result\n        return wrapper\n    return decorator\n\n@mcp.tool()\n@with_cache(ttl_seconds=60)\ndef expensive_operation(param1: str) -&gt; dict:\n    \"\"\"A computationally expensive operation that benefits from caching.\"\"\"\n    # Simulate expensive operation\n    time.sleep(2)  # In real code, this would be actual computation\n    return {\"result\": f\"Processed {param1}\", \"timestamp\": time.time()}\n</code></pre>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Input validation: Always validate inputs on the server side to prevent injection attacks</li> <li>Least privilege: MCP servers should operate with minimal permissions needed</li> <li>Token management: Securely handle authentication tokens and implement proper renewal</li> <li>Transport security: Use TLS for HTTP-based transports</li> <li>Rate limiting: Implement rate limits to prevent abuse</li> </ol>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#deployment-patterns","title":"Deployment Patterns","text":"<ol> <li>Sidecar deployment: Run MCP servers as sidecars alongside existing services</li> <li>Microservice architecture: Deploy each MCP server as a separate microservice</li> <li>Function-as-a-Service: Deploy lightweight MCP servers as serverless functions</li> <li>Proxy pattern: Use a central MCP proxy to manage multiple backend services</li> </ol>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#development-workflow","title":"Development Workflow","text":"<ol> <li>Start with the SDK: Use Anthropic's MCP SDK for your language to minimize boilerplate</li> <li>Test with the Inspector: Use the MCP Inspector tool to manually test your server</li> <li>Write integration tests: Test your MCP server with automated client requests</li> <li>Document capabilities: Create clear documentation of your server's tools and resources</li> <li>Semantic versioning: Follow semver when releasing updates to your MCP servers</li> </ol>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#conclusion","title":"Conclusion","text":"<p>The Model Context Protocol (MCP) is a significant step forward in AI integration. It provides a robust, extensible framework for connecting LLMs to the world around them \u2013 to the data and tools that ground their responses in reality. By standardizing how context is provided to models, MCP frees developers from re-inventing the wheel for each integration, and it empowers AI systems with a richer, live model of their environment.</p> <p>We've seen how MCP compares to earlier approaches (offering broader scope, persistent state, and interoperability), learned about its core components (resources, tools, prompts, etc.), and built a simple Python MCP server step-by-step. We also explored advanced patterns like agent frameworks and retrieval augmentation, where MCP serves as the connective tissue for complex AI applications.</p> <p>As of 2025, MCP is still evolving (recent updates added streaming transports, OAuth security, and more), but it's rapidly gaining adoption and community support. Anthropic maintains an open-source repository of MCP SDKs (Python, TypeScript, Java, Kotlin, C#, Rust) and dozens of ready-to-use servers.</p> <p>The vision is that eventually, AI assistants will use MCP to maintain context seamlessly as they hop between tools and data sources, much like a person using a computer \u2013 making them far more useful and reliable. By learning and leveraging MCP now, developers can be at the forefront of building context-aware, tool-empowered AI agents that work across platforms. Whether you integrate it into a chatbot, an IDE, or an autonomous agent, MCP provides the foundation to connect AI with the real world in a standardized, powerful way.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/2025/03/31/introduction/#resources-and-next-steps","title":"Resources and Next Steps","text":"<p>If you'd like to start working with MCP:</p> <ol> <li>Official Documentation: Visit the Model Context Protocol website for specifications and guides</li> <li>GitHub Repositories: Check out Anthropic's MCP SDKs and reference implementations</li> <li>Community Servers: Explore the growing ecosystem of community-contributed MCP servers</li> <li>Join the Discussion: Participate in the MCP working group to shape the future of the protocol</li> </ol> <p>Sources: The official Anthropic announcement and documentation were used to define MCP and its architecture. InfoQ and VentureBeat articles provided context on MCP's goals and industry adoption. The MCP specification and guides were referenced for technical details on primitives and message flows. Code examples were adapted from Anthropic's open-source Python SDK and quickstart tutorials. These resources, alongside community commentary, offer a comprehensive view of MCP as the emerging standard for AI context integration.</p>","tags":["agents","llm","architecture","mcp"]},{"location":"writing/archive/2025/","title":"2025","text":""},{"location":"writing/category/llms/","title":"LLMs","text":""},{"location":"writing/category/agents/","title":"Agents","text":""}]}