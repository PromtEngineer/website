<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="AI Consulting, RAG, and other personal notes."><meta name=author content="PromptX AI"><link href=https://engineerprompt.ai/writing/category/llms/ rel=canonical><link href=../agents/ rel=prev><link href=../../archive/2025/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../feed_rss_updated.xml><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.12"><title>LLMs - PromptX AI</title><link rel=stylesheet href=../../../assets/stylesheets/main.2afb09e1.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><link rel=stylesheet href=../../../stylesheets/mermaid.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-686PKP2V2V"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-686PKP2V2V",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-686PKP2V2V",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="LLMs - PromptX AI"><meta property=og:description content="AI Consulting, RAG, and other personal notes."><meta property=og:image content=https://engineerprompt.ai/assets/images/social/writing/category/llms.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://engineerprompt.ai/writing/category/llms/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="LLMs - PromptX AI"><meta name=twitter:description content="AI Consulting, RAG, and other personal notes."><meta name=twitter:image content=https://engineerprompt.ai/assets/images/social/writing/category/llms.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#llms class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="PromptX AI" class="md-header__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PromptX AI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLMs </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../services/ class=md-tabs__link> Consulting </a> </li> <li class=md-tabs__item> <a href=../../../rag-beyond-basics/ class=md-tabs__link> RAG Course </a> </li> <li class=md-tabs__item> <a href=../../../youtube/ class=md-tabs__link> YouTube Videos </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Writing </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="PromptX AI" class="md-nav__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> PromptX AI </label> <div class=md-nav__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../services/ class=md-nav__link> <span class=md-ellipsis> Consulting </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../../../rag-beyond-basics/ class=md-nav__link> <span class=md-ellipsis> RAG Course </span> </a> </li> <li class=md-nav__item> <a href=../../../youtube/ class=md-nav__link> <span class=md-ellipsis> YouTube Videos </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Writing </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Writing </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3 checked> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=true> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> LLMs </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> LLMs </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#a-visual-guide-to-llm-agents class=md-nav__link> <span class=md-ellipsis> A Visual Guide to LLM Agents </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#a-visual-guide-to-llm-agents class=md-nav__link> <span class=md-ellipsis> A Visual Guide to LLM Agents </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=llms>LLMs<a class=headerlink href=#llms title="Permanent link">&para;</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://avatars.githubusercontent.com/u/134474669?v=4" alt="Muhammad Farooq"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-28 00:00:00+00:00">2025/03/28</time></li> <li class=md-meta__item> in <a href=./ class=md-meta__link>LLMs</a>, <a href=../agents/ class=md-meta__link>Agents</a></li> <li class=md-meta__item> 13 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=a-visual-guide-to-llm-agents><a href=../../2025/03/28/a-visual-guide-to-llm-agents/ class=toclink>A Visual Guide to LLM Agents</a></h2> <h3 id=table-of-contents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#table-of-contents>Table of Contents</a></h3> <ol> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#introduction-to-large-language-models>Introduction to Large Language Models</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#from-llms-to-agents>From LLMs to Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#core-components-of-llm-agents>Core Components of LLM Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#tools-and-augmentation>Tools and Augmentation</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-planning-and-reasoning>Agent Planning and Reasoning</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-memory-systems>Agent Memory Systems</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#advanced-agent-architectures>Advanced Agent Architectures</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#multi-agent-systems>Multi-Agent Systems</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#building-and-deploying-agents>Building and Deploying Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#future-directions>Future Directions</a></li> </ol> <h3 id=introduction-to-large-language-models><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#introduction-to-large-language-models>Introduction to Large Language Models</a></h3> <p>Before diving into agents, we need to understand what Large Language Models (LLMs) are and how they function.</p> <p>LLMs are sophisticated neural networks trained on vast amounts of text data to understand and generate human-like text. These models have evolved from simple statistical approaches to complex architectures based primarily on the Transformer architecture introduced in 2017.</p> <h4 id=how-llms-work><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#how-llms-work>How LLMs Work</a></h4> <p>At their core, LLMs predict the next token (word or subword) in a sequence based on the context provided. The basic architecture consists of:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    A[Input Text] --&gt; B[Tokenization]
    B --&gt; C[Embedding Layer]
    C --&gt; D[Transformer Layers]
    D --&gt; E[Output Layer]
    E --&gt; F[Generated Text]

    style A fill:#f9f9f9,stroke:#333,stroke-width:1px
    style B fill:#e6f3ff,stroke:#333,stroke-width:1px
    style C fill:#e6f3ff,stroke:#333,stroke-width:1px
    style D fill:#cce5ff,stroke:#333,stroke-width:1px
    style E fill:#e6f3ff,stroke:#333,stroke-width:1px
    style F fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> </div> <ul> <li><strong>Tokenization</strong>: Converting input text into tokens</li> <li><strong>Embedding Layer</strong>: Transforming tokens into vector representations</li> <li><strong>Transformer Layers</strong>: Processing these vectors through attention mechanisms</li> <li><strong>Output Layer</strong>: Generating probability distributions for the next token</li> </ul> <p>Most modern LLMs use the transformer architecture, which employs self-attention mechanisms to weigh the importance of different words in context.</p> <h4 id=capabilities-and-limitations-of-traditional-llms><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#capabilities-and-limitations-of-traditional-llms>Capabilities and Limitations of Traditional LLMs</a></h4> <p><strong>Capabilities:</strong> - Text generation across various domains - Understanding context and nuance - Adapting to different writing styles - Performing various language tasks without task-specific training</p> <p><strong>Limitations:</strong> - No ability to access or verify external information beyond training data - No capability to take actions in the world - Limited understanding of temporal context (when events occurred) - No persistent memory between sessions - No ability to use tools or APIs - Risk of hallucinations (generating false information)</p> <p>These limitations highlight why moving from passive LLMs to active agents is necessary for more complex applications.</p> <h3 id=from-llms-to-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#from-llms-to-agents>From LLMs to Agents</a></h3> <p>The transition from passive language models to active agents is fundamental to understanding LLM agents.</p> <h4 id=what-defines-an-agent><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#what-defines-an-agent>What Defines an Agent?</a></h4> <p>As defined by Russell &amp; Norvig, <strong>"an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators."</strong></p> <p>This definition introduces two critical components missing in standard LLMs: 1. <strong>Perception</strong>: The ability to sense the environment 2. <strong>Action</strong>: The ability to affect the environment</p> <h4 id=the-agent-framework><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#the-agent-framework>The Agent Framework</a></h4> <p>An agent-based framework adapts this definition to work with LLMs:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart LR
    subgraph LLM["Traditional LLM"]
        A[Input Prompt] --&gt; B[Text Generation]
        B --&gt; C[Output Text]
    end

    subgraph Agent["LLM Agent"]
        D[Environment] --&gt; E[Perception]
        E --&gt; F[Reasoning/Planning]
        F --&gt; G[Tool Use]
        G --&gt; H[Actions]
        H --&gt; D
        I[Memory] --&gt; F
        F --&gt; I
    end

    LLM --&gt; Agent

    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px
    style Agent fill:#e6f3ff,stroke:#333,stroke-width:2px</code></pre> </div> <ul> <li><strong>Environment</strong>: The context in which the agent operates (could be a chat interface, document, or digital environment)</li> <li><strong>Perception</strong>: Input methods like prompts, document content, or API responses</li> <li><strong>Reasoning</strong>: Internal processing using the LLM to decide what to do</li> <li><strong>Action</strong>: Outputs that affect the environment (generating text, calling functions, using tools)</li> <li><strong>Memory</strong>: Retaining information across interactions</li> </ul> <p>This framework transforms a passive text prediction system into an entity that can intelligently interact with its surroundings.</p> <h3 id=core-components-of-llm-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#core-components-of-llm-agents>Core Components of LLM Agents</a></h3> <p>Let's explore the essential components that make up an LLM agent:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart LR
    %% Central node with dashed border
    LLM["Large Language\nModel"] 

    %% Main components with their subgraphs
    subgraph Tools[" "]
        T[Tool Use] --&gt; T1[Function Calling]
        T --&gt; T2[API Integration]
        T --&gt; T3[Database Access]
    end

    subgraph Reasoning[" "]
        R[Reasoning] --&gt; R1[Task Decomposition]
        R --&gt; R2[Chain-of-Thought]
        R --&gt; R3[Decision-Making]
    end

    subgraph Perception[" "]
        P[Perception] --&gt; P1[Text Inputs]
        P --&gt; P2[Document Understanding]
        P --&gt; P3[Multimodal Inputs]
    end

    subgraph Memory[" "]
        M[Memory] --&gt; M1[Short-Term]
        M --&gt; M2[Long-Term]
        M --&gt; M3[Episodic &amp; Semantic]
    end

    %% Connect LLM to main components
    LLM --&gt; T
    LLM --&gt; R
    LLM --&gt; P
    LLM --&gt; M

    %% Style nodes
    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px,stroke-dasharray:5 5

    %% Tool styles
    style T fill:#fd7e14,stroke:#333,stroke-width:2px
    style T1 fill:#ffe8cc,stroke:#333,stroke-width:1px
    style T2 fill:#ffe8cc,stroke:#333,stroke-width:1px
    style T3 fill:#ffe8cc,stroke:#333,stroke-width:1px

    %% Reasoning styles
    style R fill:#40c057,stroke:#333,stroke-width:2px
    style R1 fill:#d3f9d8,stroke:#333,stroke-width:1px
    style R2 fill:#d3f9d8,stroke:#333,stroke-width:1px
    style R3 fill:#d3f9d8,stroke:#333,stroke-width:1px

    %% Perception styles
    style P fill:#4dabf7,stroke:#333,stroke-width:2px
    style P1 fill:#d0ebff,stroke:#333,stroke-width:1px
    style P2 fill:#d0ebff,stroke:#333,stroke-width:1px
    style P3 fill:#d0ebff,stroke:#333,stroke-width:1px

    %% Memory styles
    style M fill:#ae3ec9,stroke:#333,stroke-width:2px
    style M1 fill:#f3d9fa,stroke:#333,stroke-width:1px
    style M2 fill:#f3d9fa,stroke:#333,stroke-width:1px
    style M3 fill:#f3d9fa,stroke:#333,stroke-width:1px

</code></pre> </div> <h4 id=environment-perception><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#environment-perception>Environment Perception</a></h4> <p>Agents need to understand their environment through various inputs:</p> <ul> <li><strong>Text Input</strong>: The most basic form of perception through prompts</li> <li><strong>Document Understanding</strong>: Processing and understanding documents</li> <li><strong>Structured Data</strong>: Working with databases, APIs, and structured information</li> <li><strong>Multimodal Input</strong>: Processing images, audio, or other data types (in advanced agents)</li> </ul> <h4 id=planning-and-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#planning-and-reasoning>Planning and Reasoning</a></h4> <p>An agent must plan its actions and reason about the best course of action:</p> <ul> <li><strong>Task Decomposition</strong>: Breaking complex tasks into manageable steps</li> <li><strong>Chain-of-Thought</strong>: Working through problems step-by-step</li> <li><strong>Decision-Making</strong>: Evaluating options and selecting the best course of action</li> <li><strong>Self-Reflection</strong>: Evaluating its own reasoning and outputs</li> </ul> <h4 id=tool-usage-and-integration><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#tool-usage-and-integration>Tool Usage and Integration</a></h4> <p>A defining characteristic of LLM agents is their ability to use tools:</p> <ul> <li><strong>Function Calling</strong>: Identifying when to call an external function</li> <li><strong>API Integration</strong>: Connecting to external services through APIs</li> <li><strong>Code Execution</strong>: Running code to perform calculations or manipulate data</li> <li><strong>Database Access</strong>: Retrieving or storing information in databases</li> </ul> <h4 id=memory-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#memory-systems>Memory Systems</a></h4> <p>Agents require memory to maintain context and learn from past interactions:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TB
    %% Short-term Memory Section
    subgraph ShortTerm["Short-Term Memory"]
        direction LR
        CH["Conversation History"] --- CD["Current Session Data"] --- AT["Active Task State"]
    end

    %% Long-term Memory Section
    subgraph LongTerm["Long-Term Memory"]
        direction LR
        VD["Vector Database"] --- KG["Knowledge Graph"] --- DD["Document Store"] --- UP["User Profiles"]
    end

    %% Memory Types Section
    subgraph MemTypes["Memory Types"]
        direction LR
        EM["Episodic Memory&lt;br/&gt;(Specific Interactions)"] --- SM["Semantic Memory&lt;br/&gt;(General Knowledge)"] --- PM["Procedural Memory&lt;br/&gt;(How to Perform Tasks)"]
    end

    %% Memory Retrieval Section
    subgraph Retrieval["Memory Retrieval"]
        direction LR
        SS["Semantic Search"] --- TF["Temporal Filtering"] --- SR["Relevance Ranking"] --- CR["Contextual Retrieval"]
    end

    %% Decision Making
    DM["Agent Decision Making"]

    %% Connections
    ShortTerm --&gt; MemTypes
    LongTerm --&gt; MemTypes
    MemTypes --&gt; Retrieval
    Retrieval --&gt; DM

    %% Styling
    style ShortTerm fill:#fff7e6,stroke:#333,stroke-width:1px
    style LongTerm fill:#e6ffe6,stroke:#333,stroke-width:1px
    style MemTypes fill:#e6f7ff,stroke:#333,stroke-width:1px
    style Retrieval fill:#ffe6e6,stroke:#333,stroke-width:1px
    style DM fill:#f0f0ff,stroke:#333,stroke-width:1px

    %% Node Styles
    classDef default fill:#fff,stroke:#333,stroke-width:1px</code></pre> </div> <ul> <li><strong>Short-Term Memory</strong>: Recent conversation history</li> <li><strong>Long-Term Memory</strong>: Persistent information stored across sessions</li> <li><strong>Episodic Memory</strong>: Specific sequences of interactions</li> <li><strong>Semantic Memory</strong>: General knowledge and facts</li> </ul> <p>These core components transform an LLM into an agent capable of complex, goal-oriented behavior.</p> <h3 id=tools-and-augmentation><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#tools-and-augmentation>Tools and Augmentation</a></h3> <p>Tools and augmentation techniques enhance the capabilities of LLM agents beyond their built-in knowledge.</p> <h4 id=types-of-tools><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#types-of-tools>Types of Tools</a></h4> <p>Modern LLM agents can leverage various tools:</p> <ul> <li><strong>Search Tools</strong>: Accessing up-to-date information from the internet</li> <li><strong>Calculators</strong>: Performing precise mathematical operations</li> <li><strong>Knowledge Bases</strong>: Accessing specific domain knowledge</li> <li><strong>Code Interpreters</strong>: Executing and debugging code</li> <li><strong>Database Interfaces</strong>: Querying and manipulating structured data</li> <li><strong>API Connectors</strong>: Interacting with external services and platforms</li> </ul> <h4 id=retrieval-augmented-generation-rag><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#retrieval-augmented-generation-rag>Retrieval Augmented Generation (RAG)</a></h4> <p>RAG is a powerful technique that combines retrieval of information with text generation:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    Q[Query/Question] --&gt; E[Embedding Model]
    E --&gt; VS[Vector Search]

    subgraph Indexing["Document Indexing (Pre-processing)"]
        D[Documents] --&gt; DC[Document Chunking]
        DC --&gt; DE[Document Embedding]
        DE --&gt; VDB[Vector Database]
    end

    VS --&gt; VDB
    VDB --&gt; RD[Retrieved Documents]

    Q --&gt; P[Prompt Construction]
    RD --&gt; P

    P --&gt; LLM[Large Language Model]
    LLM --&gt; A[Augmented Response]

    style Indexing fill:#e6f3ff,stroke:#333,stroke-width:1px
    style Q fill:#f9f9f9,stroke:#333,stroke-width:1px
    style P fill:#f9f9f9,stroke:#333,stroke-width:1px
    style LLM fill:#cce5ff,stroke:#333,stroke-width:2px
    style A fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> </div> <ol> <li><strong>Indexing</strong>: Documents are processed, chunked, and stored in a vector database</li> <li><strong>Retrieval</strong>: When a query is received, relevant documents are retrieved</li> <li><strong>Augmentation</strong>: Retrieved content is added to the prompt</li> <li><strong>Generation</strong>: The LLM generates a response based on both the query and the retrieved information</li> </ol> <p>RAG enhances accuracy by grounding responses in specific knowledge sources, reducing hallucinations and improving factual accuracy.</p> <h4 id=function-and-api-integration><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#function-and-api-integration>Function and API Integration</a></h4> <p>Function calling allows agents to interact with the world:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant LLM
    participant Function

    User-&gt;&gt;LLM: Query (e.g., "What's the weather in New York?")

    LLM-&gt;&gt;LLM: Recognize need for external data

    LLM-&gt;&gt;Function: Call function&lt;br/&gt;(get_weather, {location: "New York"})
    Function-&gt;&gt;LLM: Return data (Temperature: 72°F, Condition: Sunny)

    LLM-&gt;&gt;User: Generate response with function data&lt;br/&gt;"The current weather in New York is sunny with a temperature of 72°F."

    note over LLM,Function: Modern LLMs can determine when to&lt;br/&gt;call functions and structure the&lt;br/&gt;appropriate parameters</code></pre> </div> <ol> <li><strong>Function Definition</strong>: Functions are defined with names, descriptions, and parameter specifications</li> <li><strong>Function Detection</strong>: The LLM detects when a function should be called based on the user's query</li> <li><strong>Parameter Generation</strong>: The LLM generates the appropriate parameters</li> <li><strong>Function Execution</strong>: The function is executed, and results are returned</li> <li><strong>Response Integration</strong>: The LLM incorporates the function results into its response</li> </ol> <p>This capability enables agents to perform actions like checking the weather, booking appointments, or processing payments.</p> <h3 id=agent-planning-and-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-planning-and-reasoning>Agent Planning and Reasoning</a></h3> <p>Effective planning and reasoning are crucial for complex tasks.</p> <h4 id=prompt-engineering-for-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#prompt-engineering-for-agents>Prompt Engineering for Agents</a></h4> <p>Agent prompts typically include:</p> <ul> <li><strong>System Instructions</strong>: Defining the agent's role and capabilities</li> <li><strong>Available Tools</strong>: Descriptions of tools the agent can use</li> <li><strong>Constraints</strong>: Limitations on the agent's actions</li> <li><strong>Output Format</strong>: How the agent should structure its responses</li> <li><strong>Examples</strong>: Demonstrations of expected behavior</li> </ul> <h4 id=chain-of-thought-cot-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#chain-of-thought-cot-reasoning>Chain-of-Thought (CoT) Reasoning</a></h4> <p>CoT enables an agent to work through problems step-by-step:</p> <ol> <li><strong>Problem Analysis</strong>: Understanding the task and breaking it down</li> <li><strong>Intermediate Steps</strong>: Working through each step logically</li> <li><strong>Reflection</strong>: Checking the reasoning at each step</li> <li><strong>Solution</strong>: Arriving at the final answer based on the steps</li> </ol> <p>This approach significantly improves performance on complex reasoning tasks.</p> <h4 id=react-framework><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#react-framework>ReAct Framework</a></h4> <p>ReAct (Reasoning + Acting) interleaves thinking and action:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant Agent
    participant Tools as External Tools

    User-&gt;&gt;Agent: Task or Query

    loop Until task completion
        Agent-&gt;&gt;Agent: Thought: Reasoning about next step
        Agent-&gt;&gt;Agent: Action: Decide which tool to use
        Agent-&gt;&gt;Tools: Call appropriate tool
        Tools-&gt;&gt;Agent: Observation: Return result
        Agent-&gt;&gt;Agent: Thought: Process observation
    end

    Agent-&gt;&gt;User: Final response

    note over Agent: ReAct interleaves reasoning (thoughts)&lt;br/&gt;with actions and observations in a cycle</code></pre> </div> <ol> <li><strong>Reasoning</strong>: The agent thinks about what it needs to do</li> <li><strong>Action</strong>: The agent takes action using available tools</li> <li><strong>Observation</strong>: The agent observes the results of its action</li> <li><strong>Continued Reasoning</strong>: The agent incorporates observations into its reasoning</li> </ol> <p>This cycle continues until the task is complete, enabling dynamic, adaptive problem-solving.</p> <h3 id=agent-memory-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-memory-systems>Agent Memory Systems</a></h3> <p>Memory systems enable agents to maintain context and learn from past interactions.</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    subgraph SM["Short-Term Memory"]
        SM1["Conversation History"]
        SM2["Current Session Data"]
        SM3["Active Task State"]
    end

    subgraph LM["Long-Term Memory"]
        LM1["Vector Database"]
        LM2["Knowledge Graph"]
        LM3["Document Store"]
        LM4["User Profiles"]
    end

    subgraph MT["Memory Types"]
        MT1["Episodic Memory&lt;br/&gt;(Specific Interactions)"]
        MT2["Semantic Memory&lt;br/&gt;(General Knowledge)"]
        MT3["Procedural Memory&lt;br/&gt;(How to Perform Tasks)"]
    end

    subgraph MR["Memory Retrieval"]
        MR1["Semantic Search"]
        MR2["Temporal Filtering"]
        MR3["Relevance Ranking"]
        MR4["Contextual Retrieval"]
    end

    SM --&gt; MT
    LM --&gt; MT
    MT --&gt; MR
    MR --&gt; A[Agent Decision Making]

    style SM fill:#ffffcc,stroke:#333,stroke-width:1px
    style LM fill:#ccffcc,stroke:#333,stroke-width:1px
    style MT fill:#ccffff,stroke:#333,stroke-width:1px
    style MR fill:#ffcccc,stroke:#333,stroke-width:1px</code></pre> </div> <h4 id=short-term-context><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#short-term-context>Short-Term Context</a></h4> <p>Short-term or working memory includes:</p> <ul> <li><strong>Conversation History</strong>: The recent exchanges between user and agent</li> <li><strong>Current Session Data</strong>: Information gathered during the current interaction</li> <li><strong>Active Task State</strong>: The current progress on the task being performed</li> </ul> <p>These elements are typically handled through context window management.</p> <h4 id=long-term-memory-storage><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#long-term-memory-storage>Long-Term Memory Storage</a></h4> <p>Long-term memory enables persistent information storage:</p> <ul> <li><strong>Vector Databases</strong>: Storing semantic representations of past conversations</li> <li><strong>Knowledge Graphs</strong>: Structured representations of entities and relationships</li> <li><strong>Document Stores</strong>: Persistent storage of important information</li> <li><strong>User Profiles</strong>: Preferences and patterns specific to individual users</li> </ul> <h4 id=episodic-vs-semantic-memory><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#episodic-vs-semantic-memory>Episodic vs. Semantic Memory</a></h4> <p>Agents can implement different types of memory:</p> <ul> <li><strong>Episodic Memory</strong>: Specific sequences of interactions (e.g., "Last time we discussed home renovation options")</li> <li><strong>Semantic Memory</strong>: General knowledge and facts (e.g., "The user prefers minimalist design")</li> </ul> <h4 id=memory-retrieval-strategies><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#memory-retrieval-strategies>Memory Retrieval Strategies</a></h4> <p>Effective retrieval is critical for using stored information:</p> <ul> <li><strong>Semantic Search</strong>: Finding relevant information based on meaning</li> <li><strong>Temporal Filtering</strong>: Retrieving information based on when it was stored</li> <li><strong>Relevance Ranking</strong>: Prioritizing the most important information</li> <li><strong>Contextual Retrieval</strong>: Finding information relevant to the current context</li> </ul> <p>A well-designed memory system allows agents to build on past interactions and provide more personalized experiences.</p> <h3 id=advanced-agent-architectures><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#advanced-agent-architectures>Advanced Agent Architectures</a></h3> <p>As agents become more sophisticated, their architectures evolve to handle more complex tasks.</p> <h4 id=task-decomposition><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#task-decomposition>Task Decomposition</a></h4> <p>Complex task handling requires sophisticated decomposition:</p> <ol> <li><strong>Goal Analysis</strong>: Understanding the overall objective</li> <li><strong>Subtask Identification</strong>: Breaking down the goal into manageable parts</li> <li><strong>Dependency Mapping</strong>: Determining the order of subtasks</li> <li><strong>Resource Allocation</strong>: Assigning appropriate tools to each subtask</li> </ol> <p>This approach enables agents to tackle problems too complex to solve all at once.</p> <h4 id=self-reflection-and-self-correction><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#self-reflection-and-self-correction>Self-Reflection and Self-Correction</a></h4> <p>Advanced agents can evaluate and improve their own outputs:</p> <ol> <li><strong>Output Generation</strong>: Producing an initial response</li> <li><strong>Self-Critique</strong>: Identifying potential issues or improvements</li> <li><strong>Refinement</strong>: Revising the response based on self-critique</li> <li><strong>Verification</strong>: Checking the improved response against requirements</li> </ol> <p>This recursive improvement process enhances accuracy and quality.</p> <h4 id=verification-of-outputs><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#verification-of-outputs>Verification of Outputs</a></h4> <p>Ensuring reliability through verification:</p> <ul> <li><strong>Fact-Checking</strong>: Verifying factual claims against reliable sources</li> <li><strong>Consistency Checks</strong>: Ensuring internal consistency in responses</li> <li><strong>Hallucination Detection</strong>: Identifying when the agent is generating unfounded information</li> <li><strong>Confidence Scoring</strong>: Assessing the reliability of different parts of a response</li> </ul> <h4 id=meta-prompting-and-prompt-chaining><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#meta-prompting-and-prompt-chaining>Meta-Prompting and Prompt Chaining</a></h4> <p>Sophisticated prompting techniques:</p> <ul> <li><strong>Meta-Prompting</strong>: Using the LLM to generate or refine its own prompts</li> <li><strong>Prompt Chaining</strong>: Connecting multiple prompts in sequence to handle complex workflows</li> <li><strong>Adaptive Prompting</strong>: Modifying prompts based on user responses or task progress</li> </ul> <p>These techniques allow for more flexible and powerful agent behaviors.</p> <h3 id=multi-agent-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#multi-agent-systems>Multi-Agent Systems</a></h3> <p>Multiple specialized agents can collaborate to solve complex problems.</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    U[User] --&gt; C[Coordinator Agent]

    C --&gt; P[Planner Agent]
    C --&gt; R[Researcher Agent]
    C --&gt; E[Expert Agent]
    C --&gt; CR[Critic Agent]

    P --&gt; C
    R --&gt; C
    E --&gt; C
    CR --&gt; C

    C --&gt; U

    subgraph Communication
        P &lt;-.-&gt; R
        R &lt;-.-&gt; E
        E &lt;-.-&gt; CR
        P &lt;-.-&gt; CR
    end

    style U fill:#f9f9f9,stroke:#333,stroke-width:1px
    style C fill:#ffcc99,stroke:#333,stroke-width:2px
    style P fill:#ccffcc,stroke:#333,stroke-width:1px
    style R fill:#ccffcc,stroke:#333,stroke-width:1px
    style E fill:#ccffcc,stroke:#333,stroke-width:1px
    style CR fill:#ccffcc,stroke:#333,stroke-width:1px
    style Communication opacity:0.2</code></pre> </div> <h4 id=agent-collaboration-models><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-collaboration-models>Agent Collaboration Models</a></h4> <p>Different models for agent collaboration:</p> <ul> <li><strong>Hierarchical</strong>: Supervisor agents coordinate specialized worker agents</li> <li><strong>Peer-to-Peer</strong>: Agents communicate directly with each other</li> <li><strong>Market-Based</strong>: Agents bid for tasks based on their capabilities</li> <li><strong>Consensus-Based</strong>: Agents work together to reach agreement on solutions</li> </ul> <h4 id=specialized-agent-roles><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#specialized-agent-roles>Specialized Agent Roles</a></h4> <p>Multi-agent systems often feature specialized roles:</p> <ul> <li><strong>Planner</strong>: Designs overall strategy and breaks down tasks</li> <li><strong>Researcher</strong>: Gathers information from various sources</li> <li><strong>Expert</strong>: Provides domain-specific knowledge and analysis</li> <li><strong>Critic</strong>: Evaluates and improves outputs</li> <li><strong>Coordinator</strong>: Manages communication between agents</li> </ul> <h4 id=communication-protocols><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#communication-protocols>Communication Protocols</a></h4> <p>Effective inter-agent communication requires:</p> <ul> <li><strong>Message Formats</strong>: Structured formats for exchanging information</li> <li><strong>Dialogue Management</strong>: Tracking conversation state between agents</li> <li><strong>Knowledge Sharing</strong>: Methods for sharing relevant information</li> <li><strong>Conflict Resolution</strong>: Mechanisms for resolving disagreements</li> </ul> <h4 id=consensus-mechanisms><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#consensus-mechanisms>Consensus Mechanisms</a></h4> <p>When agents must agree on a course of action:</p> <ul> <li><strong>Voting</strong>: Simple majority or weighted voting schemes</li> <li><strong>Debate</strong>: Agents present arguments and counter-arguments</li> <li><strong>Evidence Evaluation</strong>: Assessing the quality of evidence presented</li> <li><strong>Meta-Evaluation</strong>: Using another agent to evaluate competing proposals</li> </ul> <p>Multi-agent systems enable more complex problem-solving than any single agent could achieve alone.</p> <h3 id=building-and-deploying-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#building-and-deploying-agents>Building and Deploying Agents</a></h3> <p>Practical considerations for implementing LLM agents.</p> <h4 id=frameworks-and-libraries><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#frameworks-and-libraries>Frameworks and Libraries</a></h4> <p>Popular tools for building agents:</p> <ul> <li><strong>LangChain</strong>: Framework for building language model applications</li> <li><strong>LlamaIndex</strong>: Tools for connecting LLMs to external data</li> <li><strong>AutoGPT</strong>: Autonomous AI agent framework</li> <li><strong>Microsoft Semantic Kernel</strong>: Framework for integrating AI with traditional programming</li> </ul> <h4 id=evaluation-metrics><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#evaluation-metrics>Evaluation Metrics</a></h4> <p>Assessing agent performance:</p> <ul> <li><strong>Task Completion Rate</strong>: How often the agent successfully completes tasks</li> <li><strong>Efficiency</strong>: Number of steps or time required to complete tasks</li> <li><strong>Accuracy</strong>: Correctness of information and actions</li> <li><strong>User Satisfaction</strong>: User ratings and feedback</li> <li><strong>Hallucination Rate</strong>: Frequency of unfounded claims</li> </ul> <h4 id=safety-considerations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#safety-considerations>Safety Considerations</a></h4> <p>Important safety measures:</p> <ul> <li><strong>Action Limitations</strong>: Restricting potentially harmful actions</li> <li><strong>User Confirmation</strong>: Requiring approval for significant actions</li> <li><strong>Monitoring</strong>: Tracking agent behavior for unexpected patterns</li> <li><strong>Transparency</strong>: Making reasoning and sources clear to users</li> </ul> <h4 id=deployment-patterns><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#deployment-patterns>Deployment Patterns</a></h4> <p>Common approaches to deployment:</p> <ul> <li><strong>Serverless Functions</strong>: Deploying components as cloud functions</li> <li><strong>Containerization</strong>: Packaging agents and dependencies in containers</li> <li><strong>API Services</strong>: Exposing agent capabilities through APIs</li> <li><strong>Edge Deployment</strong>: Running lightweight agents on edge devices</li> </ul> <p>Careful attention to these aspects ensures agents that are effective, reliable, and safe.</p> <h3 id=future-directions><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#future-directions>Future Directions</a></h3> <p>The field of LLM agents is rapidly evolving. Here are some emerging trends and challenges:</p> <h4 id=current-limitations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#current-limitations>Current Limitations</a></h4> <p>Areas needing improvement:</p> <ul> <li><strong>Reasoning Abilities</strong>: Enhancing logical and causal reasoning</li> <li><strong>Tool Creation</strong>: Enabling agents to create new tools as needed</li> <li><strong>True Autonomy</strong>: Reducing the need for human oversight</li> <li><strong>Cross-Domain Knowledge</strong>: Applying knowledge across different domains</li> <li><strong>Efficiency</strong>: Reducing computational requirements</li> </ul> <h4 id=research-frontiers><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#research-frontiers>Research Frontiers</a></h4> <p>Exciting areas of research:</p> <ul> <li><strong>Embodied Agents</strong>: Connecting language models to robotic systems</li> <li><strong>Multi-Modal Agents</strong>: Integrating text, vision, audio, and other modalities</li> <li><strong>Continual Learning</strong>: Agents that learn and improve through interaction</li> <li><strong>Collective Intelligence</strong>: Emergent capabilities from agent collaboration</li> <li><strong>Neural-Symbolic Approaches</strong>: Combining neural networks with symbolic reasoning</li> </ul> <h4 id=potential-applications><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#potential-applications>Potential Applications</a></h4> <p>Promising applications for advanced agents:</p> <ul> <li><strong>Personalized Education</strong>: Tutors adapted to individual learning styles</li> <li><strong>Scientific Discovery</strong>: Agents that generate and test hypotheses</li> <li><strong>Healthcare Assistance</strong>: Diagnostic and treatment planning support</li> <li><strong>Creative Collaboration</strong>: Partners for writing, design, and other creative tasks</li> <li><strong>Autonomous Systems</strong>: Self-directed systems that adapt to changing conditions</li> </ul> <h4 id=ethical-considerations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#ethical-considerations>Ethical Considerations</a></h4> <p>Important ethical questions:</p> <ul> <li><strong>Transparency</strong>: Ensuring users understand agent capabilities and limitations</li> <li><strong>Accountability</strong>: Determining responsibility for agent actions</li> <li><strong>Privacy</strong>: Protecting sensitive information used by agents</li> <li><strong>Bias</strong>: Addressing biases in training data and reasoning</li> <li><strong>Human Augmentation</strong>: Enhancing rather than replacing human capabilities</li> </ul> <p>The future of LLM agents will depend on thoughtful approaches to these challenges and opportunities.</p> <h3 id=conclusion><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#conclusion>Conclusion</a></h3> <p>LLM agents represent a significant evolution in artificial intelligence, transforming passive language models into active, capable assistants. By combining the language understanding of LLMs with the ability to perceive, reason, and act, these agents can solve increasingly complex problems and provide more valuable assistance.</p> <p>As the technology continues to develop, we can expect agents to become more autonomous, capable, and integrated into our daily lives and work. The journey from simple language models to sophisticated agents is just beginning, with many exciting possibilities on the horizon.</p> <p>The most successful approaches will likely be those that thoughtfully combine the strengths of artificial and human intelligence, creating systems that augment human capabilities rather than simply attempting to replace them.</p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../agents/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Agents"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Agents </div> </div> </a> <a href=../../archive/2025/ class="md-footer__link md-footer__link--next" aria-label="Next: 2025"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 2025 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 PromptX AI </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/PromtEngineer/localGPT target=_blank rel=noopener title="localGPT on GitHub" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=feed_rss_created.xml target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <a href=https://twitter.com/engineerrprompt target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> <a href=https://github.com/PromtEngineer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.linkedin.com/in/engineerprompt target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://www.youtube.com/@engineerprompt target=_blank rel=noopener title=www.youtube.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../javascripts/mermaid-zoom.js></script> <script src=../../../javascripts/mathjax.js></script> <script src=../../../javascripts/analytics.js></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>