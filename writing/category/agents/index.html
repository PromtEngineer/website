<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="AI Consulting, RAG, and other personal notes."><meta name=author content="PromptX AI"><link href=https://engineerprompt.ai/writing/category/agents/ rel=canonical><link href=../../ rel=prev><link href=../llms/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../feed_rss_updated.xml><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.12"><title>Agents - PromptX AI</title><link rel=stylesheet href=../../../assets/stylesheets/main.2afb09e1.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../stylesheets/extra.css><link rel=stylesheet href=../../../stylesheets/mermaid.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-686PKP2V2V"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-686PKP2V2V",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-686PKP2V2V",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Agents - PromptX AI"><meta property=og:description content="AI Consulting, RAG, and other personal notes."><meta property=og:image content=https://engineerprompt.ai/assets/images/social/writing/category/agents.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://engineerprompt.ai/writing/category/agents/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Agents - PromptX AI"><meta name=twitter:description content="AI Consulting, RAG, and other personal notes."><meta name=twitter:image content=https://engineerprompt.ai/assets/images/social/writing/category/agents.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#agents class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="PromptX AI" class="md-header__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PromptX AI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Agents </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../services/ class=md-tabs__link> Consulting </a> </li> <li class=md-tabs__item> <a href=../../../rag-beyond-basics/ class=md-tabs__link> RAG Course </a> </li> <li class=md-tabs__item> <a href=../../../youtube/ class=md-tabs__link> YouTube Videos </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Writing </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="PromptX AI" class="md-nav__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> PromptX AI </label> <div class=md-nav__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../services/ class=md-nav__link> <span class=md-ellipsis> Consulting </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../../../rag-beyond-basics/ class=md-nav__link> <span class=md-ellipsis> RAG Course </span> </a> </li> <li class=md-nav__item> <a href=../../../youtube/ class=md-nav__link> <span class=md-ellipsis> YouTube Videos </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Writing </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Writing </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3 checked> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=true> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Agents </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#model-context-protocol-mcp-a-quick-intro class=md-nav__link> <span class=md-ellipsis> Model Context Protocol (MCP) - A Quick Intro </span> </a> </li> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#a-visual-guide-to-llm-agents class=md-nav__link> <span class=md-ellipsis> A Visual Guide to LLM Agents </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../llms/ class=md-nav__link> <span class=md-ellipsis> LLMs </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#model-context-protocol-mcp-a-quick-intro class=md-nav__link> <span class=md-ellipsis> Model Context Protocol (MCP) - A Quick Intro </span> </a> </li> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#a-visual-guide-to-llm-agents class=md-nav__link> <span class=md-ellipsis> A Visual Guide to LLM Agents </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=agents>Agents<a class=headerlink href=#agents title="Permanent link">&para;</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://avatars.githubusercontent.com/u/134474669?v=4" alt="Muhammad Farooq"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-31 00:00:00+00:00">2025/03/31</time></li> <li class=md-meta__item> in <a href=../llms/ class=md-meta__link>LLMs</a>, <a href=./ class=md-meta__link>Agents</a></li> <li class=md-meta__item> 4 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=model-context-protocol-mcp-a-quick-intro><a href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/ class=toclink>Model Context Protocol (MCP) - A Quick Intro</a></h2> <h3 id=introduction><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#introduction>Introduction</a></h3> <h4 id=what-is-mcp><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#what-is-mcp>What is MCP?</a></h4> <p>The <strong>Model Context Protocol (MCP)</strong> is an open, standardized protocol introduced by Anthropic that bridges AI models with external data sources, tools, and services. Think of MCP like a <strong>"USB-C for AI applications"</strong> – it provides a universal adapter for connecting AI assistants to various content repositories, business tools, code environments, and APIs. By defining a common interface (built on JSON-RPC 2.0) for communication, MCP enables large language models (LLMs) to <strong>invoke functions, retrieve data, or use predefined prompts</strong> from external systems in a consistent and secure way.</p> <h4 id=purpose-and-problem-addressed><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#purpose-and-problem-addressed>Purpose and Problem Addressed</a></h4> <p>MCP was designed to solve a major integration challenge often called the "M×N problem" in AI development. Traditionally, integrating <em>M</em> different LLMs with <em>N</em> different tools or data sources required custom connectors for each combination – a combinatorial explosion of ad-hoc code. This meant AI systems were largely <strong>isolated from live data</strong>, trapped behind information silos unless developers painstakingly wired in each external API or database.</p> <p>MCP addresses this by providing <strong>one standardized "language" for all interactions</strong>. Developers can create one MCP-compliant interface for a data source or tool, and <em>any</em> MCP-enabled AI application can connect to it. This open standard replaces fragmented one-off integrations with a sustainable ecosystem of compatible clients and servers.</p> <p>The result is a simpler, more scalable way to give AI assistants access to the <strong>fresh, relevant context</strong> they need – whether it’s company documents, live databases, or web results.</p> <p>MCP is <strong>two-way and secure</strong>: it enables LLMs to query data and perform actions, while allowing organizations to keep data access controlled. The protocol supports OAuth 2.1 for authentication, enabling safe and secure enterprise use.</p> <h3 id=comparison-mcp-vs-simpler-function-calling-apis><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#comparison-mcp-vs-simpler-function-calling-apis>Comparison: MCP vs. Simpler Function-Calling APIs</a></h3> <table> <thead> <tr> <th>Feature</th> <th>Function-Calling (e.g. OpenAI/Google)</th> <th>MCP (Anthropic)</th> </tr> </thead> <tbody> <tr> <td><strong>Scope</strong></td> <td>Individual functions defined per-model</td> <td>Unified interface for data, tools, and prompts</td> </tr> <tr> <td><strong>Standardization</strong></td> <td>Proprietary formats (OpenAI, Google)</td> <td>Open JSON-RPC-based protocol</td> </tr> <tr> <td><strong>Interoperability</strong></td> <td>Model- and vendor-specific</td> <td>Any LLM can talk to any MCP-compliant server</td> </tr> <tr> <td><strong>State Management</strong></td> <td>Stateless (single call-response)</td> <td>Persistent sessions and complex workflows</td> </tr> <tr> <td><strong>Security</strong></td> <td>Developer-defined</td> <td>Built-in auth flows (OAuth 2.1), user approval model</td> </tr> <tr> <td><strong>Extensibility</strong></td> <td>Requires custom code per function</td> <td>Plug-and-play with reusable components</td> </tr> </tbody> </table> <h4 id=mermaid-function-calling-vs-mcp-workflow><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#mermaid-function-calling-vs-mcp-workflow>Mermaid: Function-Calling vs. MCP Workflow</a></h4> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant LLM_API as LLM (function-calling)
    participant App as Developer App
    participant API as External API
    User-&gt;&gt;LLM_API: Ask a question
    LLM_API--&gt;&gt;App: Emit function call
    App-&gt;&gt;API: Call API
    API--&gt;&gt;App: API Response
    App--&gt;&gt;LLM_API: Return result
    LLM_API--&gt;&gt;User: Final answer</code></pre> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI_Client as MCP Client
    participant MCPServer as MCP Server
    participant API as External API
    User-&gt;&gt;AI_Client: Ask a question
    AI_Client-&gt;&gt;MCPServer: tools/call
    MCPServer-&gt;&gt;API: API Call
    API--&gt;&gt;MCPServer: Response
    MCPServer--&gt;&gt;AI_Client: Result
    AI_Client--&gt;&gt;User: Final answer</code></pre> <h3 id=core-components-of-mcp><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#core-components-of-mcp>Core Components of MCP</a></h3> <h4 id=server-side-primitives><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#server-side-primitives>Server-Side Primitives</a></h4> <ul> <li><strong>Resources</strong> – Static or dynamic data sources exposed via URI (e.g. <code>file://</code>, <code>db://</code>).</li> <li><strong>Tools</strong> – Executable functions the AI can call with structured inputs/outputs.</li> <li><strong>Prompts</strong> – Predefined prompt templates or workflows returned as message sequences.</li> </ul> <h4 id=client-side-primitives><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#client-side-primitives>Client-Side Primitives</a></h4> <ul> <li><strong>Roots</strong> – Contextual entry points (e.g. current workspace, folder, URL scope).</li> <li><strong>Sampling</strong> – Allows servers to call back to the model via the client (used for chaining, agents).</li> </ul> <h4 id=mermaid-mcp-architecture><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#mermaid-mcp-architecture>Mermaid: MCP Architecture</a></h4> <pre class=mermaid><code>flowchart LR
    subgraph Host[AI Host Application]
        LLM
        Client[MCP Client]
        LLM --&gt; Client
    end
    subgraph Server1[MCP Server: Files]
        Tools1
        Resources1
        Prompts1
    end
    subgraph Server2[MCP Server: Web API]
        Tools2
        Resources2
    end
    Client -- JSON-RPC --&gt; Server1
    Client -- JSON-RPC --&gt; Server2</code></pre> <h3 id=python-tutorial-building-an-mcp-server><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#python-tutorial-building-an-mcp-server>Python Tutorial: Building an MCP Server</a></h3> <h4 id=step-1-install-sdk><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-1-install-sdk>Step 1: Install SDK</a></h4> <div class="language-bash highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a>pip<span class=w> </span>install<span class=w> </span>mcp
</span></code></pre></div> <h4 id=step-2-minimal-server><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-2-minimal-server>Step 2: Minimal Server</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <h4 id=step-3-add-a-tool><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-3-add-a-tool>Step 3: Add a Tool</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-20-1><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-20-2><a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-20-3><a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></code></pre></div> <h4 id=step-4-add-a-resource><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-4-add-a-resource>Step 4: Add a Resource</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-21-1><a id=__codelineno-21-1 name=__codelineno-21-1 href=#__codelineno-21-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;greet://</span><span class=si>{name}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-21-2><a id=__codelineno-21-2 name=__codelineno-21-2 href=#__codelineno-21-2></a><span class=k>def</span><span class=w> </span><span class=nf>greet</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-21-3><a id=__codelineno-21-3 name=__codelineno-21-3 href=#__codelineno-21-3></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Hello, </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>!&quot;</span>
</span></code></pre></div> <h4 id=step-5-add-a-prompt><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-5-add-a-prompt>Step 5: Add a Prompt</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-22-1><a id=__codelineno-22-1 name=__codelineno-22-1 href=#__codelineno-22-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp.prompts</span><span class=w> </span><span class=kn>import</span> <span class=n>base</span>
</span><span id=__span-22-2><a id=__codelineno-22-2 name=__codelineno-22-2 href=#__codelineno-22-2></a>
</span><span id=__span-22-3><a id=__codelineno-22-3 name=__codelineno-22-3 href=#__codelineno-22-3></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>prompt</span><span class=p>()</span>
</span><span id=__span-22-4><a id=__codelineno-22-4 name=__codelineno-22-4 href=#__codelineno-22-4></a><span class=k>def</span><span class=w> </span><span class=nf>solve_math</span><span class=p>(</span><span class=n>problem</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=n>base</span><span class=o>.</span><span class=n>Message</span><span class=p>]:</span>
</span><span id=__span-22-5><a id=__codelineno-22-5 name=__codelineno-22-5 href=#__codelineno-22-5></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-22-6><a id=__codelineno-22-6 name=__codelineno-22-6 href=#__codelineno-22-6></a>        <span class=n>base</span><span class=o>.</span><span class=n>SystemMessage</span><span class=p>(</span><span class=s2>&quot;You are a math assistant.&quot;</span><span class=p>),</span>
</span><span id=__span-22-7><a id=__codelineno-22-7 name=__codelineno-22-7 href=#__codelineno-22-7></a>        <span class=n>base</span><span class=o>.</span><span class=n>UserMessage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Solve: </span><span class=si>{</span><span class=n>problem</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>),</span>
</span><span id=__span-22-8><a id=__codelineno-22-8 name=__codelineno-22-8 href=#__codelineno-22-8></a>        <span class=n>base</span><span class=o>.</span><span class=n>AssistantMessage</span><span class=p>(</span><span class=s2>&quot;Let me show the steps...&quot;</span><span class=p>)</span>
</span><span id=__span-22-9><a id=__codelineno-22-9 name=__codelineno-22-9 href=#__codelineno-22-9></a>    <span class=p>]</span>
</span></code></pre></div> <h4 id=step-6-run-and-test><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#step-6-run-and-test>Step 6: Run and Test</a></h4> <div class="language-bash highlight"><pre><span></span><code><span id=__span-23-1><a id=__codelineno-23-1 name=__codelineno-23-1 href=#__codelineno-23-1></a>python<span class=w> </span>server.py
</span><span id=__span-23-2><a id=__codelineno-23-2 name=__codelineno-23-2 href=#__codelineno-23-2></a>mcp<span class=w> </span>dev<span class=w> </span>server.py
</span></code></pre></div> <h3 id=advanced-use-cases><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#advanced-use-cases>Advanced Use Cases</a></h3> <h4 id=state-management><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#state-management>State Management</a></h4> <ul> <li>Servers can hold state across requests (e.g. database session).</li> <li>Maintain open connections or cache results.</li> </ul> <h4 id=agent-frameworks><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#agent-frameworks>Agent Frameworks</a></h4> <ul> <li>LangChain supports converting MCP Tools into agent tools.</li> <li>Compatible with Claude, ChatGPT (via Agents SDK), etc.</li> </ul> <h4 id=retrieval-augmented-generation-rag><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)</a></h4> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI as Client
    participant RAGServer as Server
    participant DB as Document Store
    User-&gt;&gt;AI: Ask question
    AI-&gt;&gt;RAGServer: tools/call search
    RAGServer-&gt;&gt;DB: Query
    DB--&gt;&gt;RAGServer: Top documents
    RAGServer--&gt;&gt;AI: Text/resources
    AI--&gt;&gt;User: Informed answer</code></pre> <h4 id=multi-modal-extensions><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#multi-modal-extensions>Multi-Modal Extensions</a></h4> <ul> <li>Resources can serve images, audio, etc.</li> <li>Tools can control browsers, devices, cloud services.</li> </ul> <h4 id=security-governance><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#security-governance>Security &amp; Governance</a></h4> <ul> <li>Human approval is built-in for risky actions.</li> <li>OAuth 2.1 support for secure resource access.</li> </ul> <h3 id=conclusion><a class=toclink href=../../2025/03/31/model-context-protocol-mcp---a-quick-intro/#conclusion>Conclusion</a></h3> <p>MCP represents a new, open foundation for connecting AI to external tools and context. It standardizes how LLMs retrieve data, call functions, and use reusable prompts, making AI integration modular, scalable, and interoperable.</p> <p>Anthropic’s open protocol is being adopted across the industry, enabling a future where AI assistants plug into the real world seamlessly — through one unified protocol.</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://avatars.githubusercontent.com/u/134474669?v=4" alt="Muhammad Farooq"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-31 00:00:00+00:00">2025/03/31</time></li> <li class=md-meta__item> in <a href=../llms/ class=md-meta__link>LLMs</a>, <a href=./ class=md-meta__link>Agents</a></li> <li class=md-meta__item> 38 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=introduction><a href=../../2025/03/31/introduction/ class=toclink>Introduction</a></h2> <h3 id=what-is-mcp><a class=toclink href=../../2025/03/31/introduction/#what-is-mcp>What is MCP?</a></h3> <p>The Model Context Protocol (MCP) is an open, standardized protocol introduced by Anthropic that bridges AI models with external data sources, tools, and services. Think of MCP like a "USB-C for AI applications" – it provides a universal adapter for connecting AI assistants to various content repositories, business tools, code environments, and APIs. By defining a common interface (built on JSON-RPC 2.0) for communication, MCP enables large language models (LLMs) to invoke functions, retrieve data, or use predefined prompts from external systems in a consistent and secure way.</p> <h3 id=purpose-and-problem-addressed><a class=toclink href=../../2025/03/31/introduction/#purpose-and-problem-addressed>Purpose and Problem Addressed</a></h3> <p>MCP was designed to solve a major integration challenge often called the "M×N problem" in AI development. Traditionally, integrating M different LLMs with N different tools or data sources required custom connectors for each combination – a combinatorial explosion of ad-hoc code. This meant AI systems were largely isolated from live data, trapped behind information silos unless developers painstakingly wired in each external API or database. MCP addresses this by providing one standardized "language" for all interactions.</p> <p>In practice, developers can create one MCP-compliant interface for a data source or tool, and any MCP-enabled AI application can connect to it. This open standard thus replaces fragmented one-off integrations with a sustainable ecosystem of compatible clients and servers. The result is a simpler, more scalable way to give AI assistants access to the fresh, relevant context they need – whether it's company documents, live databases, or web results.</p> <p>Importantly, MCP is two-way and secure: it enables LLMs to query data and perform actions, while allowing organizations to keep data access controlled (the latest spec even supports OAuth 2.1 for authentication).</p> <p>In summary, MCP's purpose is to make AI integrations interoperable, secure, and context-rich, turning isolated LLMs into truly context-aware systems.</p> <h2 id=comparison-model-context-protocol-mcp-vs-simpler-function-calling-apis><a class=toclink href=../../2025/03/31/introduction/#comparison-model-context-protocol-mcp-vs-simpler-function-calling-apis>Comparison: Model Context Protocol (MCP) vs. Simpler Function-Calling APIs</a></h2> <p>MCP represents a more comprehensive and standardized approach to AI-tool integration compared to the simpler function-calling mechanisms provided by vendors like OpenAI or Google. Below we contrast MCP with typical LLM function-calling or plugin approaches:</p> <h3 id=scope-of-integration><a class=toclink href=../../2025/03/31/introduction/#scope-of-integration>Scope of Integration</a></h3> <p>Traditional function calling (e.g. OpenAI's function call API) lets a developer define a few functions that an LLM can call during a single chat session. While useful, this is relatively limited in scope – each function integration is custom and tied to one specific model or platform. In contrast, MCP covers a broader scope: it standardizes not just function calls, but also data retrieval (resources) and reusable prompt workflows across any number of tools. An MCP server can expose multiple capabilities (data, tools, prompts) at once to any compatible AI client. This turns the one-off "tool use" into a universal interface for many integrations at scale.</p> <p>For example, via MCP a single AI assistant could simultaneously connect to a code repository, a database, and a web browser tool – all through one protocol – whereas vanilla function calling would require baking in each integration separately.</p> <h3 id=standardization-and-interoperability><a class=toclink href=../../2025/03/31/introduction/#standardization-and-interoperability>Standardization and Interoperability</a></h3> <p>OpenAI's function calls and ChatGPT plugins are proprietary approaches – they work primarily within OpenAI's ecosystem (or Google's, for their functions) and follow vendor-specific formats. Each ChatGPT plugin essentially required its own mini-integration defined by an OpenAPI spec, and only certain platforms (like ChatGPT or Bing) could use those plugins.</p> <p>By contrast, MCP is an open standard from the ground up. It's vendor-agnostic and designed for broad adoption: any LLM provider or tool builder can implement MCP's JSON-RPC interface. Think of ChatGPT plugins as specialized tools in a closed toolbox, whereas MCP is an open toolkit that any AI platform or developer can utilize. This standardization means an MCP-compliant server (say for Google Drive, Slack, etc.) can be used by Anthropic's Claude, OpenAI's ChatGPT (which recently announced support), or a local open-source LLM alike. MCP's interoperability has gained momentum – even OpenAI and Microsoft have added support for it, signaling industry convergence on open standards.</p> <h3 id=state-management-and-persistence><a class=toclink href=../../2025/03/31/introduction/#state-management-and-persistence>State Management and Persistence</a></h3> <p>Simple function-calling is typically stateless beyond the immediate request. The LLM calls a function and gets a result, but there is no persistent connection – each tool use is a one-shot call embedded in the prompt/response cycle. MCP, on the other hand, establishes a persistent client–server connection. An MCP server can maintain state across multiple calls (e.g. keep a database connection open, remember prior queries, cache results, etc.). This opens the door to more complex, multi-turn interactions with tools.</p> <p>For instance, an MCP server could handle a session with an AI agent, allowing the agent to iteratively query data, refine results, or perform a sequence of actions with continuity. The client-server architecture means there's an ongoing "conversation" between the AI (client) and the tool backend (server), rather than just isolated function calls. Additionally, MCP's model of Resources (described later) allows large data to be loaded or referenced as needed, instead of stuffing everything into a single prompt window. In short, MCP is built for long-lived, context-rich interactions with external systems, unlike the transient function calls of simpler APIs.</p> <h3 id=safety-and-permissions><a class=toclink href=../../2025/03/31/introduction/#safety-and-permissions>Safety and Permissions</a></h3> <p>With basic function calling, developers must implement any approval or safety checks themselves in the loop. MCP formalizes some of this. By design, MCP assumes a human or host application is in the loop for potentially risky actions – for example, tools calls are intended to be approved by a user, and the protocol supports permissioning and authentication at a systemic level. The updated MCP spec even includes standardized OAuth 2.1 flows for granting access to protected resources. This level of built-in security and consent is beyond what the raw function-call interfaces provide, making MCP better suited for enterprise and sensitive integrations.</p> <h3 id=multi-model-flexibility><a class=toclink href=../../2025/03/31/introduction/#multi-model-flexibility>Multi-Model Flexibility</a></h3> <p>Because MCP decouples tools from any specific model, it gives developers flexibility to switch LLM providers or models without redoing integrations. For example, an application could use Anthropic's Claude today and swap to another MCP-compatible model tomorrow, and all the same MCP servers (tools) would still work. In contrast, OpenAI's function calling is tightly coupled to using OpenAI's models; there's no guarantee those function definitions would port to Google's PaLM or others. MCP's standardized interface acts as a neutral layer between models and tools.</p> <h2 id=interaction-flow-traditional-function-call-vs-mcp><a class=toclink href=../../2025/03/31/introduction/#interaction-flow-traditional-function-call-vs-mcp>Interaction Flow: Traditional Function Call vs. MCP</a></h2> <p>To illustrate the difference, let's compare the flow of an AI using an external weather API via traditional function-calling versus using MCP:</p> <h3 id=function-calling-workflow-eg-openai-api><a class=toclink href=../../2025/03/31/introduction/#function-calling-workflow-eg-openai-api>Function-Calling Workflow (e.g. OpenAI API)</a></h3> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant LLM_API as LLM (with function-calling)
    participant DevApp as Developer App
    participant WeatherAPI as Weather API
    User-&gt;&gt;LLM_API: "What is the weather in LA tomorrow?"
    note right of LLM_API: LLM decides a function is needed&lt;br/&gt;from provided definitions
    LLM_API--&gt;&gt;DevApp: Function call output (e.g. `get_weather(location="LA")`)
    DevApp-&gt;&gt;WeatherAPI: Invoke weather API (with location="LA")
    WeatherAPI--&gt;&gt;DevApp: Returns weather data
    DevApp--&gt;&gt;LLM_API: Provide function result (weather data)
    LLM_API--&gt;&gt;User: Final answer using the data</code></pre> <p>In the above function-calling flow, the developer had to predefine a get_weather function in the prompt and intercept the model's output to call the API. The interaction with the external service (Weather API) is not standardized – it's custom code in the developer's app, and the connection is not persistent (just a single call). The LLM itself doesn't maintain a direct connection to tools; it relies on the developer to mediate every call.</p> <h3 id=mcp-workflow-clientserver-with-weather-tool><a class=toclink href=../../2025/03/31/introduction/#mcp-workflow-clientserver-with-weather-tool>MCP Workflow (Client–Server with Weather Tool)</a></h3> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI_Client as AI App (MCP Client)
    participant MCPServer as Weather MCP Server
    participant WeatherAPI as Weather API
    User-&gt;&gt;AI_Client: "What is the weather in LA tomorrow?"
    AI_Client-&gt;&gt;MCPServer: (via MCP) Request tool "get-forecast" with {"location":"LA"}
    MCPServer-&gt;&gt;WeatherAPI: Fetch forecast for LA
    WeatherAPI--&gt;&gt;MCPServer: Weather data
    MCPServer--&gt;&gt;AI_Client: Result of get-forecast (data or summary)
    AI_Client--&gt;&gt;User: Final answer incorporating weather info</code></pre> <p>In the MCP flow, the AI application (client) has an established connection to a Weather MCP server that exposes a get-forecast tool. The LLM (e.g. Claude or ChatGPT with MCP support) can directly trigger that tool via the MCP client, without the developer writing custom code for this API call. The MCP server handles communicating with the Weather API and returns the result in a standardized format. The AI model receives the weather info through MCP and can respond to the user.</p> <p>Notably, this happens through a consistent protocol – any other MCP-compatible weather service would work the same way. Also, the connection can be two-way: if the Weather server had multiple actions or needed additional queries, the conversation over MCP could continue beyond a one-shot call. This demonstrates how MCP generalizes and extends the idea of function calling into a full client–server integration pattern.</p> <h2 id=core-components-of-mcp><a class=toclink href=../../2025/03/31/introduction/#core-components-of-mcp>Core Components of MCP</a></h2> <p>MCP defines a set of core components (primitives) that structure how clients and servers interact and what they can do. There are three server-side primitives (which servers provide) and two client-side primitives (which clients provide). Understanding these components is key to using MCP effectively:</p> <h3 id=server-side-primitives><a class=toclink href=../../2025/03/31/introduction/#server-side-primitives>Server-Side Primitives</a></h3> <p>These are capabilities that an MCP Server exposes to add context or functionality for the LLM. An MCP server can implement any or all of these:</p> <h4 id=resources><a class=toclink href=../../2025/03/31/introduction/#resources>Resources</a></h4> <p>Resources are pieces of data or content that the server makes available for the AI to read and include in its context. A resource might be a file's contents, a database record, an email, an image, etc. – anything that could be useful as additional information for the model. Each resource is identified by a URI (like file://docs/report.pdf or db://customers/123) and can be fetched via the protocol.</p> <p>Importantly, resources are typically read-only context: the AI doesn't execute them, it uses them as reference material. For example, a "Docs Server" might expose a document's text as a resource so the AI can quote or summarize it. Resources are usually application-controlled, meaning the client or user decides which resources to pull in (to avoid flooding the model with irrelevant data). In practice, an AI interface might let a user pick a file (resource) to share with the assistant. If truly on-demand model-driven access is needed instead, that's where tools come in. Resources help inject structured data into the LLM's prompt context in a standardized way.</p> <h4 id=tools><a class=toclink href=../../2025/03/31/introduction/#tools>Tools</a></h4> <p>Tools are executable functions that the server can perform on request. These are analogous to the "functions" in function-calling, but defined in a standard JSON schema format and invoked via MCP endpoints. Tools allow the AI to perform actions or fetch calculated information – for example, run a database query, call an external API, execute a computation, or even control a web browser.</p> <p>Each tool has a name, a description, and a JSON schema for its input parameters. The server lists available tools, and the AI (through the client) can call them by name with the required args. Tools are model-controlled with a human in the loop. This means the design assumes the AI can decide when to use a tool (e.g. the LLM's reasoning says "I should use the database_query tool now"), but the user or client must approve the action for safety.</p> <p>When invoked, the server executes the underlying function and returns the result (or error) to the client. Tools can be simple (e.g. a calculator) or very powerful (e.g. a tool that can send an email or modify data). Unlike resources, tools can have side effects or dynamic outputs – they may change external state or retrieve live data. This makes them essential for building AI agents that act, not just observe.</p> <h4 id=prompts><a class=toclink href=../../2025/03/31/introduction/#prompts>Prompts</a></h4> <p>Prompts in MCP are reusable prompt templates or workflows that servers provide. A Prompt primitive is essentially a predefined way to interact with the model, which could involve multiple steps or a structured input. Servers define prompts to standardize common interactions – for example, a prompt template to "Summarize document X in style Y", or a multi-turn workflow to "Debug an error by asking these follow-up questions".</p> <p>Each prompt is identified by a name and can accept input arguments (e.g. the document to summarize, the style to use). The client can query the server for available prompts, which might be presented to the user (for instance, an IDE could show a list of MCP prompt actions like "Explain this code" powered by the server). When a prompt is selected, the server can then guide the interaction: it might inject certain instructions to the model, include relevant resources automatically, or even orchestrate a chain of LLM calls.</p> <p>Prompts are typically user-controlled, meaning the user explicitly triggers those workflows (like choosing a predefined query or action). Under the hood, a prompt might use the other primitives – e.g. it could fetch some resource or call a tool as part of its process – to produce the final result. Prompts let developers encapsulate complex behaviors or multi-step conversations behind a single command, making them easy to reuse.</p> <p>In summary, on the server side: Resources = data context, Tools = actions/functions, and Prompts = preset conversational patterns. These primitives "speak" JSON-RPC – e.g. there are standard methods like resources/list, resources/read, tools/list, tools/call, prompts/list, etc., which the client can call to discover and use these primitives. By separating them, MCP makes clear what the AI is intending to do – whether it's just reading data, executing a function, or following a guided script, each has a defined protocol.</p> <h3 id=client-side-primitives><a class=toclink href=../../2025/03/31/introduction/#client-side-primitives>Client-Side Primitives</a></h3> <p>These are features that an MCP Client (host application) provides, which servers can leverage. There are two main client-side primitives:</p> <h4 id=roots><a class=toclink href=../../2025/03/31/introduction/#roots>Roots</a></h4> <p>A Root is essentially a boundary or entry point that the client suggests to the server for where to focus. When an MCP connection starts, the client can send one or more "root URIs" to the server. This informs the server about the relevant workspace or scope.</p> <p>For instance, if a developer is working in /home/user/myproject/, the client might set that as a root for a Filesystem server – indicating the server should consider that directory as the project context (and not roam outside it). Or a root could be a specific URL or database name that the server should use as the primary endpoint. Roots thus provide guidance and organization, especially when multiple data sources are in use.</p> <p>They do not hard-enforce access limitations (the server could technically go beyond, unless the server itself restricts it), but they serve as a contract of what the current context is. This helps keep interactions focused and secure – the server knows what subset of data it should operate on, and the client knows the server won't unexpectedly access unrelated data. Not all scenarios require roots, but they are very useful in development tools (for setting project scope) and similar contexts.</p> <h4 id=sampling><a class=toclink href=../../2025/03/31/introduction/#sampling>Sampling</a></h4> <p>Sampling is a powerful client-side feature that allows the server to ask the client's LLM to generate a completion. In simpler terms, it lets the server turn around and say: "I need the AI to complete/answer this sub-task for me." This might sound unusual, but it enables advanced workflows like agents that can reason recursively.</p> <p>For example, imagine a complex tool that, in the middle of its function, realizes it needs an LLM's help to parse something or make a decision – the server can send a sampling request to the client, providing a prompt, and the client will invoke the LLM to get a result, then return it to the server. The server can then proceed using that result. All of this happens under the hood of the protocol, with the crucial caveat that the user (or host application) should approve any such additional LLM call.</p> <p>The design is meant for "agents calling themselves," but with human oversight to avoid infinite loops or undesired actions. Sampling requests include a formatted message (or conversation) that the server wants the model to continue, and possibly preferences for which model or how to balance speed vs. accuracy.</p> <p>This feature essentially lets the server compose intelligence: an MCP server could chain multiple LLM calls or do intermediate reasoning by leveraging the client's model. It's an advanced capability (not all clients support it yet, and it should be used judiciously), but it opens the door to sophisticated agent behaviors. With Sampling, MCP isn't just one LLM interacting with tools – it could be tools temporarily invoking the LLM in return, enabling nested AI reasoning.</p> <p>These five components – Resources, Tools, Prompts, Roots, and Sampling – form the core of MCP's design. The protocol is essentially a set of JSON-RPC methods that implement these primitives in a standardized way. By mixing and matching them, one can achieve various integration patterns. For example, a given MCP server might only implement Tools (if it purely offers actions like a Calculator server), or implement Resources + Prompts (if it mainly offers data and some templated queries on that data), etc. Clients similarly may or may not support Sampling or Roots depending on the application. The upshot is that MCP gives a common structure to describe "what an AI can do" in a given context: read this data, use these tools, follow these templates.</p> <h2 id=architecture-clientserver-design><a class=toclink href=../../2025/03/31/introduction/#architecture-clientserver-design>Architecture: Client–Server Design</a></h2> <p>At a high level, MCP follows a classic client–server architecture tailored to AI needs. The design involves a Host application that incorporates an MCP client, which connects to one or more external MCP servers. Let's break down the pieces:</p> <h3 id=mcp-host-and-client><a class=toclink href=../../2025/03/31/introduction/#mcp-host-and-client>MCP Host and Client</a></h3> <p>The host is the main AI application or agent environment. For example, Claude Desktop (Anthropic's chat app) is an MCP host, as are certain IDE extensions, chat UIs, or agent orchestration frameworks. The host contains the MCP client component, which is responsible for managing the connection to servers and mediating between the LLM and those servers.</p> <p>The client knows how to speak MCP (it implements the protocol on the client side: sending requests like tools/call to servers, and handling incoming requests like sampling/createMessage from servers). It maintains a 1:1 connection with each server it uses. In practice, a host might spawn multiple client connections if it's using multiple servers at once (for example, connect to a GitHub server and a Slack server concurrently).</p> <p>The LLM itself (the model) is usually integrated into the host – for instance, Claude (the model) running locally or via API is what generates the responses, and the MCP client is feeding it information and tool results.</p> <h3 id=mcp-server><a class=toclink href=../../2025/03/31/introduction/#mcp-server>MCP Server</a></h3> <p>An MCP server is a lightweight program or process that exposes a specific set of capabilities (the primitives we discussed) via the MCP protocol. Each server typically corresponds to a particular domain or service. For example, one server might interface with a file system (exposing files as resources, and perhaps a prompt for searching files), another might interface with Google Drive, another with GitHub, a database, or a web browser, etc.</p> <p>The server runs separately from the AI model – it could be on the same machine or a remote one – and communicates with the client over a communication channel. Servers declare what capabilities they have during initialization (for instance, "I support resources and tools, but not prompts") so the client knows how to interact.</p> <p>Importantly, servers are stateless with respect to the protocol (they handle requests as they come) but can maintain internal state or connections (e.g. keep a DB session). They do not directly talk to the LLM; instead, all interaction is through the structured protocol messages.</p> <h3 id=communication-transports-json-rpc><a class=toclink href=../../2025/03/31/introduction/#communication-transports-json-rpc>Communication (Transports &amp; JSON-RPC)</a></h3> <p>MCP's communication is built on JSON-RPC 2.0 as the message format. This means every action (like listing tools or calling a resource) is a JSON request with a method name and params, and responses are JSON objects with results or errors. The protocol supports both requests (with responses) and notifications (one-way messages) in both directions – so the server can call methods on the client (like sampling/createMessage) and the client can call methods on the server (like tools/call).</p> <p>The underlying transport layer can vary. Two common transport modes are:</p> <p><strong>Stdio (Standard I/O)</strong>: Ideal for local setups, the client can launch the server as a subprocess and communicate via its stdin/stdout streams. This is simple and secure for local integrations – e.g. Claude Desktop can run an MCP server on your machine via stdio.</p> <p><strong>HTTP (with SSE or WebSocket)</strong>: For remote servers, MCP can run over HTTP. Initially, the spec used HTTP + Server-Sent Events (SSE) for server-to-client streaming, but an updated streamable HTTP transport was introduced (Mar 2025) to allow full bidirectional streaming over a single connection. In either case, the client might make HTTP POST requests for each JSON-RPC call and keep a channel open for streaming responses. WebSocket is another possible transport for continuous two-way communication.</p> <p>The transport is abstracted in MCP – developers don't usually worry about it beyond choosing one that fits their deployment. The key is that client and server establish a connection and exchange JSON-RPC messages. The first messages in a session are an initial handshake: the client sends an initialize request (with its MCP version and what features it supports, e.g. "I support roots and sampling") and the server responds with its own capabilities and version. Once initialization is done, they exchange an initialized notification and then the session is ready for general use.</p> <p>Either side can then send requests or notifications according to the protocol. For example, right after init, the client might call prompts/list or tools/list to discover what the server offers. From then on, the LLM (through the client) can start using those tools/resources.</p> <h3 id=lifecycle><a class=toclink href=../../2025/03/31/introduction/#lifecycle>Lifecycle</a></h3> <p>The MCP connection stays alive as long as needed. If the user closes the host app or a certain task is done, the client can send a shutdown message or simply disconnect the transport, ending the session. Servers and clients are expected to handle disconnects or errors gracefully (MCP defines standard error codes for invalid requests, internal errors, etc., similar to JSON-RPC's spec).</p> <p>Below is a diagram of the core MCP architecture with its main components and data flow:</p> <pre class=mermaid><code>flowchart LR
    subgraph AI_Host[Host Application]
        direction TB
        AILLM["LLM (AI Assistant)"]
        MCPClient["MCP Client Component"]
        AILLM -- uses --&gt; MCPClient
    end
    subgraph MCP_Server1["MCP Server: Tool/Data Source 1"]
        direction TB
        Tools1["Tools"]
        Resources1["Resources"]
        Prompts1["Prompts"]
    end
    subgraph MCP_Server2["MCP Server: Tool/Data Source 2"]
        direction TB
        Tools2["Tools"]
        Resources2["Resources"]
        %% Prompts2 not implemented in this one for illustration
    end
    %% Host connects to multiple servers
    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server1
    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server2
    Resources1 -- fetch data --&gt; DataSource1["Local/Remote Data\n(e.g. files, DB)"]
    Tools1 -- perform actions --&gt; DataSource1
    Tools2 -- perform actions --&gt; Service2["External Service API"]
    Resources2 -- fetch data --&gt; Service2</code></pre> <p>In this diagram, the Host application (which contains the AI model and the MCP client library) maintains connections to two MCP servers. Server1 might be a local server (e.g. providing file system access – hence its tools/resources tie into local files or a database), and Server2 might be a remote service integration (e.g. a weather API server, which calls an external web service). The MCP client handles the JSON-RPC messaging to each server.</p> <p>When the LLM needs something, it formulates a request that the client sends to the appropriate server; when a server needs something (like performing a sampling request or notifying of new data), it sends that to the client. The primitives (Tools, Resources, Prompts) inside each server define what it can do.</p> <p>For example, if the user asks the AI "Summarize the latest sales data", the AI client might use a Prompts primitive from Server1 that knows how to retrieve sales records (as Resources) and then guide the model to summarize them. Meanwhile, if the user asks "Also, what's the weather in LA?", the AI client can call a Tool on Server2 to get that info. All of this happens through the uniform MCP interface.</p> <p>As the ecosystem grows, we might have dozens of servers (for Git, Jira, Gmail, etc.), but crucially, an AI app doesn't need to have custom code for each – it just speaks MCP to whichever servers are available.</p> <h3 id=standardization><a class=toclink href=../../2025/03/31/introduction/#standardization>Standardization</a></h3> <p>Because both client and server follow the MCP spec, any compatible client can talk to any server. This fosters an ecosystem where companies and the community are building a "library" of MCP servers for many common tools. Anthropic has open-sourced reference servers for Google Drive, Slack, Git, GitHub, Postgres, web browser automation, and more. Community contributors have added many others (at the time of writing, hundreds of MCP connectors exist for various services).</p> <p>On the client side, multiple products have added MCP support (Claude's apps, developer IDE plugins, agent frameworks like LangChain, etc.). This architecture and standardization are what turn MCP into that "universal port" for AI – it decouples the integration logic (server side) from the AI agent logic (client side), with a well-defined protocol in between.</p> <h2 id=python-tutorial-building-and-using-mcp-step-by-step><a class=toclink href=../../2025/03/31/introduction/#python-tutorial-building-and-using-mcp-step-by-step>Python Tutorial: Building and Using MCP (Step-by-Step)</a></h2> <p>Now that we've covered concepts, let's get hands-on with a Python tutorial using MCP. We'll walk through creating a simple MCP server and demonstrate a client interacting with it. For this tutorial, we'll use the official MCP Python SDK (open-sourced by Anthropic) for convenience. (The SDK abstracts a lot of JSON-RPC boilerplate and lets us focus on defining our tools/resources). If you haven't already, you can install the SDK via pip:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-57-1><a id=__codelineno-57-1 name=__codelineno-57-1 href=#__codelineno-57-1></a>pip<span class=w> </span>install<span class=w> </span>mcp
</span></code></pre></div> <p>(The package name is mcp. This includes the server framework and some CLI tools.) Let's proceed in steps:</p> <h3 id=1-setting-up-a-basic-mcp-server><a class=toclink href=../../2025/03/31/introduction/#1-setting-up-a-basic-mcp-server>1. Setting Up a Basic MCP Server</a></h3> <p>First, we'll create a minimal MCP server script in Python. This server will expose a trivial functionality just to verify everything works. We'll use the SDK's FastMCP class to create a server instance and run it.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-58-1><a id=__codelineno-58-1 name=__codelineno-58-1 href=#__codelineno-58-1></a><span class=c1># server.py</span>
</span><span id=__span-58-2><a id=__codelineno-58-2 name=__codelineno-58-2 href=#__codelineno-58-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-58-3><a id=__codelineno-58-3 name=__codelineno-58-3 href=#__codelineno-58-3></a>
</span><span id=__span-58-4><a id=__codelineno-58-4 name=__codelineno-58-4 href=#__codelineno-58-4></a><span class=c1># Initialize an MCP server with a name</span>
</span><span id=__span-58-5><a id=__codelineno-58-5 name=__codelineno-58-5 href=#__codelineno-58-5></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-58-6><a id=__codelineno-58-6 name=__codelineno-58-6 href=#__codelineno-58-6></a>
</span><span id=__span-58-7><a id=__codelineno-58-7 name=__codelineno-58-7 href=#__codelineno-58-7></a><span class=c1># Start the server (using stdio transport by default).</span>
</span><span id=__span-58-8><a id=__codelineno-58-8 name=__codelineno-58-8 href=#__codelineno-58-8></a><span class=c1># This call will block and wait for a client connection.</span>
</span><span id=__span-58-9><a id=__codelineno-58-9 name=__codelineno-58-9 href=#__codelineno-58-9></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>In this snippet, we import FastMCP and instantiate it with a name "Demo Server". The FastMCP class is a high-level server that handles MCP protocol compliance and transport setup for us. By calling mcp.serve(), we tell it to begin listening for an MCP client. By default this uses the standard I/O transport (suitable if this script is launched by a host app). At this point, our server doesn't actually expose any tools or resources yet – it's an empty shell. But it can respond to basic protocol requests like initialize and will report that it has no capabilities.</p> <h3 id=2-exposing-a-tool-function-on-the-server><a class=toclink href=../../2025/03/31/introduction/#2-exposing-a-tool-function-on-the-server>2. Exposing a Tool (Function) on the Server</a></h3> <p>Now we'll add a simple Tool to our server. The SDK provides a convenient decorator <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.tool to turn a Python function into an MCP tool. For demonstration, we'll add a basic arithmetic tool and then run the server.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-59-1><a id=__codelineno-59-1 name=__codelineno-59-1 href=#__codelineno-59-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-59-2><a id=__codelineno-59-2 name=__codelineno-59-2 href=#__codelineno-59-2></a>
</span><span id=__span-59-3><a id=__codelineno-59-3 name=__codelineno-59-3 href=#__codelineno-59-3></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-59-4><a id=__codelineno-59-4 name=__codelineno-59-4 href=#__codelineno-59-4></a>
</span><span id=__span-59-5><a id=__codelineno-59-5 name=__codelineno-59-5 href=#__codelineno-59-5></a><span class=c1># Define a tool using a decorator. This tool adds two numbers.</span>
</span><span id=__span-59-6><a id=__codelineno-59-6 name=__codelineno-59-6 href=#__codelineno-59-6></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-59-7><a id=__codelineno-59-7 name=__codelineno-59-7 href=#__codelineno-59-7></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-59-8><a id=__codelineno-59-8 name=__codelineno-59-8 href=#__codelineno-59-8></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-59-9><a id=__codelineno-59-9 name=__codelineno-59-9 href=#__codelineno-59-9></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-59-10><a id=__codelineno-59-10 name=__codelineno-59-10 href=#__codelineno-59-10></a>
</span><span id=__span-59-11><a id=__codelineno-59-11 name=__codelineno-59-11 href=#__codelineno-59-11></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>With the <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.tool() decorator, the SDK automatically registers our add function as a Tool primitive. Under the hood, it will assign it a name (defaulting to the function name "add" here) and generate a JSON schema for the input parameters a and b (both integers, in this case). It also captures the docstring as the tool's description, which is useful for the AI to understand what it does.</p> <p>When this server is running, an MCP client that connects can call tools/list and will see something like:</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-60-1><a id=__codelineno-60-1 name=__codelineno-60-1 href=#__codelineno-60-1></a><span class=p>{</span>
</span><span id=__span-60-2><a id=__codelineno-60-2 name=__codelineno-60-2 href=#__codelineno-60-2></a><span class=w>  </span><span class=nt>&quot;tools&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-60-3><a id=__codelineno-60-3 name=__codelineno-60-3 href=#__codelineno-60-3></a><span class=w>    </span><span class=p>{</span>
</span><span id=__span-60-4><a id=__codelineno-60-4 name=__codelineno-60-4 href=#__codelineno-60-4></a><span class=w>      </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;add&quot;</span><span class=p>,</span>
</span><span id=__span-60-5><a id=__codelineno-60-5 name=__codelineno-60-5 href=#__codelineno-60-5></a><span class=w>      </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;Add two numbers and return the result.&quot;</span><span class=p>,</span>
</span><span id=__span-60-6><a id=__codelineno-60-6 name=__codelineno-60-6 href=#__codelineno-60-6></a><span class=w>      </span><span class=nt>&quot;inputSchema&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-60-7><a id=__codelineno-60-7 name=__codelineno-60-7 href=#__codelineno-60-7></a><span class=w>        </span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;object&quot;</span><span class=p>,</span>
</span><span id=__span-60-8><a id=__codelineno-60-8 name=__codelineno-60-8 href=#__codelineno-60-8></a><span class=w>        </span><span class=nt>&quot;properties&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-60-9><a id=__codelineno-60-9 name=__codelineno-60-9 href=#__codelineno-60-9></a><span class=w>          </span><span class=nt>&quot;a&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;integer&quot;</span><span class=p>},</span>
</span><span id=__span-60-10><a id=__codelineno-60-10 name=__codelineno-60-10 href=#__codelineno-60-10></a><span class=w>          </span><span class=nt>&quot;b&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;integer&quot;</span><span class=p>}</span>
</span><span id=__span-60-11><a id=__codelineno-60-11 name=__codelineno-60-11 href=#__codelineno-60-11></a><span class=w>        </span><span class=p>}</span>
</span><span id=__span-60-12><a id=__codelineno-60-12 name=__codelineno-60-12 href=#__codelineno-60-12></a><span class=w>      </span><span class=p>}</span>
</span><span id=__span-60-13><a id=__codelineno-60-13 name=__codelineno-60-13 href=#__codelineno-60-13></a><span class=w>    </span><span class=p>}</span>
</span><span id=__span-60-14><a id=__codelineno-60-14 name=__codelineno-60-14 href=#__codelineno-60-14></a><span class=w>  </span><span class=p>]</span>
</span><span id=__span-60-15><a id=__codelineno-60-15 name=__codelineno-60-15 href=#__codelineno-60-15></a><span class=p>}</span>
</span></code></pre></div> <p>This is how the client discovers what functions are available. The inputSchema is derived from our function signature (using type hints, or it would default to accepting arbitrary JSON structure if not annotated). Now the add tool can be invoked via the MCP method tools/call with appropriate parameters.</p> <p>Let's test our server's tool quickly (in a real scenario, we'd connect an actual client, but here we'll simulate what a client would do for illustration). We can write a tiny MCP client snippet or use the SDK's CLI. For simplicity, imagine the client calls:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-61-1><a id=__codelineno-61-1 name=__codelineno-61-1 href=#__codelineno-61-1></a><span class=c1># Pseudo-client code (for illustration only)</span>
</span><span id=__span-61-2><a id=__codelineno-61-2 name=__codelineno-61-2 href=#__codelineno-61-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp</span><span class=w> </span><span class=kn>import</span> <span class=n>Client</span>  <span class=c1># assume the SDK has a Client class for connecting</span>
</span><span id=__span-61-3><a id=__codelineno-61-3 name=__codelineno-61-3 href=#__codelineno-61-3></a>
</span><span id=__span-61-4><a id=__codelineno-61-4 name=__codelineno-61-4 href=#__codelineno-61-4></a><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=o>.</span><span class=n>connect_stdio</span><span class=p>(</span><span class=s2>&quot;./server.py&quot;</span><span class=p>)</span>   <span class=c1># Launch server.py and connect via stdio</span>
</span><span id=__span-61-5><a id=__codelineno-61-5 name=__codelineno-61-5 href=#__codelineno-61-5></a><span class=n>tools</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>list_tools</span><span class=p>()</span>                    <span class=c1># should list the &quot;add&quot; tool</span>
</span><span id=__span-61-6><a id=__codelineno-61-6 name=__codelineno-61-6 href=#__codelineno-61-6></a><span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>call_tool</span><span class=p>(</span><span class=s2>&quot;add&quot;</span><span class=p>,</span> <span class=p>{</span><span class=s2>&quot;a&quot;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s2>&quot;b&quot;</span><span class=p>:</span> <span class=mi>4</span><span class=p>})</span>
</span><span id=__span-61-7><a id=__codelineno-61-7 name=__codelineno-61-7 href=#__codelineno-61-7></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>  <span class=c1># expected output: 7</span>
</span><span id=__span-61-8><a id=__codelineno-61-8 name=__codelineno-61-8 href=#__codelineno-61-8></a><span class=n>client</span><span class=o>.</span><span class=n>disconnect</span><span class=p>()</span>
</span></code></pre></div> <p>In reality, one might use the mcp CLI tool: for example, running mcp dev server.py will start the server and open an interactive interface (MCP Inspector) where you can manually invoke tools and see the results. Or, if using Claude Desktop, one could configure it to run this server and then simply ask Claude, "What's 3+4?" – the model could decide to use the add tool and would get the answer 7 from the server.</p> <p>The key takeaway is that adding a tool is as simple as defining a Python function with the SDK; MCP takes care of exposing it in a standardized way.</p> <h3 id=3-exposing-a-resource-data-on-the-server><a class=toclink href=../../2025/03/31/introduction/#3-exposing-a-resource-data-on-the-server>3. Exposing a Resource (Data) on the Server</a></h3> <p>Let's extend our server to also provide a Resource. We'll make a resource that gives a friendly greeting. This will demonstrate how resources can supply text for the model's context. We use the <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.resource("uri_pattern") decorator to register a resource handler.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-62-1><a id=__codelineno-62-1 name=__codelineno-62-1 href=#__codelineno-62-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-62-2><a id=__codelineno-62-2 name=__codelineno-62-2 href=#__codelineno-62-2></a>
</span><span id=__span-62-3><a id=__codelineno-62-3 name=__codelineno-62-3 href=#__codelineno-62-3></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-62-4><a id=__codelineno-62-4 name=__codelineno-62-4 href=#__codelineno-62-4></a>
</span><span id=__span-62-5><a id=__codelineno-62-5 name=__codelineno-62-5 href=#__codelineno-62-5></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-62-6><a id=__codelineno-62-6 name=__codelineno-62-6 href=#__codelineno-62-6></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-62-7><a id=__codelineno-62-7 name=__codelineno-62-7 href=#__codelineno-62-7></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-62-8><a id=__codelineno-62-8 name=__codelineno-62-8 href=#__codelineno-62-8></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-62-9><a id=__codelineno-62-9 name=__codelineno-62-9 href=#__codelineno-62-9></a>
</span><span id=__span-62-10><a id=__codelineno-62-10 name=__codelineno-62-10 href=#__codelineno-62-10></a><span class=c1># Define a resource. The URI pattern {name} means the client can request e.g. &quot;greet://Alice&quot;</span>
</span><span id=__span-62-11><a id=__codelineno-62-11 name=__codelineno-62-11 href=#__codelineno-62-11></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;greet://</span><span class=si>{name}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-62-12><a id=__codelineno-62-12 name=__codelineno-62-12 href=#__codelineno-62-12></a><span class=k>def</span><span class=w> </span><span class=nf>get_greeting</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-62-13><a id=__codelineno-62-13 name=__codelineno-62-13 href=#__codelineno-62-13></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Resource content: returns a greeting for the given name.&quot;&quot;&quot;</span>
</span><span id=__span-62-14><a id=__codelineno-62-14 name=__codelineno-62-14 href=#__codelineno-62-14></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Hello, </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>! Welcome to MCP.&quot;</span>
</span><span id=__span-62-15><a id=__codelineno-62-15 name=__codelineno-62-15 href=#__codelineno-62-15></a>
</span><span id=__span-62-16><a id=__codelineno-62-16 name=__codelineno-62-16 href=#__codelineno-62-16></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>Here we used <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.resource("greet://{name}"). This tells the server that it can handle any resource URI of the form greet://<something> – the <something> will be passed as the name argument to our function get_greeting. So if the client later requests the resource greet://Alice, our function returns "Hello, Alice! Welcome to MCP.". The server will transmit that string as the content of the resource. We provided a description in the docstring as well.</p> <p>The client's perspective: if it calls resources/list, it might not list all possible greetings (since {name} is a parameterized resource), but some servers do list resource "directories" or examples. In any case, the client can directly do a resources/read on a specific URI. For example, an AI client could be prompted (via system message) that greet://{name} resources are available for use. If the user asks "Can you greet Alice?", the AI could fetch greet://Alice from the server (the client would call resources/read with uri: "greet://Alice"). The server invokes get_greeting("Alice") and returns the text. The LLM then sees that text and can respond to the user with it.</p> <p>Resources are a nice way to serve static or computed data that the model might include in its answer verbatim. They behave somewhat like HTTP GET endpoints (in REST analogy).</p> <p>Now our Demo Server has both a Tool (add) and a Resource (greet://{name}).</p> <h3 id=4-adding-a-prompt-reusable-template><a class=toclink href=../../2025/03/31/introduction/#4-adding-a-prompt-reusable-template>4. Adding a Prompt (Reusable Template)</a></h3> <p>For completeness, let's add a Prompt to our server. Suppose we want a standardized way for the AI to request a calculation and explanation. We'll create a prompt that, given a math problem, instructs the model to solve it step-by-step.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-63-1><a id=__codelineno-63-1 name=__codelineno-63-1 href=#__codelineno-63-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-63-2><a id=__codelineno-63-2 name=__codelineno-63-2 href=#__codelineno-63-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp.prompts</span><span class=w> </span><span class=kn>import</span> <span class=n>base</span>  <span class=c1># utility for prompt message objects</span>
</span><span id=__span-63-3><a id=__codelineno-63-3 name=__codelineno-63-3 href=#__codelineno-63-3></a>
</span><span id=__span-63-4><a id=__codelineno-63-4 name=__codelineno-63-4 href=#__codelineno-63-4></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-63-5><a id=__codelineno-63-5 name=__codelineno-63-5 href=#__codelineno-63-5></a>
</span><span id=__span-63-6><a id=__codelineno-63-6 name=__codelineno-63-6 href=#__codelineno-63-6></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-63-7><a id=__codelineno-63-7 name=__codelineno-63-7 href=#__codelineno-63-7></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-63-8><a id=__codelineno-63-8 name=__codelineno-63-8 href=#__codelineno-63-8></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-63-9><a id=__codelineno-63-9 name=__codelineno-63-9 href=#__codelineno-63-9></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-63-10><a id=__codelineno-63-10 name=__codelineno-63-10 href=#__codelineno-63-10></a>
</span><span id=__span-63-11><a id=__codelineno-63-11 name=__codelineno-63-11 href=#__codelineno-63-11></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;greet://</span><span class=si>{name}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-63-12><a id=__codelineno-63-12 name=__codelineno-63-12 href=#__codelineno-63-12></a><span class=k>def</span><span class=w> </span><span class=nf>get_greeting</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-63-13><a id=__codelineno-63-13 name=__codelineno-63-13 href=#__codelineno-63-13></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Resource content: returns a greeting for the given name.&quot;&quot;&quot;</span>
</span><span id=__span-63-14><a id=__codelineno-63-14 name=__codelineno-63-14 href=#__codelineno-63-14></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Hello, </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>! Welcome to MCP.&quot;</span>
</span><span id=__span-63-15><a id=__codelineno-63-15 name=__codelineno-63-15 href=#__codelineno-63-15></a>
</span><span id=__span-63-16><a id=__codelineno-63-16 name=__codelineno-63-16 href=#__codelineno-63-16></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>prompt</span><span class=p>()</span>
</span><span id=__span-63-17><a id=__codelineno-63-17 name=__codelineno-63-17 href=#__codelineno-63-17></a><span class=k>def</span><span class=w> </span><span class=nf>solve_math</span><span class=p>(</span><span class=n>problem</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=n>base</span><span class=o>.</span><span class=n>Message</span><span class=p>]:</span>
</span><span id=__span-63-18><a id=__codelineno-63-18 name=__codelineno-63-18 href=#__codelineno-63-18></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Prompt: guides the model to solve a math problem.&quot;&quot;&quot;</span>
</span><span id=__span-63-19><a id=__codelineno-63-19 name=__codelineno-63-19 href=#__codelineno-63-19></a>    <span class=c1># This returns a conversation (list of messages) as a template</span>
</span><span id=__span-63-20><a id=__codelineno-63-20 name=__codelineno-63-20 href=#__codelineno-63-20></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-63-21><a id=__codelineno-63-21 name=__codelineno-63-21 href=#__codelineno-63-21></a>        <span class=n>base</span><span class=o>.</span><span class=n>SystemMessage</span><span class=p>(</span><span class=s2>&quot;You are a helpful math assistant.&quot;</span><span class=p>),</span>
</span><span id=__span-63-22><a id=__codelineno-63-22 name=__codelineno-63-22 href=#__codelineno-63-22></a>        <span class=n>base</span><span class=o>.</span><span class=n>UserMessage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Please solve the following problem: </span><span class=si>{</span><span class=n>problem</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>),</span>
</span><span id=__span-63-23><a id=__codelineno-63-23 name=__codelineno-63-23 href=#__codelineno-63-23></a>        <span class=n>base</span><span class=o>.</span><span class=n>AssistantMessage</span><span class=p>(</span><span class=s2>&quot;Sure, let me break it down...&quot;</span><span class=p>),</span>
</span><span id=__span-63-24><a id=__codelineno-63-24 name=__codelineno-63-24 href=#__codelineno-63-24></a>    <span class=p>]</span>
</span><span id=__span-63-25><a id=__codelineno-63-25 name=__codelineno-63-25 href=#__codelineno-63-25></a>
</span><span id=__span-63-26><a id=__codelineno-63-26 name=__codelineno-63-26 href=#__codelineno-63-26></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>We used <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.prompt() to define solve_math. In this case, our function returns a list of message objects (using the SDK's base.SystemMessage, base.UserMessage, etc., which likely correspond to the roles in a chat prompt). This defines a prompt template where the assistant is primed with a system role and an initial user request, and even an initial assistant response to indicate it will explain step-by-step. The specifics aren't too important – the idea is that the server can package this multi-turn template and offer it as a named prompt.</p> <p>The client will see this if it calls prompts/list:</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-64-1><a id=__codelineno-64-1 name=__codelineno-64-1 href=#__codelineno-64-1></a><span class=p>{</span>
</span><span id=__span-64-2><a id=__codelineno-64-2 name=__codelineno-64-2 href=#__codelineno-64-2></a><span class=w>  </span><span class=nt>&quot;prompts&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-64-3><a id=__codelineno-64-3 name=__codelineno-64-3 href=#__codelineno-64-3></a><span class=w>    </span><span class=p>{</span>
</span><span id=__span-64-4><a id=__codelineno-64-4 name=__codelineno-64-4 href=#__codelineno-64-4></a><span class=w>      </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;solve_math&quot;</span><span class=p>,</span>
</span><span id=__span-64-5><a id=__codelineno-64-5 name=__codelineno-64-5 href=#__codelineno-64-5></a><span class=w>      </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;Prompt: guides the model to solve a math problem.&quot;</span><span class=p>,</span>
</span><span id=__span-64-6><a id=__codelineno-64-6 name=__codelineno-64-6 href=#__codelineno-64-6></a><span class=w>      </span><span class=nt>&quot;arguments&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-64-7><a id=__codelineno-64-7 name=__codelineno-64-7 href=#__codelineno-64-7></a><span class=w>        </span><span class=p>{</span>
</span><span id=__span-64-8><a id=__codelineno-64-8 name=__codelineno-64-8 href=#__codelineno-64-8></a><span class=w>          </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;problem&quot;</span><span class=p>,</span>
</span><span id=__span-64-9><a id=__codelineno-64-9 name=__codelineno-64-9 href=#__codelineno-64-9></a><span class=w>          </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
</span><span id=__span-64-10><a id=__codelineno-64-10 name=__codelineno-64-10 href=#__codelineno-64-10></a><span class=w>          </span><span class=nt>&quot;required&quot;</span><span class=p>:</span><span class=w> </span><span class=kc>true</span>
</span><span id=__span-64-11><a id=__codelineno-64-11 name=__codelineno-64-11 href=#__codelineno-64-11></a><span class=w>        </span><span class=p>}</span>
</span><span id=__span-64-12><a id=__codelineno-64-12 name=__codelineno-64-12 href=#__codelineno-64-12></a><span class=w>      </span><span class=p>]</span>
</span><span id=__span-64-13><a id=__codelineno-64-13 name=__codelineno-64-13 href=#__codelineno-64-13></a><span class=w>    </span><span class=p>}</span>
</span><span id=__span-64-14><a id=__codelineno-64-14 name=__codelineno-64-14 href=#__codelineno-64-14></a><span class=w>  </span><span class=p>]</span>
</span><span id=__span-64-15><a id=__codelineno-64-15 name=__codelineno-64-15 href=#__codelineno-64-15></a><span class=p>}</span>
</span></code></pre></div> <p>The client (or user) can then invoke this prompt. For instance, a UI might show an option "Solve a math problem" which triggers prompts/execute (or a similar method) on the server with name: "solve_math" and problem: "2+2*5" as an argument. The server would return the prepared messages; the client would feed those to the LLM, and the LLM would continue the conversation from that context (producing a detailed solution). Essentially, the prompt primitive allows packaging expert instructions or workflows so the model can perform them on demand.</p> <p>We now have a server that demonstrates all three server-side primitives: - A Tool (add) - A Resource (greet://{name}) - A Prompt (solve_math)</p> <h3 id=5-running-the-server-and-client-interaction><a class=toclink href=../../2025/03/31/introduction/#5-running-the-server-and-client-interaction>5. Running the Server and Client Interaction</a></h3> <p>To test our MCP server, we need an MCP client. In real-world usage, the "client" might be a GUI application (Claude Desktop) or an agent loop. Here, we can simulate a simple client or use the CLI. For brevity, let's illustrate using the CLI approach to interact with our server:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-65-1><a id=__codelineno-65-1 name=__codelineno-65-1 href=#__codelineno-65-1></a><span class=c1># In one terminal, run the server (it will wait for client connection)</span>
</span><span id=__span-65-2><a id=__codelineno-65-2 name=__codelineno-65-2 href=#__codelineno-65-2></a>python<span class=w> </span>server.py
</span></code></pre></div> <p>Now, in another terminal, we can use the mcp CLI (installed with the SDK) to connect:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-66-1><a id=__codelineno-66-1 name=__codelineno-66-1 href=#__codelineno-66-1></a>mcp<span class=w> </span>dev<span class=w> </span>server.py
</span></code></pre></div> <p>This should connect to the running server (or launch it if not already) and drop us into an interactive MCP Inspector. We could then try: - Listing tools: this should show the add tool. - Calling a tool: e.g. call add with {"a":5,"b":7} and expect result 12. - Reading a resource: e.g. request resource URI greet://Alice and see the text. - Executing a prompt: e.g. run prompt solve_math with argument "12*3-4" and watch the assistant's step-by-step solution.</p> <p>If all goes well, the server will log or display calls as it handles them. The SDK likely prints logs for each request. You'd see that our Python functions are triggered accordingly.</p> <p>For a programmatic approach, one could also use the Python SDK's client classes to do the same. For example:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-67-1><a id=__codelineno-67-1 name=__codelineno-67-1 href=#__codelineno-67-1></a><span class=c1># Using Python to connect via stdio (hypothetical example)</span>
</span><span id=__span-67-2><a id=__codelineno-67-2 name=__codelineno-67-2 href=#__codelineno-67-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.client</span><span class=w> </span><span class=kn>import</span> <span class=n>StdioClientTransport</span><span class=p>,</span> <span class=n>Client</span>
</span><span id=__span-67-3><a id=__codelineno-67-3 name=__codelineno-67-3 href=#__codelineno-67-3></a>
</span><span id=__span-67-4><a id=__codelineno-67-4 name=__codelineno-67-4 href=#__codelineno-67-4></a><span class=c1># Start the server process and connect transport (pseudo-code)</span>
</span><span id=__span-67-5><a id=__codelineno-67-5 name=__codelineno-67-5 href=#__codelineno-67-5></a><span class=n>transport</span> <span class=o>=</span> <span class=n>StdioClientTransport</span><span class=p>(</span><span class=s2>&quot;./server.py&quot;</span><span class=p>)</span>  <span class=c1># launch our server script</span>
</span><span id=__span-67-6><a id=__codelineno-67-6 name=__codelineno-67-6 href=#__codelineno-67-6></a><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=p>(</span><span class=n>transport</span><span class=p>)</span>
</span><span id=__span-67-7><a id=__codelineno-67-7 name=__codelineno-67-7 href=#__codelineno-67-7></a>
</span><span id=__span-67-8><a id=__codelineno-67-8 name=__codelineno-67-8 href=#__codelineno-67-8></a><span class=c1># Initialize the MCP connection</span>
</span><span id=__span-67-9><a id=__codelineno-67-9 name=__codelineno-67-9 href=#__codelineno-67-9></a><span class=n>client</span><span class=o>.</span><span class=n>initialize</span><span class=p>()</span>  <span class=c1># sends initialize handshake</span>
</span><span id=__span-67-10><a id=__codelineno-67-10 name=__codelineno-67-10 href=#__codelineno-67-10></a>
</span><span id=__span-67-11><a id=__codelineno-67-11 name=__codelineno-67-11 href=#__codelineno-67-11></a><span class=c1># List and call the &#39;add&#39; tool</span>
</span><span id=__span-67-12><a id=__codelineno-67-12 name=__codelineno-67-12 href=#__codelineno-67-12></a><span class=n>tools</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&quot;tools/list&quot;</span><span class=p>,</span> <span class=p>{})</span>
</span><span id=__span-67-13><a id=__codelineno-67-13 name=__codelineno-67-13 href=#__codelineno-67-13></a><span class=nb>print</span><span class=p>(</span><span class=n>tools</span><span class=p>)</span>  <span class=c1># should include &#39;add&#39;</span>
</span><span id=__span-67-14><a id=__codelineno-67-14 name=__codelineno-67-14 href=#__codelineno-67-14></a><span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&quot;tools/call&quot;</span><span class=p>,</span> <span class=p>{</span><span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=s2>&quot;add&quot;</span><span class=p>,</span> <span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&quot;a&quot;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&quot;b&quot;</span><span class=p>:</span> <span class=mi>3</span><span class=p>}})</span>
</span><span id=__span-67-15><a id=__codelineno-67-15 name=__codelineno-67-15 href=#__codelineno-67-15></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>  <span class=c1># should output {&#39;result&#39;: 5}</span>
</span><span id=__span-67-16><a id=__codelineno-67-16 name=__codelineno-67-16 href=#__codelineno-67-16></a>
</span><span id=__span-67-17><a id=__codelineno-67-17 name=__codelineno-67-17 href=#__codelineno-67-17></a><span class=n>client</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></code></pre></div> <p>The actual API may differ, but the concept is that the client sends JSON-RPC requests like {"method": "tools/call", "params": {"tool": "add", "params": {"a":2,"b":3}}} and receives a response {"result": 5}. The SDK's Client class would wrap this in a nicer interface.</p> <p>Through this step-by-step exercise, we saw how to implement an MCP server in Python with minimal effort (just defining functions with decorators) and how a client might interact. In a real deployment, you might build much more complex servers – connecting to real databases, handling authentication, streaming large resources, etc. – but the pattern remains the same. Anthropic's SDK and spec provides a lot of guidance and examples for building robust servers (including lifecycle management, dependency injection, error handling, etc., as shown in their docs).</p> <h2 id=advanced-use-cases-and-workflows><a class=toclink href=../../2025/03/31/introduction/#advanced-use-cases-and-workflows>Advanced Use Cases and Workflows</a></h2> <p>With the fundamentals in place, let's discuss some advanced use cases of MCP and how it fits into larger AI systems:</p> <h3 id=handling-complex-state-and-workflows><a class=toclink href=../../2025/03/31/introduction/#handling-complex-state-and-workflows>Handling Complex State and Workflows</a></h3> <p>One powerful aspect of MCP's design is that a server can maintain and manage complex state over the course of a session. For example, consider an MCP server that interfaces with a user's email. Such a server might need to handle authentication, caching of email headers, incremental fetching of emails, etc.</p> <p>Using MCP, the server could manage an OAuth login flow (especially with the new OAuth 2.1 support in the protocol for secure client-server auth), store the user's token, and reuse it for subsequent requests – all internally. The AI client just calls high-level tools like list_emails or send_email, and the server handles the gritty details with its state.</p> <p>Another scenario is maintaining conversational or task state. Suppose you have an agent working on a coding task using multiple tools: a Git server, a documentation server, etc. The agent might ask the Git server to open a file (as a resource), edit it (via a tool), then ask for a diff. The Git MCP server could keep track of an open repo and the last opened file across those calls, rather than requiring the AI to specify everything each time.</p> <p>This is facilitated by MCP's session – since the connection is persistent, the server can hold onto objects in memory (like a database connection or a file handle) between calls. In our earlier Python example, the SDK even showed a concept of a lifespan context where you can initialize resources at server startup (e.g. connect to a DB) and clean up on shutdown. That means complex state (like a database pool or an in-memory cache) can be established once and reused, improving efficiency and making the server more capable.</p> <p>Chaining and Multi-step workflows: Using prompts and sampling, servers can also coordinate multi-step processes. For instance, a single MCP prompt might involve the server first fetching some data (resource), then formulating a series of messages (prompt template), possibly even calling an internal model (sampling) to pre-analyze something, and finally returning a polished prompt for the client's LLM to answer. This is a form of agent orchestration that can be hidden behind a single MCP call. Essentially, the server can implement a mini-agent or state machine, but expose it as a simple interface to the client. The separation of concerns here is neat: the server handles procedure and state, the client's LLM handles language generation.</p> <h3 id=integration-with-agent-frameworks><a class=toclink href=../../2025/03/31/introduction/#integration-with-agent-frameworks>Integration with Agent Frameworks</a></h3> <p>MCP is quickly being adopted by AI agent frameworks and libraries, which is a testament to its flexibility. One notable integration is with LangChain, a popular framework for chaining LLM calls and tools. The LangChain team and community have built adapters to use MCP servers as LangChain tools.</p> <p>For example, rather than writing a custom tool wrapper for every API in LangChain, you can now spin up an MCP server (or use an existing one) and then use a converter that turns all of that server's Tools into LangChain Tool objects. This means LangChain-powered agents can immediately gain access to the huge library of MCP connectors (there are MCP servers for many services).</p> <p>Conversely, tools defined within LangChain could potentially be exposed via an MCP server to other systems. Beyond LangChain, other agent frameworks and IDE assistants are on board. The "fast-agent" project, for instance, is an MCP client that supports all features including Sampling and can coordinate multi-modal inputs.</p> <p>Developer tools like Cursor editor and Continue (VS Code extension) have added MCP support so that code assistants can use external tools uniformly. Even Microsoft's Guidance / Autogen frameworks and OpenAI's new Agents SDK are aligning with these standards.</p> <p>The fact that OpenAI itself announced MCP support in its Agents SDK shows that even proprietary agent ecosystems see value in interoperability. What does this mean for a developer? If you're building an AI agent, you could use MCP as the backbone for tool usage. Instead of hardcoding how to call each API, you let the agent discover available MCP servers and use them. The agent can remain focused on planning and high-level reasoning, delegating actual tool execution to MCP servers.</p> <p>This leads to more modular agent design – you can mix and match capabilities by just running different servers. Want to add calendar access to your agent? Just run a Calendar MCP server and connect it; no need to retrain the model or rewrite logic, because the model will see the new "calendar tool" via MCP and can start using it (assuming it's been instructed appropriately). This plug-and-play nature fits nicely with the vision of agents that can expand their toolsets dynamically.</p> <h3 id=retrieval-augmented-generation-rag-with-mcp><a class=toclink href=../../2025/03/31/introduction/#retrieval-augmented-generation-rag-with-mcp>Retrieval-Augmented Generation (RAG) with MCP</a></h3> <p>Retrieval-Augmented Generation refers to the workflow where an LLM fetches relevant external information (from a document corpus or knowledge base) to augment its responses. MCP is an excellent vehicle for implementing RAG in a standardized way. Instead of custom retrieval code, one can create an MCP server that encapsulates the retrieval logic.</p> <p>For example, imagine you have a vector database of documents or an ElasticSearch index. You could build an MCP server KnowledgeBaseServer with a tool search(query: str) -&gt; list[str] (or which returns resource URIs). The AI model, when asked a question, can call this search tool via MCP to get relevant text snippets (as either direct text results or as resource references that it then reads). Those snippets then become part of the context for answering the question. Because MCP can handle streaming and large data via resources, even bigger documents can be fetched in chunks as needed.</p> <p>The advantage is consistency and reuse: any MCP-aware AI app could use that same server for retrieval. In fact, Anthropic's documentation includes a quickstart where Claude uses an MCP server to fetch weather info – which is essentially a form of retrieval (getting factual data) before answering. The pattern extends to any knowledge domain.</p> <p>Let's sketch an advanced RAG flow with MCP in a sequence diagram for clarity:</p> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI_Client as AI (MCP Client)
    participant KB_Server as KnowledgeBase MCP Server
    participant DataStore as Document DB / Index
    User-&gt;&gt;AI_Client: "What were the key points of Project X's design?"
    AI_Client-&gt;&gt;KB_Server: tools/call: search({"query": "Project X design key points"})
    KB_Server-&gt;&gt;DataStore: Perform semantic search for "Project X design key points"
    DataStore--&gt;&gt;KB_Server: Returns top relevant doc snippets
    Note over KB_Server: e.g. returns a Resource list or text results
    KB_Server--&gt;&gt;AI_Client: Search results (as resources or text)
    AI_Client-&gt;&gt;AI_Client: (LLM incorporates results into its prompt)
    AI_Client--&gt;&gt;User: "The key design points of Project X are ... (with details from docs)"</code></pre> <p>In this RAG scenario, the heavy lifting of retrieval is done by the KB_Server. The AI client just knows that when it needs info, it can call the search tool. This decoupling means you could swap out the implementation (maybe use a different database or algorithm) by switching the server, without changing the AI's approach. Moreover, because MCP is model-agnostic, even if you used a different LLM tomorrow, it could perform the same retrieval process by using the server.</p> <p>MCP also helps maintain conversation flow in RAG. Traditional retrieval might not keep state of what's already fetched, but an MCP server could track which documents have been shown to the model (to avoid repetition or to follow up with more details as needed). It could also expose each document as a Resource with a URI, so that the model can say "let me read resource doc://123" in parts. All of this can be done with a clear protocol, rather than custom ad-hoc methods.</p> <h4 id=rag-implementation-strategies-with-mcp><a class=toclink href=../../2025/03/31/introduction/#rag-implementation-strategies-with-mcp>RAG Implementation Strategies with MCP</a></h4> <p>There are several approaches to implementing RAG with MCP, each with its own advantages:</p> <h5 id=1-tool-based-retrieval><a class=toclink href=../../2025/03/31/introduction/#1-tool-based-retrieval>1. Tool-based Retrieval</a></h5> <p>The simplest approach is to expose a search tool that returns relevant content directly. The model asks a question, calls the search tool with appropriate keywords, and gets back content in the tool response:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-68-1><a id=__codelineno-68-1 name=__codelineno-68-1 href=#__codelineno-68-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-68-2><a id=__codelineno-68-2 name=__codelineno-68-2 href=#__codelineno-68-2></a><span class=k>def</span><span class=w> </span><span class=nf>search_knowledge_base</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>max_results</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>]:</span>
</span><span id=__span-68-3><a id=__codelineno-68-3 name=__codelineno-68-3 href=#__codelineno-68-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Search the knowledge base for documents matching the query.&quot;&quot;&quot;</span>
</span><span id=__span-68-4><a id=__codelineno-68-4 name=__codelineno-68-4 href=#__codelineno-68-4></a>    <span class=c1># Search logic using vector DB, keyword search, etc.</span>
</span><span id=__span-68-5><a id=__codelineno-68-5 name=__codelineno-68-5 href=#__codelineno-68-5></a>    <span class=c1># Each result has the document content and metadata</span>
</span><span id=__span-68-6><a id=__codelineno-68-6 name=__codelineno-68-6 href=#__codelineno-68-6></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-68-7><a id=__codelineno-68-7 name=__codelineno-68-7 href=#__codelineno-68-7></a>        <span class=p>{</span><span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=s2>&quot;...&quot;</span><span class=p>,</span> <span class=s2>&quot;source&quot;</span><span class=p>:</span> <span class=s2>&quot;doc1.pdf&quot;</span><span class=p>,</span> <span class=s2>&quot;relevance&quot;</span><span class=p>:</span> <span class=mf>0.92</span><span class=p>},</span>
</span><span id=__span-68-8><a id=__codelineno-68-8 name=__codelineno-68-8 href=#__codelineno-68-8></a>        <span class=p>{</span><span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=s2>&quot;...&quot;</span><span class=p>,</span> <span class=s2>&quot;source&quot;</span><span class=p>:</span> <span class=s2>&quot;doc2.pdf&quot;</span><span class=p>,</span> <span class=s2>&quot;relevance&quot;</span><span class=p>:</span> <span class=mf>0.87</span><span class=p>},</span>
</span><span id=__span-68-9><a id=__codelineno-68-9 name=__codelineno-68-9 href=#__codelineno-68-9></a>        <span class=c1># ...</span>
</span><span id=__span-68-10><a id=__codelineno-68-10 name=__codelineno-68-10 href=#__codelineno-68-10></a>    <span class=p>]</span>
</span></code></pre></div> <p>This approach is straightforward but may have limitations with token context windows if the search returns large amounts of text.</p> <h5 id=2-resource-based-retrieval><a class=toclink href=../../2025/03/31/introduction/#2-resource-based-retrieval>2. Resource-based Retrieval</a></h5> <p>A more flexible approach is to expose documents as resources, so the model can request them by URI:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-69-1><a id=__codelineno-69-1 name=__codelineno-69-1 href=#__codelineno-69-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-69-2><a id=__codelineno-69-2 name=__codelineno-69-2 href=#__codelineno-69-2></a><span class=k>def</span><span class=w> </span><span class=nf>search_documents</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>max_results</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
</span><span id=__span-69-3><a id=__codelineno-69-3 name=__codelineno-69-3 href=#__codelineno-69-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Search documents and return resource URIs of relevant documents.&quot;&quot;&quot;</span>
</span><span id=__span-69-4><a id=__codelineno-69-4 name=__codelineno-69-4 href=#__codelineno-69-4></a>    <span class=c1># Search logic using vector DB, etc.</span>
</span><span id=__span-69-5><a id=__codelineno-69-5 name=__codelineno-69-5 href=#__codelineno-69-5></a>    <span class=c1># Return URIs instead of content</span>
</span><span id=__span-69-6><a id=__codelineno-69-6 name=__codelineno-69-6 href=#__codelineno-69-6></a>    <span class=k>return</span> <span class=p>[</span><span class=s2>&quot;doc://123&quot;</span><span class=p>,</span> <span class=s2>&quot;doc://456&quot;</span><span class=p>,</span> <span class=s2>&quot;doc://789&quot;</span><span class=p>]</span>
</span><span id=__span-69-7><a id=__codelineno-69-7 name=__codelineno-69-7 href=#__codelineno-69-7></a>
</span><span id=__span-69-8><a id=__codelineno-69-8 name=__codelineno-69-8 href=#__codelineno-69-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;doc://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-69-9><a id=__codelineno-69-9 name=__codelineno-69-9 href=#__codelineno-69-9></a><span class=k>def</span><span class=w> </span><span class=nf>get_document</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-69-10><a id=__codelineno-69-10 name=__codelineno-69-10 href=#__codelineno-69-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get the content of a document by ID.&quot;&quot;&quot;</span>
</span><span id=__span-69-11><a id=__codelineno-69-11 name=__codelineno-69-11 href=#__codelineno-69-11></a>    <span class=c1># Retrieve document with the given ID</span>
</span><span id=__span-69-12><a id=__codelineno-69-12 name=__codelineno-69-12 href=#__codelineno-69-12></a>    <span class=k>return</span> <span class=s2>&quot;Document content goes here...&quot;</span>
</span></code></pre></div> <p>This approach lets the model decide which documents to actually fetch after seeing search results, and it can request document content in chunks if needed.</p> <h5 id=3-hybrid-chunking-and-streaming><a class=toclink href=../../2025/03/31/introduction/#3-hybrid-chunking-and-streaming>3. Hybrid: Chunking and Streaming</a></h5> <p>For very large documents, an MCP server can implement chunking and pagination:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-70-1><a id=__codelineno-70-1 name=__codelineno-70-1 href=#__codelineno-70-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;doc://</span><span class=si>{id}</span><span class=s2>/chunk/</span><span class=si>{chunk_number}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-70-2><a id=__codelineno-70-2 name=__codelineno-70-2 href=#__codelineno-70-2></a><span class=k>def</span><span class=w> </span><span class=nf>get_document_chunk</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>chunk_number</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-70-3><a id=__codelineno-70-3 name=__codelineno-70-3 href=#__codelineno-70-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get a specific chunk of a document.&quot;&quot;&quot;</span>
</span><span id=__span-70-4><a id=__codelineno-70-4 name=__codelineno-70-4 href=#__codelineno-70-4></a>    <span class=c1># Logic to fetch and return only that chunk</span>
</span><span id=__span-70-5><a id=__codelineno-70-5 name=__codelineno-70-5 href=#__codelineno-70-5></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Content of chunk </span><span class=si>{</span><span class=n>chunk_number</span><span class=si>}</span><span class=s2> from doc </span><span class=si>{</span><span class=nb>id</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-70-6><a id=__codelineno-70-6 name=__codelineno-70-6 href=#__codelineno-70-6></a>
</span><span id=__span-70-7><a id=__codelineno-70-7 name=__codelineno-70-7 href=#__codelineno-70-7></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-70-8><a id=__codelineno-70-8 name=__codelineno-70-8 href=#__codelineno-70-8></a><span class=k>def</span><span class=w> </span><span class=nf>get_document_metadata</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-70-9><a id=__codelineno-70-9 name=__codelineno-70-9 href=#__codelineno-70-9></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get metadata about a document, including total chunks.&quot;&quot;&quot;</span>
</span><span id=__span-70-10><a id=__codelineno-70-10 name=__codelineno-70-10 href=#__codelineno-70-10></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-70-11><a id=__codelineno-70-11 name=__codelineno-70-11 href=#__codelineno-70-11></a>        <span class=s2>&quot;title&quot;</span><span class=p>:</span> <span class=s2>&quot;Example Document&quot;</span><span class=p>,</span>
</span><span id=__span-70-12><a id=__codelineno-70-12 name=__codelineno-70-12 href=#__codelineno-70-12></a>        <span class=s2>&quot;total_chunks&quot;</span><span class=p>:</span> <span class=mi>15</span><span class=p>,</span>
</span><span id=__span-70-13><a id=__codelineno-70-13 name=__codelineno-70-13 href=#__codelineno-70-13></a>        <span class=s2>&quot;summary&quot;</span><span class=p>:</span> <span class=s2>&quot;A brief overview of the document...&quot;</span>
</span><span id=__span-70-14><a id=__codelineno-70-14 name=__codelineno-70-14 href=#__codelineno-70-14></a>    <span class=p>}</span>
</span></code></pre></div> <p>This allows the model to navigate large documents efficiently, requesting only the most relevant portions.</p> <h5 id=4-semantic-routing><a class=toclink href=../../2025/03/31/introduction/#4-semantic-routing>4. Semantic Routing</a></h5> <p>An advanced MCP server could even implement semantic routing of queries to the right knowledge source:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-71-1><a id=__codelineno-71-1 name=__codelineno-71-1 href=#__codelineno-71-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-71-2><a id=__codelineno-71-2 name=__codelineno-71-2 href=#__codelineno-71-2></a><span class=k>def</span><span class=w> </span><span class=nf>route_query</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-71-3><a id=__codelineno-71-3 name=__codelineno-71-3 href=#__codelineno-71-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Route a query to the appropriate knowledge source.&quot;&quot;&quot;</span>
</span><span id=__span-71-4><a id=__codelineno-71-4 name=__codelineno-71-4 href=#__codelineno-71-4></a>    <span class=c1># Analyze the query and determine which source to use</span>
</span><span id=__span-71-5><a id=__codelineno-71-5 name=__codelineno-71-5 href=#__codelineno-71-5></a>    <span class=k>if</span> <span class=s2>&quot;financial&quot;</span> <span class=ow>in</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>():</span>
</span><span id=__span-71-6><a id=__codelineno-71-6 name=__codelineno-71-6 href=#__codelineno-71-6></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-71-7><a id=__codelineno-71-7 name=__codelineno-71-7 href=#__codelineno-71-7></a>            <span class=s2>&quot;recommended_source&quot;</span><span class=p>:</span> <span class=s2>&quot;financial_db&quot;</span><span class=p>,</span>
</span><span id=__span-71-8><a id=__codelineno-71-8 name=__codelineno-71-8 href=#__codelineno-71-8></a>            <span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=n>query</span><span class=p>,</span>
</span><span id=__span-71-9><a id=__codelineno-71-9 name=__codelineno-71-9 href=#__codelineno-71-9></a>            <span class=s2>&quot;confidence&quot;</span><span class=p>:</span> <span class=mf>0.85</span>
</span><span id=__span-71-10><a id=__codelineno-71-10 name=__codelineno-71-10 href=#__codelineno-71-10></a>        <span class=p>}</span>
</span><span id=__span-71-11><a id=__codelineno-71-11 name=__codelineno-71-11 href=#__codelineno-71-11></a>    <span class=k>elif</span> <span class=s2>&quot;technical&quot;</span> <span class=ow>in</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>():</span>
</span><span id=__span-71-12><a id=__codelineno-71-12 name=__codelineno-71-12 href=#__codelineno-71-12></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-71-13><a id=__codelineno-71-13 name=__codelineno-71-13 href=#__codelineno-71-13></a>            <span class=s2>&quot;recommended_source&quot;</span><span class=p>:</span> <span class=s2>&quot;technical_docs&quot;</span><span class=p>,</span>
</span><span id=__span-71-14><a id=__codelineno-71-14 name=__codelineno-71-14 href=#__codelineno-71-14></a>            <span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=n>query</span><span class=p>,</span>
</span><span id=__span-71-15><a id=__codelineno-71-15 name=__codelineno-71-15 href=#__codelineno-71-15></a>            <span class=s2>&quot;confidence&quot;</span><span class=p>:</span> <span class=mf>0.92</span>
</span><span id=__span-71-16><a id=__codelineno-71-16 name=__codelineno-71-16 href=#__codelineno-71-16></a>        <span class=p>}</span>
</span><span id=__span-71-17><a id=__codelineno-71-17 name=__codelineno-71-17 href=#__codelineno-71-17></a>    <span class=c1># ... other routing logic</span>
</span></code></pre></div> <p>This helps the model choose the right information source before retrieval.</p> <p>Each of these patterns can be mixed and matched to create sophisticated RAG systems, all communicating through the standardized MCP interface.</p> <h3 id=beyond-text-multi-modal-and-other-extensions><a class=toclink href=../../2025/03/31/introduction/#beyond-text-multi-modal-and-other-extensions>Beyond Text – Multi-Modal and Other Extensions</a></h3> <p>While our focus has been on text-based LLMs, MCP is not limited to text. The protocol can carry binary data (resources can be images or audio clips, with appropriate encoding). The Sampling primitive even has a provision for image generation or interpretation by specifying the content type (text or image) in the request.</p> <p>This means one could have an MCP server for, say, an image database, where images are exposed as resources (with mimeType) and the model could request them, or even an OCR tool as a Tool that returns text from an image. The modular design of MCP allows layering new capabilities.</p> <p>For example, Microsoft's recent contribution of a Playwright MCP server (for web browsing automation) shows how MCP can enable completely new tool modes: that server exposes browser actions like clicking, typing, and navigation as MCP tools. An AI agent using that server can browse the web like a user, but through standardized calls (browser_navigate, browser_click, etc.).</p> <p>This is far more complex than a basic function call, yet MCP handles it smoothly by formalizing those actions as tools with JSON schemas. The server even streams back page content as resources for the AI to read. This kind of advanced use case – essentially turning the AI into an automated agent in a real environment – is made feasible by MCP's architecture. And because it's standardized, improvements like better metadata (tool annotations, as added in MCP's update) benefit all tools across the board.</p> <h4 id=multi-modal-server-examples><a class=toclink href=../../2025/03/31/introduction/#multi-modal-server-examples>Multi-Modal Server Examples</a></h4> <p>Here are some examples of what multi-modal MCP servers might look like:</p> <h5 id=1-image-processing-server><a class=toclink href=../../2025/03/31/introduction/#1-image-processing-server>1. Image Processing Server</a></h5> <div class="language-python highlight"><pre><span></span><code><span id=__span-72-1><a id=__codelineno-72-1 name=__codelineno-72-1 href=#__codelineno-72-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-72-2><a id=__codelineno-72-2 name=__codelineno-72-2 href=#__codelineno-72-2></a><span class=kn>import</span><span class=w> </span><span class=nn>base64</span>
</span><span id=__span-72-3><a id=__codelineno-72-3 name=__codelineno-72-3 href=#__codelineno-72-3></a><span class=kn>from</span><span class=w> </span><span class=nn>PIL</span><span class=w> </span><span class=kn>import</span> <span class=n>Image</span>
</span><span id=__span-72-4><a id=__codelineno-72-4 name=__codelineno-72-4 href=#__codelineno-72-4></a><span class=kn>import</span><span class=w> </span><span class=nn>io</span>
</span><span id=__span-72-5><a id=__codelineno-72-5 name=__codelineno-72-5 href=#__codelineno-72-5></a>
</span><span id=__span-72-6><a id=__codelineno-72-6 name=__codelineno-72-6 href=#__codelineno-72-6></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Image Processing Server&quot;</span><span class=p>)</span>
</span><span id=__span-72-7><a id=__codelineno-72-7 name=__codelineno-72-7 href=#__codelineno-72-7></a>
</span><span id=__span-72-8><a id=__codelineno-72-8 name=__codelineno-72-8 href=#__codelineno-72-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;image://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-72-9><a id=__codelineno-72-9 name=__codelineno-72-9 href=#__codelineno-72-9></a><span class=k>def</span><span class=w> </span><span class=nf>get_image</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bytes</span><span class=p>:</span>
</span><span id=__span-72-10><a id=__codelineno-72-10 name=__codelineno-72-10 href=#__codelineno-72-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return image data as bytes with appropriate MIME type.&quot;&quot;&quot;</span>
</span><span id=__span-72-11><a id=__codelineno-72-11 name=__codelineno-72-11 href=#__codelineno-72-11></a>    <span class=c1># Load image from storage</span>
</span><span id=__span-72-12><a id=__codelineno-72-12 name=__codelineno-72-12 href=#__codelineno-72-12></a>    <span class=n>image_bytes</span> <span class=o>=</span> <span class=n>load_image_from_storage</span><span class=p>(</span><span class=nb>id</span><span class=p>)</span>
</span><span id=__span-72-13><a id=__codelineno-72-13 name=__codelineno-72-13 href=#__codelineno-72-13></a>    <span class=k>return</span> <span class=n>image_bytes</span>  <span class=c1># MCP can handle binary data with proper MIME type</span>
</span><span id=__span-72-14><a id=__codelineno-72-14 name=__codelineno-72-14 href=#__codelineno-72-14></a>
</span><span id=__span-72-15><a id=__codelineno-72-15 name=__codelineno-72-15 href=#__codelineno-72-15></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-72-16><a id=__codelineno-72-16 name=__codelineno-72-16 href=#__codelineno-72-16></a><span class=k>def</span><span class=w> </span><span class=nf>analyze_image</span><span class=p>(</span><span class=n>image_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-72-17><a id=__codelineno-72-17 name=__codelineno-72-17 href=#__codelineno-72-17></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Analyze an image and return metadata and detected objects.&quot;&quot;&quot;</span>
</span><span id=__span-72-18><a id=__codelineno-72-18 name=__codelineno-72-18 href=#__codelineno-72-18></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-72-19><a id=__codelineno-72-19 name=__codelineno-72-19 href=#__codelineno-72-19></a>    <span class=n>image_id</span> <span class=o>=</span> <span class=n>image_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;image://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-72-20><a id=__codelineno-72-20 name=__codelineno-72-20 href=#__codelineno-72-20></a>
</span><span id=__span-72-21><a id=__codelineno-72-21 name=__codelineno-72-21 href=#__codelineno-72-21></a>    <span class=c1># Get image data</span>
</span><span id=__span-72-22><a id=__codelineno-72-22 name=__codelineno-72-22 href=#__codelineno-72-22></a>    <span class=n>image_data</span> <span class=o>=</span> <span class=n>load_image_from_storage</span><span class=p>(</span><span class=n>image_id</span><span class=p>)</span>
</span><span id=__span-72-23><a id=__codelineno-72-23 name=__codelineno-72-23 href=#__codelineno-72-23></a>
</span><span id=__span-72-24><a id=__codelineno-72-24 name=__codelineno-72-24 href=#__codelineno-72-24></a>    <span class=c1># Run image analysis (just mock results for this example)</span>
</span><span id=__span-72-25><a id=__codelineno-72-25 name=__codelineno-72-25 href=#__codelineno-72-25></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-72-26><a id=__codelineno-72-26 name=__codelineno-72-26 href=#__codelineno-72-26></a>        <span class=s2>&quot;resolution&quot;</span><span class=p>:</span> <span class=s2>&quot;1920x1080&quot;</span><span class=p>,</span>
</span><span id=__span-72-27><a id=__codelineno-72-27 name=__codelineno-72-27 href=#__codelineno-72-27></a>        <span class=s2>&quot;detected_objects&quot;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&quot;person&quot;</span><span class=p>,</span> <span class=s2>&quot;car&quot;</span><span class=p>,</span> <span class=s2>&quot;tree&quot;</span><span class=p>],</span>
</span><span id=__span-72-28><a id=__codelineno-72-28 name=__codelineno-72-28 href=#__codelineno-72-28></a>        <span class=s2>&quot;dominant_colors&quot;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&quot;blue&quot;</span><span class=p>,</span> <span class=s2>&quot;green&quot;</span><span class=p>],</span>
</span><span id=__span-72-29><a id=__codelineno-72-29 name=__codelineno-72-29 href=#__codelineno-72-29></a>        <span class=s2>&quot;estimated_style&quot;</span><span class=p>:</span> <span class=s2>&quot;photograph&quot;</span>
</span><span id=__span-72-30><a id=__codelineno-72-30 name=__codelineno-72-30 href=#__codelineno-72-30></a>    <span class=p>}</span>
</span><span id=__span-72-31><a id=__codelineno-72-31 name=__codelineno-72-31 href=#__codelineno-72-31></a>
</span><span id=__span-72-32><a id=__codelineno-72-32 name=__codelineno-72-32 href=#__codelineno-72-32></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-72-33><a id=__codelineno-72-33 name=__codelineno-72-33 href=#__codelineno-72-33></a><span class=k>def</span><span class=w> </span><span class=nf>ocr_image</span><span class=p>(</span><span class=n>image_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-72-34><a id=__codelineno-72-34 name=__codelineno-72-34 href=#__codelineno-72-34></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Extract text from an image using OCR.&quot;&quot;&quot;</span>
</span><span id=__span-72-35><a id=__codelineno-72-35 name=__codelineno-72-35 href=#__codelineno-72-35></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-72-36><a id=__codelineno-72-36 name=__codelineno-72-36 href=#__codelineno-72-36></a>    <span class=n>image_id</span> <span class=o>=</span> <span class=n>image_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;image://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-72-37><a id=__codelineno-72-37 name=__codelineno-72-37 href=#__codelineno-72-37></a>
</span><span id=__span-72-38><a id=__codelineno-72-38 name=__codelineno-72-38 href=#__codelineno-72-38></a>    <span class=c1># Perform OCR on the image</span>
</span><span id=__span-72-39><a id=__codelineno-72-39 name=__codelineno-72-39 href=#__codelineno-72-39></a>    <span class=c1># (in a real implementation, would use an OCR library)</span>
</span><span id=__span-72-40><a id=__codelineno-72-40 name=__codelineno-72-40 href=#__codelineno-72-40></a>    <span class=k>return</span> <span class=s2>&quot;Text extracted from the image would appear here.&quot;</span>
</span></code></pre></div> <h5 id=2-audio-processing-server><a class=toclink href=../../2025/03/31/introduction/#2-audio-processing-server>2. Audio Processing Server</a></h5> <div class="language-python highlight"><pre><span></span><code><span id=__span-73-1><a id=__codelineno-73-1 name=__codelineno-73-1 href=#__codelineno-73-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;audio://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-73-2><a id=__codelineno-73-2 name=__codelineno-73-2 href=#__codelineno-73-2></a><span class=k>def</span><span class=w> </span><span class=nf>get_audio</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bytes</span><span class=p>:</span>
</span><span id=__span-73-3><a id=__codelineno-73-3 name=__codelineno-73-3 href=#__codelineno-73-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return audio data as bytes with appropriate MIME type.&quot;&quot;&quot;</span>
</span><span id=__span-73-4><a id=__codelineno-73-4 name=__codelineno-73-4 href=#__codelineno-73-4></a>    <span class=c1># Load audio from storage</span>
</span><span id=__span-73-5><a id=__codelineno-73-5 name=__codelineno-73-5 href=#__codelineno-73-5></a>    <span class=n>audio_bytes</span> <span class=o>=</span> <span class=n>load_audio_from_storage</span><span class=p>(</span><span class=nb>id</span><span class=p>)</span>
</span><span id=__span-73-6><a id=__codelineno-73-6 name=__codelineno-73-6 href=#__codelineno-73-6></a>    <span class=k>return</span> <span class=n>audio_bytes</span>
</span><span id=__span-73-7><a id=__codelineno-73-7 name=__codelineno-73-7 href=#__codelineno-73-7></a>
</span><span id=__span-73-8><a id=__codelineno-73-8 name=__codelineno-73-8 href=#__codelineno-73-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-73-9><a id=__codelineno-73-9 name=__codelineno-73-9 href=#__codelineno-73-9></a><span class=k>def</span><span class=w> </span><span class=nf>transcribe_audio</span><span class=p>(</span><span class=n>audio_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-73-10><a id=__codelineno-73-10 name=__codelineno-73-10 href=#__codelineno-73-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Transcribe speech in audio to text.&quot;&quot;&quot;</span>
</span><span id=__span-73-11><a id=__codelineno-73-11 name=__codelineno-73-11 href=#__codelineno-73-11></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-73-12><a id=__codelineno-73-12 name=__codelineno-73-12 href=#__codelineno-73-12></a>    <span class=n>audio_id</span> <span class=o>=</span> <span class=n>audio_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;audio://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-73-13><a id=__codelineno-73-13 name=__codelineno-73-13 href=#__codelineno-73-13></a>
</span><span id=__span-73-14><a id=__codelineno-73-14 name=__codelineno-73-14 href=#__codelineno-73-14></a>    <span class=c1># Get audio data and transcribe</span>
</span><span id=__span-73-15><a id=__codelineno-73-15 name=__codelineno-73-15 href=#__codelineno-73-15></a>    <span class=c1># (in a real implementation, would use a speech-to-text service)</span>
</span><span id=__span-73-16><a id=__codelineno-73-16 name=__codelineno-73-16 href=#__codelineno-73-16></a>    <span class=k>return</span> <span class=s2>&quot;Transcription of the audio would appear here.&quot;</span>
</span><span id=__span-73-17><a id=__codelineno-73-17 name=__codelineno-73-17 href=#__codelineno-73-17></a>
</span><span id=__span-73-18><a id=__codelineno-73-18 name=__codelineno-73-18 href=#__codelineno-73-18></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-73-19><a id=__codelineno-73-19 name=__codelineno-73-19 href=#__codelineno-73-19></a><span class=k>def</span><span class=w> </span><span class=nf>analyze_audio</span><span class=p>(</span><span class=n>audio_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-73-20><a id=__codelineno-73-20 name=__codelineno-73-20 href=#__codelineno-73-20></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Analyze audio properties and content.&quot;&quot;&quot;</span>
</span><span id=__span-73-21><a id=__codelineno-73-21 name=__codelineno-73-21 href=#__codelineno-73-21></a>    <span class=c1># Implementation details...</span>
</span><span id=__span-73-22><a id=__codelineno-73-22 name=__codelineno-73-22 href=#__codelineno-73-22></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-73-23><a id=__codelineno-73-23 name=__codelineno-73-23 href=#__codelineno-73-23></a>        <span class=s2>&quot;duration_seconds&quot;</span><span class=p>:</span> <span class=mi>120</span><span class=p>,</span>
</span><span id=__span-73-24><a id=__codelineno-73-24 name=__codelineno-73-24 href=#__codelineno-73-24></a>        <span class=s2>&quot;language_detected&quot;</span><span class=p>:</span> <span class=s2>&quot;English&quot;</span><span class=p>,</span>
</span><span id=__span-73-25><a id=__codelineno-73-25 name=__codelineno-73-25 href=#__codelineno-73-25></a>        <span class=s2>&quot;speakers_detected&quot;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span><span id=__span-73-26><a id=__codelineno-73-26 name=__codelineno-73-26 href=#__codelineno-73-26></a>        <span class=s2>&quot;music_detected&quot;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-73-27><a id=__codelineno-73-27 name=__codelineno-73-27 href=#__codelineno-73-27></a>        <span class=s2>&quot;audio_quality&quot;</span><span class=p>:</span> <span class=s2>&quot;High&quot;</span>
</span><span id=__span-73-28><a id=__codelineno-73-28 name=__codelineno-73-28 href=#__codelineno-73-28></a>    <span class=p>}</span>
</span></code></pre></div> <p>These examples show how MCP can handle different types of media while maintaining the same protocol structure. The AI can request media as resources and then analyze them using tools, all through the standard MCP interface.</p> <h3 id=workflow-orchestration-and-human-oversight><a class=toclink href=../../2025/03/31/introduction/#workflow-orchestration-and-human-oversight>Workflow Orchestration and Human Oversight</a></h3> <p>Finally, it's worth noting how MCP can fit into human-in-the-loop workflows. Because MCP clients (hosts) can intercept every tool invocation or resource request, a developer can implement policies like "ask user for permission before executing destructive tools" or log all data access for auditing. This is critical in enterprise settings.</p> <p>For instance, if an AI agent tries to call a "delete_record" tool on a database server, the MCP client could pop up a confirmation to the user. Or if a sampling request is made by a server (i.e., the server wants the AI to do something), the client can require a user click before letting it proceed. These controls ensure that even as we give AI agents more power via protocols like MCP, we maintain safety and governance.</p> <p>Moreover, MCP's open nature encourages community-driven best practices. Early adopters like Block (Square) highlighted that open protocols enable transparency and collaboration on making AI more helpful and less mechanical. We're likely to see shared schemas for common tasks, security frameworks, and audit tools develop around MCP as it matures.</p> <h4 id=implementing-human-in-the-loop-controls><a class=toclink href=../../2025/03/31/introduction/#implementing-human-in-the-loop-controls>Implementing Human-in-the-Loop Controls</a></h4> <p>Let's look at how a client might implement human approval for sensitive operations:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-74-1><a id=__codelineno-74-1 name=__codelineno-74-1 href=#__codelineno-74-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.client</span><span class=w> </span><span class=kn>import</span> <span class=n>MCPClient</span>
</span><span id=__span-74-2><a id=__codelineno-74-2 name=__codelineno-74-2 href=#__codelineno-74-2></a>
</span><span id=__span-74-3><a id=__codelineno-74-3 name=__codelineno-74-3 href=#__codelineno-74-3></a><span class=k>class</span><span class=w> </span><span class=nc>HumanOversightClient</span><span class=p>(</span><span class=n>MCPClient</span><span class=p>):</span>
</span><span id=__span-74-4><a id=__codelineno-74-4 name=__codelineno-74-4 href=#__codelineno-74-4></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>transport</span><span class=p>,</span> <span class=n>ui_interface</span><span class=p>):</span>
</span><span id=__span-74-5><a id=__codelineno-74-5 name=__codelineno-74-5 href=#__codelineno-74-5></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>transport</span><span class=p>)</span>
</span><span id=__span-74-6><a id=__codelineno-74-6 name=__codelineno-74-6 href=#__codelineno-74-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>ui</span> <span class=o>=</span> <span class=n>ui_interface</span>  <span class=c1># Interface to show prompts to the user</span>
</span><span id=__span-74-7><a id=__codelineno-74-7 name=__codelineno-74-7 href=#__codelineno-74-7></a>
</span><span id=__span-74-8><a id=__codelineno-74-8 name=__codelineno-74-8 href=#__codelineno-74-8></a>        <span class=c1># Define which tools require explicit approval</span>
</span><span id=__span-74-9><a id=__codelineno-74-9 name=__codelineno-74-9 href=#__codelineno-74-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>sensitive_tools</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-74-10><a id=__codelineno-74-10 name=__codelineno-74-10 href=#__codelineno-74-10></a>            <span class=s2>&quot;delete_record&quot;</span><span class=p>,</span>
</span><span id=__span-74-11><a id=__codelineno-74-11 name=__codelineno-74-11 href=#__codelineno-74-11></a>            <span class=s2>&quot;send_email&quot;</span><span class=p>,</span>
</span><span id=__span-74-12><a id=__codelineno-74-12 name=__codelineno-74-12 href=#__codelineno-74-12></a>            <span class=s2>&quot;execute_transaction&quot;</span><span class=p>,</span>
</span><span id=__span-74-13><a id=__codelineno-74-13 name=__codelineno-74-13 href=#__codelineno-74-13></a>            <span class=s2>&quot;modify_permissions&quot;</span>
</span><span id=__span-74-14><a id=__codelineno-74-14 name=__codelineno-74-14 href=#__codelineno-74-14></a>        <span class=p>]</span>
</span><span id=__span-74-15><a id=__codelineno-74-15 name=__codelineno-74-15 href=#__codelineno-74-15></a>
</span><span id=__span-74-16><a id=__codelineno-74-16 name=__codelineno-74-16 href=#__codelineno-74-16></a>        <span class=c1># Track usage and approvals</span>
</span><span id=__span-74-17><a id=__codelineno-74-17 name=__codelineno-74-17 href=#__codelineno-74-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-74-18><a id=__codelineno-74-18 name=__codelineno-74-18 href=#__codelineno-74-18></a>
</span><span id=__span-74-19><a id=__codelineno-74-19 name=__codelineno-74-19 href=#__codelineno-74-19></a>    <span class=k>async</span> <span class=k>def</span><span class=w> </span><span class=nf>call_tool</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>server_id</span><span class=p>,</span> <span class=n>tool_name</span><span class=p>,</span> <span class=n>params</span><span class=p>):</span>
</span><span id=__span-74-20><a id=__codelineno-74-20 name=__codelineno-74-20 href=#__codelineno-74-20></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Override to add human approval for sensitive tools.&quot;&quot;&quot;</span>
</span><span id=__span-74-21><a id=__codelineno-74-21 name=__codelineno-74-21 href=#__codelineno-74-21></a>        <span class=c1># Log the tool call attempt</span>
</span><span id=__span-74-22><a id=__codelineno-74-22 name=__codelineno-74-22 href=#__codelineno-74-22></a>        <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span><span id=__span-74-23><a id=__codelineno-74-23 name=__codelineno-74-23 href=#__codelineno-74-23></a>            <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>(),</span>
</span><span id=__span-74-24><a id=__codelineno-74-24 name=__codelineno-74-24 href=#__codelineno-74-24></a>            <span class=s2>&quot;action&quot;</span><span class=p>:</span> <span class=s2>&quot;tool_call_attempt&quot;</span><span class=p>,</span>
</span><span id=__span-74-25><a id=__codelineno-74-25 name=__codelineno-74-25 href=#__codelineno-74-25></a>            <span class=s2>&quot;server&quot;</span><span class=p>:</span> <span class=n>server_id</span><span class=p>,</span>
</span><span id=__span-74-26><a id=__codelineno-74-26 name=__codelineno-74-26 href=#__codelineno-74-26></a>            <span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=n>tool_name</span><span class=p>,</span>
</span><span id=__span-74-27><a id=__codelineno-74-27 name=__codelineno-74-27 href=#__codelineno-74-27></a>            <span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=n>params</span>
</span><span id=__span-74-28><a id=__codelineno-74-28 name=__codelineno-74-28 href=#__codelineno-74-28></a>        <span class=p>})</span>
</span><span id=__span-74-29><a id=__codelineno-74-29 name=__codelineno-74-29 href=#__codelineno-74-29></a>
</span><span id=__span-74-30><a id=__codelineno-74-30 name=__codelineno-74-30 href=#__codelineno-74-30></a>        <span class=c1># Check if this tool requires approval</span>
</span><span id=__span-74-31><a id=__codelineno-74-31 name=__codelineno-74-31 href=#__codelineno-74-31></a>        <span class=k>if</span> <span class=n>tool_name</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>sensitive_tools</span><span class=p>:</span>
</span><span id=__span-74-32><a id=__codelineno-74-32 name=__codelineno-74-32 href=#__codelineno-74-32></a>            <span class=c1># Format the request for human review</span>
</span><span id=__span-74-33><a id=__codelineno-74-33 name=__codelineno-74-33 href=#__codelineno-74-33></a>            <span class=n>approval_text</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;The AI wants to use tool &#39;</span><span class=si>{</span><span class=n>tool_name</span><span class=si>}</span><span class=s2>&#39; with these parameters:</span><span class=se>\n</span><span class=s2>&quot;</span>
</span><span id=__span-74-34><a id=__codelineno-74-34 name=__codelineno-74-34 href=#__codelineno-74-34></a>            <span class=n>approval_text</span> <span class=o>+=</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>indent</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-74-35><a id=__codelineno-74-35 name=__codelineno-74-35 href=#__codelineno-74-35></a>
</span><span id=__span-74-36><a id=__codelineno-74-36 name=__codelineno-74-36 href=#__codelineno-74-36></a>            <span class=c1># Ask for user approval</span>
</span><span id=__span-74-37><a id=__codelineno-74-37 name=__codelineno-74-37 href=#__codelineno-74-37></a>            <span class=n>approved</span> <span class=o>=</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>ui</span><span class=o>.</span><span class=n>ask_for_approval</span><span class=p>(</span><span class=n>approval_text</span><span class=p>)</span>
</span><span id=__span-74-38><a id=__codelineno-74-38 name=__codelineno-74-38 href=#__codelineno-74-38></a>
</span><span id=__span-74-39><a id=__codelineno-74-39 name=__codelineno-74-39 href=#__codelineno-74-39></a>            <span class=c1># Log the decision</span>
</span><span id=__span-74-40><a id=__codelineno-74-40 name=__codelineno-74-40 href=#__codelineno-74-40></a>            <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span><span id=__span-74-41><a id=__codelineno-74-41 name=__codelineno-74-41 href=#__codelineno-74-41></a>                <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>(),</span>
</span><span id=__span-74-42><a id=__codelineno-74-42 name=__codelineno-74-42 href=#__codelineno-74-42></a>                <span class=s2>&quot;action&quot;</span><span class=p>:</span> <span class=s2>&quot;approval_decision&quot;</span><span class=p>,</span>
</span><span id=__span-74-43><a id=__codelineno-74-43 name=__codelineno-74-43 href=#__codelineno-74-43></a>                <span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=n>tool_name</span><span class=p>,</span>
</span><span id=__span-74-44><a id=__codelineno-74-44 name=__codelineno-74-44 href=#__codelineno-74-44></a>                <span class=s2>&quot;approved&quot;</span><span class=p>:</span> <span class=n>approved</span>
</span><span id=__span-74-45><a id=__codelineno-74-45 name=__codelineno-74-45 href=#__codelineno-74-45></a>            <span class=p>})</span>
</span><span id=__span-74-46><a id=__codelineno-74-46 name=__codelineno-74-46 href=#__codelineno-74-46></a>
</span><span id=__span-74-47><a id=__codelineno-74-47 name=__codelineno-74-47 href=#__codelineno-74-47></a>            <span class=k>if</span> <span class=ow>not</span> <span class=n>approved</span><span class=p>:</span>
</span><span id=__span-74-48><a id=__codelineno-74-48 name=__codelineno-74-48 href=#__codelineno-74-48></a>                <span class=c1># Return a standard rejection response</span>
</span><span id=__span-74-49><a id=__codelineno-74-49 name=__codelineno-74-49 href=#__codelineno-74-49></a>                <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-74-50><a id=__codelineno-74-50 name=__codelineno-74-50 href=#__codelineno-74-50></a>                    <span class=s2>&quot;error&quot;</span><span class=p>:</span> <span class=s2>&quot;User did not approve this operation&quot;</span>
</span><span id=__span-74-51><a id=__codelineno-74-51 name=__codelineno-74-51 href=#__codelineno-74-51></a>                <span class=p>}</span>
</span><span id=__span-74-52><a id=__codelineno-74-52 name=__codelineno-74-52 href=#__codelineno-74-52></a>
</span><span id=__span-74-53><a id=__codelineno-74-53 name=__codelineno-74-53 href=#__codelineno-74-53></a>        <span class=c1># If no approval needed or approval granted, proceed with the call</span>
</span><span id=__span-74-54><a id=__codelineno-74-54 name=__codelineno-74-54 href=#__codelineno-74-54></a>        <span class=k>return</span> <span class=k>await</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>call_tool</span><span class=p>(</span><span class=n>server_id</span><span class=p>,</span> <span class=n>tool_name</span><span class=p>,</span> <span class=n>params</span><span class=p>)</span>
</span></code></pre></div> <p>This example demonstrates how a client can wrap standard MCP methods with approval flows. Similar patterns can be applied to resource access and sampling requests.</p> <h4 id=enterprise-governance-patterns><a class=toclink href=../../2025/03/31/introduction/#enterprise-governance-patterns>Enterprise Governance Patterns</a></h4> <p>For enterprise deployments, MCP enables several governance patterns:</p> <ol> <li> <p><strong>Role-Based Access Control (RBAC)</strong>: MCP servers can implement authentication and authorization, mapping user roles to allowed tools and resources.</p> </li> <li> <p><strong>Audit Logging</strong>: All MCP interactions can be logged for compliance and review.</p> </li> <li> <p><strong>Action Policies</strong>: Organizations can define policies about which tools require approval and from whom.</p> </li> <li> <p><strong>Sandboxing</strong>: MCP servers can be deployed in controlled environments with limited access to backend systems.</p> </li> <li> <p><strong>Rate Limiting</strong>: Servers can implement throttling to prevent abuse or unintended resource consumption.</p> </li> </ol> <p>These patterns can be implemented consistently across different MCP servers, creating a standardized governance framework for AI tool use.</p> <h2 id=performance-considerations-and-best-practices><a class=toclink href=../../2025/03/31/introduction/#performance-considerations-and-best-practices>Performance Considerations and Best Practices</a></h2> <p>When implementing MCP in production systems, several performance considerations and best practices should be kept in mind:</p> <h3 id=latency-management><a class=toclink href=../../2025/03/31/introduction/#latency-management>Latency Management</a></h3> <p>MCP introduces additional communication steps between the AI and external systems. To minimize latency:</p> <ol> <li><strong>Co-locate servers and clients</strong> when possible to reduce network overhead</li> <li><strong>Implement caching</strong> at the server level for frequently accessed resources</li> <li><strong>Use streaming responses</strong> for large data transfers rather than waiting for complete results</li> <li><strong>Balance chunking strategies</strong> - too many small requests can be inefficient, while too few large requests can block the UI</li> </ol> <div class="language-python highlight"><pre><span></span><code><span id=__span-75-1><a id=__codelineno-75-1 name=__codelineno-75-1 href=#__codelineno-75-1></a><span class=c1># Example of an MCP server with caching</span>
</span><span id=__span-75-2><a id=__codelineno-75-2 name=__codelineno-75-2 href=#__codelineno-75-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-75-3><a id=__codelineno-75-3 name=__codelineno-75-3 href=#__codelineno-75-3></a><span class=kn>import</span><span class=w> </span><span class=nn>functools</span>
</span><span id=__span-75-4><a id=__codelineno-75-4 name=__codelineno-75-4 href=#__codelineno-75-4></a>
</span><span id=__span-75-5><a id=__codelineno-75-5 name=__codelineno-75-5 href=#__codelineno-75-5></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Cached Server&quot;</span><span class=p>)</span>
</span><span id=__span-75-6><a id=__codelineno-75-6 name=__codelineno-75-6 href=#__codelineno-75-6></a>
</span><span id=__span-75-7><a id=__codelineno-75-7 name=__codelineno-75-7 href=#__codelineno-75-7></a><span class=c1># Simple in-memory cache for demonstration</span>
</span><span id=__span-75-8><a id=__codelineno-75-8 name=__codelineno-75-8 href=#__codelineno-75-8></a><span class=n>cache</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-75-9><a id=__codelineno-75-9 name=__codelineno-75-9 href=#__codelineno-75-9></a>
</span><span id=__span-75-10><a id=__codelineno-75-10 name=__codelineno-75-10 href=#__codelineno-75-10></a><span class=k>def</span><span class=w> </span><span class=nf>with_cache</span><span class=p>(</span><span class=n>ttl_seconds</span><span class=o>=</span><span class=mi>300</span><span class=p>):</span>
</span><span id=__span-75-11><a id=__codelineno-75-11 name=__codelineno-75-11 href=#__codelineno-75-11></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Decorator to cache function results.&quot;&quot;&quot;</span>
</span><span id=__span-75-12><a id=__codelineno-75-12 name=__codelineno-75-12 href=#__codelineno-75-12></a>    <span class=k>def</span><span class=w> </span><span class=nf>decorator</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span><span id=__span-75-13><a id=__codelineno-75-13 name=__codelineno-75-13 href=#__codelineno-75-13></a>        <span class=nd>@functools</span><span class=o>.</span><span class=n>wraps</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span><span id=__span-75-14><a id=__codelineno-75-14 name=__codelineno-75-14 href=#__codelineno-75-14></a>        <span class=k>def</span><span class=w> </span><span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span><span id=__span-75-15><a id=__codelineno-75-15 name=__codelineno-75-15 href=#__codelineno-75-15></a>            <span class=c1># Create a cache key from function name and arguments</span>
</span><span id=__span-75-16><a id=__codelineno-75-16 name=__codelineno-75-16 href=#__codelineno-75-16></a>            <span class=n>key</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>func</span><span class=o>.</span><span class=vm>__name__</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>args</span><span class=p>)</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>kwargs</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-75-17><a id=__codelineno-75-17 name=__codelineno-75-17 href=#__codelineno-75-17></a>
</span><span id=__span-75-18><a id=__codelineno-75-18 name=__codelineno-75-18 href=#__codelineno-75-18></a>            <span class=c1># Check if result is in cache and not expired</span>
</span><span id=__span-75-19><a id=__codelineno-75-19 name=__codelineno-75-19 href=#__codelineno-75-19></a>            <span class=k>if</span> <span class=n>key</span> <span class=ow>in</span> <span class=n>cache</span> <span class=ow>and</span> <span class=p>(</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>][</span><span class=s1>&#39;time&#39;</span><span class=p>])</span> <span class=o>&lt;</span> <span class=n>ttl_seconds</span><span class=p>:</span>
</span><span id=__span-75-20><a id=__codelineno-75-20 name=__codelineno-75-20 href=#__codelineno-75-20></a>                <span class=k>return</span> <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>][</span><span class=s1>&#39;result&#39;</span><span class=p>]</span>
</span><span id=__span-75-21><a id=__codelineno-75-21 name=__codelineno-75-21 href=#__codelineno-75-21></a>
</span><span id=__span-75-22><a id=__codelineno-75-22 name=__codelineno-75-22 href=#__codelineno-75-22></a>            <span class=c1># Call the function and cache the result</span>
</span><span id=__span-75-23><a id=__codelineno-75-23 name=__codelineno-75-23 href=#__codelineno-75-23></a>            <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span><span id=__span-75-24><a id=__codelineno-75-24 name=__codelineno-75-24 href=#__codelineno-75-24></a>            <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-75-25><a id=__codelineno-75-25 name=__codelineno-75-25 href=#__codelineno-75-25></a>                <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span><span id=__span-75-26><a id=__codelineno-75-26 name=__codelineno-75-26 href=#__codelineno-75-26></a>                <span class=s1>&#39;time&#39;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span><span id=__span-75-27><a id=__codelineno-75-27 name=__codelineno-75-27 href=#__codelineno-75-27></a>            <span class=p>}</span>
</span><span id=__span-75-28><a id=__codelineno-75-28 name=__codelineno-75-28 href=#__codelineno-75-28></a>            <span class=k>return</span> <span class=n>result</span>
</span><span id=__span-75-29><a id=__codelineno-75-29 name=__codelineno-75-29 href=#__codelineno-75-29></a>        <span class=k>return</span> <span class=n>wrapper</span>
</span><span id=__span-75-30><a id=__codelineno-75-30 name=__codelineno-75-30 href=#__codelineno-75-30></a>    <span class=k>return</span> <span class=n>decorator</span>
</span><span id=__span-75-31><a id=__codelineno-75-31 name=__codelineno-75-31 href=#__codelineno-75-31></a>
</span><span id=__span-75-32><a id=__codelineno-75-32 name=__codelineno-75-32 href=#__codelineno-75-32></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-75-33><a id=__codelineno-75-33 name=__codelineno-75-33 href=#__codelineno-75-33></a><span class=nd>@with_cache</span><span class=p>(</span><span class=n>ttl_seconds</span><span class=o>=</span><span class=mi>60</span><span class=p>)</span>
</span><span id=__span-75-34><a id=__codelineno-75-34 name=__codelineno-75-34 href=#__codelineno-75-34></a><span class=k>def</span><span class=w> </span><span class=nf>expensive_operation</span><span class=p>(</span><span class=n>param1</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-75-35><a id=__codelineno-75-35 name=__codelineno-75-35 href=#__codelineno-75-35></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;A computationally expensive operation that benefits from caching.&quot;&quot;&quot;</span>
</span><span id=__span-75-36><a id=__codelineno-75-36 name=__codelineno-75-36 href=#__codelineno-75-36></a>    <span class=c1># Simulate expensive operation</span>
</span><span id=__span-75-37><a id=__codelineno-75-37 name=__codelineno-75-37 href=#__codelineno-75-37></a>    <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># In real code, this would be actual computation</span>
</span><span id=__span-75-38><a id=__codelineno-75-38 name=__codelineno-75-38 href=#__codelineno-75-38></a>    <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;result&quot;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&quot;Processed </span><span class=si>{</span><span class=n>param1</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>,</span> <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()}</span>
</span></code></pre></div> <h3 id=security-best-practices><a class=toclink href=../../2025/03/31/introduction/#security-best-practices>Security Best Practices</a></h3> <ol> <li><strong>Input validation</strong>: Always validate inputs on the server side to prevent injection attacks</li> <li><strong>Least privilege</strong>: MCP servers should operate with minimal permissions needed</li> <li><strong>Token management</strong>: Securely handle authentication tokens and implement proper renewal</li> <li><strong>Transport security</strong>: Use TLS for HTTP-based transports</li> <li><strong>Rate limiting</strong>: Implement rate limits to prevent abuse</li> </ol> <h3 id=deployment-patterns><a class=toclink href=../../2025/03/31/introduction/#deployment-patterns>Deployment Patterns</a></h3> <ol> <li><strong>Sidecar deployment</strong>: Run MCP servers as sidecars alongside existing services</li> <li><strong>Microservice architecture</strong>: Deploy each MCP server as a separate microservice</li> <li><strong>Function-as-a-Service</strong>: Deploy lightweight MCP servers as serverless functions</li> <li><strong>Proxy pattern</strong>: Use a central MCP proxy to manage multiple backend services</li> </ol> <h3 id=development-workflow><a class=toclink href=../../2025/03/31/introduction/#development-workflow>Development Workflow</a></h3> <ol> <li><strong>Start with the SDK</strong>: Use Anthropic's MCP SDK for your language to minimize boilerplate</li> <li><strong>Test with the Inspector</strong>: Use the MCP Inspector tool to manually test your server</li> <li><strong>Write integration tests</strong>: Test your MCP server with automated client requests</li> <li><strong>Document capabilities</strong>: Create clear documentation of your server's tools and resources</li> <li><strong>Semantic versioning</strong>: Follow semver when releasing updates to your MCP servers</li> </ol> <h2 id=conclusion><a class=toclink href=../../2025/03/31/introduction/#conclusion>Conclusion</a></h2> <p>The Model Context Protocol (MCP) is a significant step forward in AI integration. It provides a robust, extensible framework for connecting LLMs to the world around them – to the data and tools that ground their responses in reality. By standardizing how context is provided to models, MCP frees developers from re-inventing the wheel for each integration, and it empowers AI systems with a richer, live model of their environment.</p> <p>We've seen how MCP compares to earlier approaches (offering broader scope, persistent state, and interoperability), learned about its core components (resources, tools, prompts, etc.), and built a simple Python MCP server step-by-step. We also explored advanced patterns like agent frameworks and retrieval augmentation, where MCP serves as the connective tissue for complex AI applications.</p> <p>As of 2025, MCP is still evolving (recent updates added streaming transports, OAuth security, and more), but it's rapidly gaining adoption and community support. Anthropic maintains an open-source repository of MCP SDKs (Python, TypeScript, Java, Kotlin, C#, Rust) and dozens of ready-to-use servers.</p> <p>The vision is that eventually, AI assistants will use MCP to maintain context seamlessly as they hop between tools and data sources, much like a person using a computer – making them far more useful and reliable. By learning and leveraging MCP now, developers can be at the forefront of building context-aware, tool-empowered AI agents that work across platforms. Whether you integrate it into a chatbot, an IDE, or an autonomous agent, MCP provides the foundation to connect AI with the real world in a standardized, powerful way.</p> <h3 id=resources-and-next-steps><a class=toclink href=../../2025/03/31/introduction/#resources-and-next-steps>Resources and Next Steps</a></h3> <p>If you'd like to start working with MCP:</p> <ol> <li><strong>Official Documentation</strong>: Visit the Model Context Protocol website for specifications and guides</li> <li><strong>GitHub Repositories</strong>: Check out Anthropic's MCP SDKs and reference implementations</li> <li><strong>Community Servers</strong>: Explore the growing ecosystem of community-contributed MCP servers</li> <li><strong>Join the Discussion</strong>: Participate in the MCP working group to shape the future of the protocol</li> </ol> <p>Sources: The official Anthropic announcement and documentation were used to define MCP and its architecture. InfoQ and VentureBeat articles provided context on MCP's goals and industry adoption. The MCP specification and guides were referenced for technical details on primitives and message flows. Code examples were adapted from Anthropic's open-source Python SDK and quickstart tutorials. These resources, alongside community commentary, offer a comprehensive view of MCP as the emerging standard for AI context integration.</p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://avatars.githubusercontent.com/u/134474669?v=4" alt="Muhammad Farooq"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2025-03-28 00:00:00+00:00">2025/03/28</time></li> <li class=md-meta__item> in <a href=../llms/ class=md-meta__link>LLMs</a>, <a href=./ class=md-meta__link>Agents</a></li> <li class=md-meta__item> 13 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=a-visual-guide-to-llm-agents><a href=../../2025/03/28/a-visual-guide-to-llm-agents/ class=toclink>A Visual Guide to LLM Agents</a></h2> <h3 id=table-of-contents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#table-of-contents>Table of Contents</a></h3> <ol> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#introduction-to-large-language-models>Introduction to Large Language Models</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#from-llms-to-agents>From LLMs to Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#core-components-of-llm-agents>Core Components of LLM Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#tools-and-augmentation>Tools and Augmentation</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-planning-and-reasoning>Agent Planning and Reasoning</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-memory-systems>Agent Memory Systems</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#advanced-agent-architectures>Advanced Agent Architectures</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#multi-agent-systems>Multi-Agent Systems</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#building-and-deploying-agents>Building and Deploying Agents</a></li> <li><a href=../../2025/03/28/a-visual-guide-to-llm-agents/#future-directions>Future Directions</a></li> </ol> <h3 id=introduction-to-large-language-models><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#introduction-to-large-language-models>Introduction to Large Language Models</a></h3> <p>Before diving into agents, we need to understand what Large Language Models (LLMs) are and how they function.</p> <p>LLMs are sophisticated neural networks trained on vast amounts of text data to understand and generate human-like text. These models have evolved from simple statistical approaches to complex architectures based primarily on the Transformer architecture introduced in 2017.</p> <h4 id=how-llms-work><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#how-llms-work>How LLMs Work</a></h4> <p>At their core, LLMs predict the next token (word or subword) in a sequence based on the context provided. The basic architecture consists of:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    A[Input Text] --&gt; B[Tokenization]
    B --&gt; C[Embedding Layer]
    C --&gt; D[Transformer Layers]
    D --&gt; E[Output Layer]
    E --&gt; F[Generated Text]

    style A fill:#f9f9f9,stroke:#333,stroke-width:1px
    style B fill:#e6f3ff,stroke:#333,stroke-width:1px
    style C fill:#e6f3ff,stroke:#333,stroke-width:1px
    style D fill:#cce5ff,stroke:#333,stroke-width:1px
    style E fill:#e6f3ff,stroke:#333,stroke-width:1px
    style F fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> </div> <ul> <li><strong>Tokenization</strong>: Converting input text into tokens</li> <li><strong>Embedding Layer</strong>: Transforming tokens into vector representations</li> <li><strong>Transformer Layers</strong>: Processing these vectors through attention mechanisms</li> <li><strong>Output Layer</strong>: Generating probability distributions for the next token</li> </ul> <p>Most modern LLMs use the transformer architecture, which employs self-attention mechanisms to weigh the importance of different words in context.</p> <h4 id=capabilities-and-limitations-of-traditional-llms><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#capabilities-and-limitations-of-traditional-llms>Capabilities and Limitations of Traditional LLMs</a></h4> <p><strong>Capabilities:</strong> - Text generation across various domains - Understanding context and nuance - Adapting to different writing styles - Performing various language tasks without task-specific training</p> <p><strong>Limitations:</strong> - No ability to access or verify external information beyond training data - No capability to take actions in the world - Limited understanding of temporal context (when events occurred) - No persistent memory between sessions - No ability to use tools or APIs - Risk of hallucinations (generating false information)</p> <p>These limitations highlight why moving from passive LLMs to active agents is necessary for more complex applications.</p> <h3 id=from-llms-to-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#from-llms-to-agents>From LLMs to Agents</a></h3> <p>The transition from passive language models to active agents is fundamental to understanding LLM agents.</p> <h4 id=what-defines-an-agent><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#what-defines-an-agent>What Defines an Agent?</a></h4> <p>As defined by Russell &amp; Norvig, <strong>"an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators."</strong></p> <p>This definition introduces two critical components missing in standard LLMs: 1. <strong>Perception</strong>: The ability to sense the environment 2. <strong>Action</strong>: The ability to affect the environment</p> <h4 id=the-agent-framework><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#the-agent-framework>The Agent Framework</a></h4> <p>An agent-based framework adapts this definition to work with LLMs:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart LR
    subgraph LLM["Traditional LLM"]
        A[Input Prompt] --&gt; B[Text Generation]
        B --&gt; C[Output Text]
    end

    subgraph Agent["LLM Agent"]
        D[Environment] --&gt; E[Perception]
        E --&gt; F[Reasoning/Planning]
        F --&gt; G[Tool Use]
        G --&gt; H[Actions]
        H --&gt; D
        I[Memory] --&gt; F
        F --&gt; I
    end

    LLM --&gt; Agent

    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px
    style Agent fill:#e6f3ff,stroke:#333,stroke-width:2px</code></pre> </div> <ul> <li><strong>Environment</strong>: The context in which the agent operates (could be a chat interface, document, or digital environment)</li> <li><strong>Perception</strong>: Input methods like prompts, document content, or API responses</li> <li><strong>Reasoning</strong>: Internal processing using the LLM to decide what to do</li> <li><strong>Action</strong>: Outputs that affect the environment (generating text, calling functions, using tools)</li> <li><strong>Memory</strong>: Retaining information across interactions</li> </ul> <p>This framework transforms a passive text prediction system into an entity that can intelligently interact with its surroundings.</p> <h3 id=core-components-of-llm-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#core-components-of-llm-agents>Core Components of LLM Agents</a></h3> <p>Let's explore the essential components that make up an LLM agent:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart LR
    %% Central node with dashed border
    LLM["Large Language\nModel"] 

    %% Main components with their subgraphs
    subgraph Tools[" "]
        T[Tool Use] --&gt; T1[Function Calling]
        T --&gt; T2[API Integration]
        T --&gt; T3[Database Access]
    end

    subgraph Reasoning[" "]
        R[Reasoning] --&gt; R1[Task Decomposition]
        R --&gt; R2[Chain-of-Thought]
        R --&gt; R3[Decision-Making]
    end

    subgraph Perception[" "]
        P[Perception] --&gt; P1[Text Inputs]
        P --&gt; P2[Document Understanding]
        P --&gt; P3[Multimodal Inputs]
    end

    subgraph Memory[" "]
        M[Memory] --&gt; M1[Short-Term]
        M --&gt; M2[Long-Term]
        M --&gt; M3[Episodic &amp; Semantic]
    end

    %% Connect LLM to main components
    LLM --&gt; T
    LLM --&gt; R
    LLM --&gt; P
    LLM --&gt; M

    %% Style nodes
    style LLM fill:#f5f5f5,stroke:#333,stroke-width:1px,stroke-dasharray:5 5

    %% Tool styles
    style T fill:#fd7e14,stroke:#333,stroke-width:2px
    style T1 fill:#ffe8cc,stroke:#333,stroke-width:1px
    style T2 fill:#ffe8cc,stroke:#333,stroke-width:1px
    style T3 fill:#ffe8cc,stroke:#333,stroke-width:1px

    %% Reasoning styles
    style R fill:#40c057,stroke:#333,stroke-width:2px
    style R1 fill:#d3f9d8,stroke:#333,stroke-width:1px
    style R2 fill:#d3f9d8,stroke:#333,stroke-width:1px
    style R3 fill:#d3f9d8,stroke:#333,stroke-width:1px

    %% Perception styles
    style P fill:#4dabf7,stroke:#333,stroke-width:2px
    style P1 fill:#d0ebff,stroke:#333,stroke-width:1px
    style P2 fill:#d0ebff,stroke:#333,stroke-width:1px
    style P3 fill:#d0ebff,stroke:#333,stroke-width:1px

    %% Memory styles
    style M fill:#ae3ec9,stroke:#333,stroke-width:2px
    style M1 fill:#f3d9fa,stroke:#333,stroke-width:1px
    style M2 fill:#f3d9fa,stroke:#333,stroke-width:1px
    style M3 fill:#f3d9fa,stroke:#333,stroke-width:1px

</code></pre> </div> <h4 id=environment-perception><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#environment-perception>Environment Perception</a></h4> <p>Agents need to understand their environment through various inputs:</p> <ul> <li><strong>Text Input</strong>: The most basic form of perception through prompts</li> <li><strong>Document Understanding</strong>: Processing and understanding documents</li> <li><strong>Structured Data</strong>: Working with databases, APIs, and structured information</li> <li><strong>Multimodal Input</strong>: Processing images, audio, or other data types (in advanced agents)</li> </ul> <h4 id=planning-and-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#planning-and-reasoning>Planning and Reasoning</a></h4> <p>An agent must plan its actions and reason about the best course of action:</p> <ul> <li><strong>Task Decomposition</strong>: Breaking complex tasks into manageable steps</li> <li><strong>Chain-of-Thought</strong>: Working through problems step-by-step</li> <li><strong>Decision-Making</strong>: Evaluating options and selecting the best course of action</li> <li><strong>Self-Reflection</strong>: Evaluating its own reasoning and outputs</li> </ul> <h4 id=tool-usage-and-integration><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#tool-usage-and-integration>Tool Usage and Integration</a></h4> <p>A defining characteristic of LLM agents is their ability to use tools:</p> <ul> <li><strong>Function Calling</strong>: Identifying when to call an external function</li> <li><strong>API Integration</strong>: Connecting to external services through APIs</li> <li><strong>Code Execution</strong>: Running code to perform calculations or manipulate data</li> <li><strong>Database Access</strong>: Retrieving or storing information in databases</li> </ul> <h4 id=memory-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#memory-systems>Memory Systems</a></h4> <p>Agents require memory to maintain context and learn from past interactions:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TB
    %% Short-term Memory Section
    subgraph ShortTerm["Short-Term Memory"]
        direction LR
        CH["Conversation History"] --- CD["Current Session Data"] --- AT["Active Task State"]
    end

    %% Long-term Memory Section
    subgraph LongTerm["Long-Term Memory"]
        direction LR
        VD["Vector Database"] --- KG["Knowledge Graph"] --- DD["Document Store"] --- UP["User Profiles"]
    end

    %% Memory Types Section
    subgraph MemTypes["Memory Types"]
        direction LR
        EM["Episodic Memory&lt;br/&gt;(Specific Interactions)"] --- SM["Semantic Memory&lt;br/&gt;(General Knowledge)"] --- PM["Procedural Memory&lt;br/&gt;(How to Perform Tasks)"]
    end

    %% Memory Retrieval Section
    subgraph Retrieval["Memory Retrieval"]
        direction LR
        SS["Semantic Search"] --- TF["Temporal Filtering"] --- SR["Relevance Ranking"] --- CR["Contextual Retrieval"]
    end

    %% Decision Making
    DM["Agent Decision Making"]

    %% Connections
    ShortTerm --&gt; MemTypes
    LongTerm --&gt; MemTypes
    MemTypes --&gt; Retrieval
    Retrieval --&gt; DM

    %% Styling
    style ShortTerm fill:#fff7e6,stroke:#333,stroke-width:1px
    style LongTerm fill:#e6ffe6,stroke:#333,stroke-width:1px
    style MemTypes fill:#e6f7ff,stroke:#333,stroke-width:1px
    style Retrieval fill:#ffe6e6,stroke:#333,stroke-width:1px
    style DM fill:#f0f0ff,stroke:#333,stroke-width:1px

    %% Node Styles
    classDef default fill:#fff,stroke:#333,stroke-width:1px</code></pre> </div> <ul> <li><strong>Short-Term Memory</strong>: Recent conversation history</li> <li><strong>Long-Term Memory</strong>: Persistent information stored across sessions</li> <li><strong>Episodic Memory</strong>: Specific sequences of interactions</li> <li><strong>Semantic Memory</strong>: General knowledge and facts</li> </ul> <p>These core components transform an LLM into an agent capable of complex, goal-oriented behavior.</p> <h3 id=tools-and-augmentation><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#tools-and-augmentation>Tools and Augmentation</a></h3> <p>Tools and augmentation techniques enhance the capabilities of LLM agents beyond their built-in knowledge.</p> <h4 id=types-of-tools><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#types-of-tools>Types of Tools</a></h4> <p>Modern LLM agents can leverage various tools:</p> <ul> <li><strong>Search Tools</strong>: Accessing up-to-date information from the internet</li> <li><strong>Calculators</strong>: Performing precise mathematical operations</li> <li><strong>Knowledge Bases</strong>: Accessing specific domain knowledge</li> <li><strong>Code Interpreters</strong>: Executing and debugging code</li> <li><strong>Database Interfaces</strong>: Querying and manipulating structured data</li> <li><strong>API Connectors</strong>: Interacting with external services and platforms</li> </ul> <h4 id=retrieval-augmented-generation-rag><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#retrieval-augmented-generation-rag>Retrieval Augmented Generation (RAG)</a></h4> <p>RAG is a powerful technique that combines retrieval of information with text generation:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    Q[Query/Question] --&gt; E[Embedding Model]
    E --&gt; VS[Vector Search]

    subgraph Indexing["Document Indexing (Pre-processing)"]
        D[Documents] --&gt; DC[Document Chunking]
        DC --&gt; DE[Document Embedding]
        DE --&gt; VDB[Vector Database]
    end

    VS --&gt; VDB
    VDB --&gt; RD[Retrieved Documents]

    Q --&gt; P[Prompt Construction]
    RD --&gt; P

    P --&gt; LLM[Large Language Model]
    LLM --&gt; A[Augmented Response]

    style Indexing fill:#e6f3ff,stroke:#333,stroke-width:1px
    style Q fill:#f9f9f9,stroke:#333,stroke-width:1px
    style P fill:#f9f9f9,stroke:#333,stroke-width:1px
    style LLM fill:#cce5ff,stroke:#333,stroke-width:2px
    style A fill:#f9f9f9,stroke:#333,stroke-width:1px</code></pre> </div> <ol> <li><strong>Indexing</strong>: Documents are processed, chunked, and stored in a vector database</li> <li><strong>Retrieval</strong>: When a query is received, relevant documents are retrieved</li> <li><strong>Augmentation</strong>: Retrieved content is added to the prompt</li> <li><strong>Generation</strong>: The LLM generates a response based on both the query and the retrieved information</li> </ol> <p>RAG enhances accuracy by grounding responses in specific knowledge sources, reducing hallucinations and improving factual accuracy.</p> <h4 id=function-and-api-integration><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#function-and-api-integration>Function and API Integration</a></h4> <p>Function calling allows agents to interact with the world:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant LLM
    participant Function

    User-&gt;&gt;LLM: Query (e.g., "What's the weather in New York?")

    LLM-&gt;&gt;LLM: Recognize need for external data

    LLM-&gt;&gt;Function: Call function&lt;br/&gt;(get_weather, {location: "New York"})
    Function-&gt;&gt;LLM: Return data (Temperature: 72°F, Condition: Sunny)

    LLM-&gt;&gt;User: Generate response with function data&lt;br/&gt;"The current weather in New York is sunny with a temperature of 72°F."

    note over LLM,Function: Modern LLMs can determine when to&lt;br/&gt;call functions and structure the&lt;br/&gt;appropriate parameters</code></pre> </div> <ol> <li><strong>Function Definition</strong>: Functions are defined with names, descriptions, and parameter specifications</li> <li><strong>Function Detection</strong>: The LLM detects when a function should be called based on the user's query</li> <li><strong>Parameter Generation</strong>: The LLM generates the appropriate parameters</li> <li><strong>Function Execution</strong>: The function is executed, and results are returned</li> <li><strong>Response Integration</strong>: The LLM incorporates the function results into its response</li> </ol> <p>This capability enables agents to perform actions like checking the weather, booking appointments, or processing payments.</p> <h3 id=agent-planning-and-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-planning-and-reasoning>Agent Planning and Reasoning</a></h3> <p>Effective planning and reasoning are crucial for complex tasks.</p> <h4 id=prompt-engineering-for-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#prompt-engineering-for-agents>Prompt Engineering for Agents</a></h4> <p>Agent prompts typically include:</p> <ul> <li><strong>System Instructions</strong>: Defining the agent's role and capabilities</li> <li><strong>Available Tools</strong>: Descriptions of tools the agent can use</li> <li><strong>Constraints</strong>: Limitations on the agent's actions</li> <li><strong>Output Format</strong>: How the agent should structure its responses</li> <li><strong>Examples</strong>: Demonstrations of expected behavior</li> </ul> <h4 id=chain-of-thought-cot-reasoning><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#chain-of-thought-cot-reasoning>Chain-of-Thought (CoT) Reasoning</a></h4> <p>CoT enables an agent to work through problems step-by-step:</p> <ol> <li><strong>Problem Analysis</strong>: Understanding the task and breaking it down</li> <li><strong>Intermediate Steps</strong>: Working through each step logically</li> <li><strong>Reflection</strong>: Checking the reasoning at each step</li> <li><strong>Solution</strong>: Arriving at the final answer based on the steps</li> </ol> <p>This approach significantly improves performance on complex reasoning tasks.</p> <h4 id=react-framework><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#react-framework>ReAct Framework</a></h4> <p>ReAct (Reasoning + Acting) interleaves thinking and action:</p> <div class=mermaid-wrapper> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant Agent
    participant Tools as External Tools

    User-&gt;&gt;Agent: Task or Query

    loop Until task completion
        Agent-&gt;&gt;Agent: Thought: Reasoning about next step
        Agent-&gt;&gt;Agent: Action: Decide which tool to use
        Agent-&gt;&gt;Tools: Call appropriate tool
        Tools-&gt;&gt;Agent: Observation: Return result
        Agent-&gt;&gt;Agent: Thought: Process observation
    end

    Agent-&gt;&gt;User: Final response

    note over Agent: ReAct interleaves reasoning (thoughts)&lt;br/&gt;with actions and observations in a cycle</code></pre> </div> <ol> <li><strong>Reasoning</strong>: The agent thinks about what it needs to do</li> <li><strong>Action</strong>: The agent takes action using available tools</li> <li><strong>Observation</strong>: The agent observes the results of its action</li> <li><strong>Continued Reasoning</strong>: The agent incorporates observations into its reasoning</li> </ol> <p>This cycle continues until the task is complete, enabling dynamic, adaptive problem-solving.</p> <h3 id=agent-memory-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-memory-systems>Agent Memory Systems</a></h3> <p>Memory systems enable agents to maintain context and learn from past interactions.</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    subgraph SM["Short-Term Memory"]
        SM1["Conversation History"]
        SM2["Current Session Data"]
        SM3["Active Task State"]
    end

    subgraph LM["Long-Term Memory"]
        LM1["Vector Database"]
        LM2["Knowledge Graph"]
        LM3["Document Store"]
        LM4["User Profiles"]
    end

    subgraph MT["Memory Types"]
        MT1["Episodic Memory&lt;br/&gt;(Specific Interactions)"]
        MT2["Semantic Memory&lt;br/&gt;(General Knowledge)"]
        MT3["Procedural Memory&lt;br/&gt;(How to Perform Tasks)"]
    end

    subgraph MR["Memory Retrieval"]
        MR1["Semantic Search"]
        MR2["Temporal Filtering"]
        MR3["Relevance Ranking"]
        MR4["Contextual Retrieval"]
    end

    SM --&gt; MT
    LM --&gt; MT
    MT --&gt; MR
    MR --&gt; A[Agent Decision Making]

    style SM fill:#ffffcc,stroke:#333,stroke-width:1px
    style LM fill:#ccffcc,stroke:#333,stroke-width:1px
    style MT fill:#ccffff,stroke:#333,stroke-width:1px
    style MR fill:#ffcccc,stroke:#333,stroke-width:1px</code></pre> </div> <h4 id=short-term-context><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#short-term-context>Short-Term Context</a></h4> <p>Short-term or working memory includes:</p> <ul> <li><strong>Conversation History</strong>: The recent exchanges between user and agent</li> <li><strong>Current Session Data</strong>: Information gathered during the current interaction</li> <li><strong>Active Task State</strong>: The current progress on the task being performed</li> </ul> <p>These elements are typically handled through context window management.</p> <h4 id=long-term-memory-storage><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#long-term-memory-storage>Long-Term Memory Storage</a></h4> <p>Long-term memory enables persistent information storage:</p> <ul> <li><strong>Vector Databases</strong>: Storing semantic representations of past conversations</li> <li><strong>Knowledge Graphs</strong>: Structured representations of entities and relationships</li> <li><strong>Document Stores</strong>: Persistent storage of important information</li> <li><strong>User Profiles</strong>: Preferences and patterns specific to individual users</li> </ul> <h4 id=episodic-vs-semantic-memory><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#episodic-vs-semantic-memory>Episodic vs. Semantic Memory</a></h4> <p>Agents can implement different types of memory:</p> <ul> <li><strong>Episodic Memory</strong>: Specific sequences of interactions (e.g., "Last time we discussed home renovation options")</li> <li><strong>Semantic Memory</strong>: General knowledge and facts (e.g., "The user prefers minimalist design")</li> </ul> <h4 id=memory-retrieval-strategies><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#memory-retrieval-strategies>Memory Retrieval Strategies</a></h4> <p>Effective retrieval is critical for using stored information:</p> <ul> <li><strong>Semantic Search</strong>: Finding relevant information based on meaning</li> <li><strong>Temporal Filtering</strong>: Retrieving information based on when it was stored</li> <li><strong>Relevance Ranking</strong>: Prioritizing the most important information</li> <li><strong>Contextual Retrieval</strong>: Finding information relevant to the current context</li> </ul> <p>A well-designed memory system allows agents to build on past interactions and provide more personalized experiences.</p> <h3 id=advanced-agent-architectures><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#advanced-agent-architectures>Advanced Agent Architectures</a></h3> <p>As agents become more sophisticated, their architectures evolve to handle more complex tasks.</p> <h4 id=task-decomposition><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#task-decomposition>Task Decomposition</a></h4> <p>Complex task handling requires sophisticated decomposition:</p> <ol> <li><strong>Goal Analysis</strong>: Understanding the overall objective</li> <li><strong>Subtask Identification</strong>: Breaking down the goal into manageable parts</li> <li><strong>Dependency Mapping</strong>: Determining the order of subtasks</li> <li><strong>Resource Allocation</strong>: Assigning appropriate tools to each subtask</li> </ol> <p>This approach enables agents to tackle problems too complex to solve all at once.</p> <h4 id=self-reflection-and-self-correction><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#self-reflection-and-self-correction>Self-Reflection and Self-Correction</a></h4> <p>Advanced agents can evaluate and improve their own outputs:</p> <ol> <li><strong>Output Generation</strong>: Producing an initial response</li> <li><strong>Self-Critique</strong>: Identifying potential issues or improvements</li> <li><strong>Refinement</strong>: Revising the response based on self-critique</li> <li><strong>Verification</strong>: Checking the improved response against requirements</li> </ol> <p>This recursive improvement process enhances accuracy and quality.</p> <h4 id=verification-of-outputs><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#verification-of-outputs>Verification of Outputs</a></h4> <p>Ensuring reliability through verification:</p> <ul> <li><strong>Fact-Checking</strong>: Verifying factual claims against reliable sources</li> <li><strong>Consistency Checks</strong>: Ensuring internal consistency in responses</li> <li><strong>Hallucination Detection</strong>: Identifying when the agent is generating unfounded information</li> <li><strong>Confidence Scoring</strong>: Assessing the reliability of different parts of a response</li> </ul> <h4 id=meta-prompting-and-prompt-chaining><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#meta-prompting-and-prompt-chaining>Meta-Prompting and Prompt Chaining</a></h4> <p>Sophisticated prompting techniques:</p> <ul> <li><strong>Meta-Prompting</strong>: Using the LLM to generate or refine its own prompts</li> <li><strong>Prompt Chaining</strong>: Connecting multiple prompts in sequence to handle complex workflows</li> <li><strong>Adaptive Prompting</strong>: Modifying prompts based on user responses or task progress</li> </ul> <p>These techniques allow for more flexible and powerful agent behaviors.</p> <h3 id=multi-agent-systems><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#multi-agent-systems>Multi-Agent Systems</a></h3> <p>Multiple specialized agents can collaborate to solve complex problems.</p> <div class=mermaid-wrapper> <pre class=mermaid><code>flowchart TD
    U[User] --&gt; C[Coordinator Agent]

    C --&gt; P[Planner Agent]
    C --&gt; R[Researcher Agent]
    C --&gt; E[Expert Agent]
    C --&gt; CR[Critic Agent]

    P --&gt; C
    R --&gt; C
    E --&gt; C
    CR --&gt; C

    C --&gt; U

    subgraph Communication
        P &lt;-.-&gt; R
        R &lt;-.-&gt; E
        E &lt;-.-&gt; CR
        P &lt;-.-&gt; CR
    end

    style U fill:#f9f9f9,stroke:#333,stroke-width:1px
    style C fill:#ffcc99,stroke:#333,stroke-width:2px
    style P fill:#ccffcc,stroke:#333,stroke-width:1px
    style R fill:#ccffcc,stroke:#333,stroke-width:1px
    style E fill:#ccffcc,stroke:#333,stroke-width:1px
    style CR fill:#ccffcc,stroke:#333,stroke-width:1px
    style Communication opacity:0.2</code></pre> </div> <h4 id=agent-collaboration-models><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#agent-collaboration-models>Agent Collaboration Models</a></h4> <p>Different models for agent collaboration:</p> <ul> <li><strong>Hierarchical</strong>: Supervisor agents coordinate specialized worker agents</li> <li><strong>Peer-to-Peer</strong>: Agents communicate directly with each other</li> <li><strong>Market-Based</strong>: Agents bid for tasks based on their capabilities</li> <li><strong>Consensus-Based</strong>: Agents work together to reach agreement on solutions</li> </ul> <h4 id=specialized-agent-roles><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#specialized-agent-roles>Specialized Agent Roles</a></h4> <p>Multi-agent systems often feature specialized roles:</p> <ul> <li><strong>Planner</strong>: Designs overall strategy and breaks down tasks</li> <li><strong>Researcher</strong>: Gathers information from various sources</li> <li><strong>Expert</strong>: Provides domain-specific knowledge and analysis</li> <li><strong>Critic</strong>: Evaluates and improves outputs</li> <li><strong>Coordinator</strong>: Manages communication between agents</li> </ul> <h4 id=communication-protocols><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#communication-protocols>Communication Protocols</a></h4> <p>Effective inter-agent communication requires:</p> <ul> <li><strong>Message Formats</strong>: Structured formats for exchanging information</li> <li><strong>Dialogue Management</strong>: Tracking conversation state between agents</li> <li><strong>Knowledge Sharing</strong>: Methods for sharing relevant information</li> <li><strong>Conflict Resolution</strong>: Mechanisms for resolving disagreements</li> </ul> <h4 id=consensus-mechanisms><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#consensus-mechanisms>Consensus Mechanisms</a></h4> <p>When agents must agree on a course of action:</p> <ul> <li><strong>Voting</strong>: Simple majority or weighted voting schemes</li> <li><strong>Debate</strong>: Agents present arguments and counter-arguments</li> <li><strong>Evidence Evaluation</strong>: Assessing the quality of evidence presented</li> <li><strong>Meta-Evaluation</strong>: Using another agent to evaluate competing proposals</li> </ul> <p>Multi-agent systems enable more complex problem-solving than any single agent could achieve alone.</p> <h3 id=building-and-deploying-agents><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#building-and-deploying-agents>Building and Deploying Agents</a></h3> <p>Practical considerations for implementing LLM agents.</p> <h4 id=frameworks-and-libraries><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#frameworks-and-libraries>Frameworks and Libraries</a></h4> <p>Popular tools for building agents:</p> <ul> <li><strong>LangChain</strong>: Framework for building language model applications</li> <li><strong>LlamaIndex</strong>: Tools for connecting LLMs to external data</li> <li><strong>AutoGPT</strong>: Autonomous AI agent framework</li> <li><strong>Microsoft Semantic Kernel</strong>: Framework for integrating AI with traditional programming</li> </ul> <h4 id=evaluation-metrics><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#evaluation-metrics>Evaluation Metrics</a></h4> <p>Assessing agent performance:</p> <ul> <li><strong>Task Completion Rate</strong>: How often the agent successfully completes tasks</li> <li><strong>Efficiency</strong>: Number of steps or time required to complete tasks</li> <li><strong>Accuracy</strong>: Correctness of information and actions</li> <li><strong>User Satisfaction</strong>: User ratings and feedback</li> <li><strong>Hallucination Rate</strong>: Frequency of unfounded claims</li> </ul> <h4 id=safety-considerations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#safety-considerations>Safety Considerations</a></h4> <p>Important safety measures:</p> <ul> <li><strong>Action Limitations</strong>: Restricting potentially harmful actions</li> <li><strong>User Confirmation</strong>: Requiring approval for significant actions</li> <li><strong>Monitoring</strong>: Tracking agent behavior for unexpected patterns</li> <li><strong>Transparency</strong>: Making reasoning and sources clear to users</li> </ul> <h4 id=deployment-patterns><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#deployment-patterns>Deployment Patterns</a></h4> <p>Common approaches to deployment:</p> <ul> <li><strong>Serverless Functions</strong>: Deploying components as cloud functions</li> <li><strong>Containerization</strong>: Packaging agents and dependencies in containers</li> <li><strong>API Services</strong>: Exposing agent capabilities through APIs</li> <li><strong>Edge Deployment</strong>: Running lightweight agents on edge devices</li> </ul> <p>Careful attention to these aspects ensures agents that are effective, reliable, and safe.</p> <h3 id=future-directions><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#future-directions>Future Directions</a></h3> <p>The field of LLM agents is rapidly evolving. Here are some emerging trends and challenges:</p> <h4 id=current-limitations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#current-limitations>Current Limitations</a></h4> <p>Areas needing improvement:</p> <ul> <li><strong>Reasoning Abilities</strong>: Enhancing logical and causal reasoning</li> <li><strong>Tool Creation</strong>: Enabling agents to create new tools as needed</li> <li><strong>True Autonomy</strong>: Reducing the need for human oversight</li> <li><strong>Cross-Domain Knowledge</strong>: Applying knowledge across different domains</li> <li><strong>Efficiency</strong>: Reducing computational requirements</li> </ul> <h4 id=research-frontiers><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#research-frontiers>Research Frontiers</a></h4> <p>Exciting areas of research:</p> <ul> <li><strong>Embodied Agents</strong>: Connecting language models to robotic systems</li> <li><strong>Multi-Modal Agents</strong>: Integrating text, vision, audio, and other modalities</li> <li><strong>Continual Learning</strong>: Agents that learn and improve through interaction</li> <li><strong>Collective Intelligence</strong>: Emergent capabilities from agent collaboration</li> <li><strong>Neural-Symbolic Approaches</strong>: Combining neural networks with symbolic reasoning</li> </ul> <h4 id=potential-applications><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#potential-applications>Potential Applications</a></h4> <p>Promising applications for advanced agents:</p> <ul> <li><strong>Personalized Education</strong>: Tutors adapted to individual learning styles</li> <li><strong>Scientific Discovery</strong>: Agents that generate and test hypotheses</li> <li><strong>Healthcare Assistance</strong>: Diagnostic and treatment planning support</li> <li><strong>Creative Collaboration</strong>: Partners for writing, design, and other creative tasks</li> <li><strong>Autonomous Systems</strong>: Self-directed systems that adapt to changing conditions</li> </ul> <h4 id=ethical-considerations><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#ethical-considerations>Ethical Considerations</a></h4> <p>Important ethical questions:</p> <ul> <li><strong>Transparency</strong>: Ensuring users understand agent capabilities and limitations</li> <li><strong>Accountability</strong>: Determining responsibility for agent actions</li> <li><strong>Privacy</strong>: Protecting sensitive information used by agents</li> <li><strong>Bias</strong>: Addressing biases in training data and reasoning</li> <li><strong>Human Augmentation</strong>: Enhancing rather than replacing human capabilities</li> </ul> <p>The future of LLM agents will depend on thoughtful approaches to these challenges and opportunities.</p> <h3 id=conclusion><a class=toclink href=../../2025/03/28/a-visual-guide-to-llm-agents/#conclusion>Conclusion</a></h3> <p>LLM agents represent a significant evolution in artificial intelligence, transforming passive language models into active, capable assistants. By combining the language understanding of LLMs with the ability to perceive, reason, and act, these agents can solve increasingly complex problems and provide more valuable assistance.</p> <p>As the technology continues to develop, we can expect agents to become more autonomous, capable, and integrated into our daily lives and work. The journey from simple language models to sophisticated agents is just beginning, with many exciting possibilities on the horizon.</p> <p>The most successful approaches will likely be those that thoughtfully combine the strengths of artificial and human intelligence, creating systems that augment human capabilities rather than simply attempting to replace them.</p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../ class="md-footer__link md-footer__link--prev" aria-label="Previous: Deeper Dives: Thoughts on AI, Code, and Building Better Systems"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Deeper Dives: Thoughts on AI, Code, and Building Better Systems </div> </div> </a> <a href=../llms/ class="md-footer__link md-footer__link--next" aria-label="Next: LLMs"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> LLMs </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 PromptX AI </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/PromtEngineer/localGPT target=_blank rel=noopener title="localGPT on GitHub" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=feed_rss_created.xml target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <a href=https://twitter.com/engineerrprompt target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> <a href=https://github.com/PromtEngineer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.linkedin.com/in/engineerprompt target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://www.youtube.com/@engineerprompt target=_blank rel=noopener title=www.youtube.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../javascripts/mermaid-zoom.js></script> <script src=../../../javascripts/mathjax.js></script> <script src=../../../javascripts/analytics.js></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>