<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Model Context Protocol (MCP) - A Quick Intro"><meta name=author content="PromptX AI"><link href=https://engineerprompt.ai/writing/2025/03/31/introduction/ rel=canonical><link href=../../28/a-visual-guide-to-llm-agents/ rel=prev><link href=../model-context-protocol-mcp---a-quick-intro/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../../../feed_rss_updated.xml><link rel=icon href=../../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.12"><title>Introduction - PromptX AI</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.2afb09e1.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../../../stylesheets/extra.css><link rel=stylesheet href=../../../../../stylesheets/mermaid.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-686PKP2V2V"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-686PKP2V2V",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-686PKP2V2V",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Introduction - PromptX AI"><meta property=og:description content="Model Context Protocol (MCP) - A Quick Intro"><meta property=og:image content=https://engineerprompt.ai/assets/images/social/writing/posts/mcp_detailed_tutorial.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://engineerprompt.ai/writing/2025/03/31/introduction/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Introduction - PromptX AI"><meta name=twitter:description content="Model Context Protocol (MCP) - A Quick Intro"><meta name=twitter:image content=https://engineerprompt.ai/assets/images/social/writing/posts/mcp_detailed_tutorial.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#introduction class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../.. title="PromptX AI" class="md-header__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PromptX AI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Introduction </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../../services/ class=md-tabs__link> Consulting </a> </li> <li class=md-tabs__item> <a href=../../../../../rag-beyond-basics/ class=md-tabs__link> RAG Course </a> </li> <li class=md-tabs__item> <a href=../../../../../youtube/ class=md-tabs__link> YouTube Videos </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Writing </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title="PromptX AI" class="md-nav__button md-logo" aria-label="PromptX AI" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> PromptX AI </label> <div class=md-nav__source> <a href=https://github.com/PromtEngineer/localGPT title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> localGPT </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../services/ class=md-nav__link> <span class=md-ellipsis> Consulting </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class=md-nav__item> <a href=../../../../../rag-beyond-basics/ class=md-nav__link> <span class=md-ellipsis> RAG Course </span> </a> </li> <li class=md-nav__item> <a href=../../../../../youtube/ class=md-nav__link> <span class=md-ellipsis> YouTube Videos </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Writing </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=true> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Writing </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/agents/ class=md-nav__link> <span class=md-ellipsis> Agents </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/llms/ class=md-nav__link> <span class=md-ellipsis> LLMs </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-mcp class=md-nav__link> <span class=md-ellipsis> What is MCP? </span> </a> </li> <li class=md-nav__item> <a href=#purpose-and-problem-addressed class=md-nav__link> <span class=md-ellipsis> Purpose and Problem Addressed </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src="https://avatars.githubusercontent.com/u/134474669?v=4" alt="Muhammad Farooq"> </span> <span class=md-profile__description> <strong> <a href="https://x.com/intent/follow?screen_name=engineerrprompt">Muhammad Farooq</a> </strong> <br> Creator </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-03-31 00:00:00+00:00" class=md-ellipsis>2025/03/31</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=../../../../category/llms/ >LLMs</a>, <a href=../../../../category/agents/ >Agents</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 38 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <a href=https://github.com/PromtEngineer/localGPT/edit/main/docs/writing/posts/mcp_detailed_tutorial.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg> </a> <a href=https://github.com/PromtEngineer/localGPT/raw/main/docs/writing/posts/mcp_detailed_tutorial.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5"/></svg> </a> <h1 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h1> <h2 id=what-is-mcp>What is MCP?<a class=headerlink href=#what-is-mcp title="Permanent link">&para;</a></h2> <p>The Model Context Protocol (MCP) is an open, standardized protocol introduced by Anthropic that bridges AI models with external data sources, tools, and services. Think of MCP like a "USB-C for AI applications" – it provides a universal adapter for connecting AI assistants to various content repositories, business tools, code environments, and APIs. By defining a common interface (built on JSON-RPC 2.0) for communication, MCP enables large language models (LLMs) to invoke functions, retrieve data, or use predefined prompts from external systems in a consistent and secure way.</p> <h2 id=purpose-and-problem-addressed>Purpose and Problem Addressed<a class=headerlink href=#purpose-and-problem-addressed title="Permanent link">&para;</a></h2> <p>MCP was designed to solve a major integration challenge often called the "M×N problem" in AI development. Traditionally, integrating M different LLMs with N different tools or data sources required custom connectors for each combination – a combinatorial explosion of ad-hoc code. This meant AI systems were largely isolated from live data, trapped behind information silos unless developers painstakingly wired in each external API or database. MCP addresses this by providing one standardized "language" for all interactions.</p> <p>In practice, developers can create one MCP-compliant interface for a data source or tool, and any MCP-enabled AI application can connect to it. This open standard thus replaces fragmented one-off integrations with a sustainable ecosystem of compatible clients and servers. The result is a simpler, more scalable way to give AI assistants access to the fresh, relevant context they need – whether it's company documents, live databases, or web results.</p> <p>Importantly, MCP is two-way and secure: it enables LLMs to query data and perform actions, while allowing organizations to keep data access controlled (the latest spec even supports OAuth 2.1 for authentication).</p> <p>In summary, MCP's purpose is to make AI integrations interoperable, secure, and context-rich, turning isolated LLMs into truly context-aware systems.</p> <h1 id=comparison-model-context-protocol-mcp-vs-simpler-function-calling-apis>Comparison: Model Context Protocol (MCP) vs. Simpler Function-Calling APIs<a class=headerlink href=#comparison-model-context-protocol-mcp-vs-simpler-function-calling-apis title="Permanent link">&para;</a></h1> <p>MCP represents a more comprehensive and standardized approach to AI-tool integration compared to the simpler function-calling mechanisms provided by vendors like OpenAI or Google. Below we contrast MCP with typical LLM function-calling or plugin approaches:</p> <h2 id=scope-of-integration>Scope of Integration<a class=headerlink href=#scope-of-integration title="Permanent link">&para;</a></h2> <p>Traditional function calling (e.g. OpenAI's function call API) lets a developer define a few functions that an LLM can call during a single chat session. While useful, this is relatively limited in scope – each function integration is custom and tied to one specific model or platform. In contrast, MCP covers a broader scope: it standardizes not just function calls, but also data retrieval (resources) and reusable prompt workflows across any number of tools. An MCP server can expose multiple capabilities (data, tools, prompts) at once to any compatible AI client. This turns the one-off "tool use" into a universal interface for many integrations at scale.</p> <p>For example, via MCP a single AI assistant could simultaneously connect to a code repository, a database, and a web browser tool – all through one protocol – whereas vanilla function calling would require baking in each integration separately.</p> <h2 id=standardization-and-interoperability>Standardization and Interoperability<a class=headerlink href=#standardization-and-interoperability title="Permanent link">&para;</a></h2> <p>OpenAI's function calls and ChatGPT plugins are proprietary approaches – they work primarily within OpenAI's ecosystem (or Google's, for their functions) and follow vendor-specific formats. Each ChatGPT plugin essentially required its own mini-integration defined by an OpenAPI spec, and only certain platforms (like ChatGPT or Bing) could use those plugins.</p> <p>By contrast, MCP is an open standard from the ground up. It's vendor-agnostic and designed for broad adoption: any LLM provider or tool builder can implement MCP's JSON-RPC interface. Think of ChatGPT plugins as specialized tools in a closed toolbox, whereas MCP is an open toolkit that any AI platform or developer can utilize. This standardization means an MCP-compliant server (say for Google Drive, Slack, etc.) can be used by Anthropic's Claude, OpenAI's ChatGPT (which recently announced support), or a local open-source LLM alike. MCP's interoperability has gained momentum – even OpenAI and Microsoft have added support for it, signaling industry convergence on open standards.</p> <h2 id=state-management-and-persistence>State Management and Persistence<a class=headerlink href=#state-management-and-persistence title="Permanent link">&para;</a></h2> <p>Simple function-calling is typically stateless beyond the immediate request. The LLM calls a function and gets a result, but there is no persistent connection – each tool use is a one-shot call embedded in the prompt/response cycle. MCP, on the other hand, establishes a persistent client–server connection. An MCP server can maintain state across multiple calls (e.g. keep a database connection open, remember prior queries, cache results, etc.). This opens the door to more complex, multi-turn interactions with tools.</p> <p>For instance, an MCP server could handle a session with an AI agent, allowing the agent to iteratively query data, refine results, or perform a sequence of actions with continuity. The client-server architecture means there's an ongoing "conversation" between the AI (client) and the tool backend (server), rather than just isolated function calls. Additionally, MCP's model of Resources (described later) allows large data to be loaded or referenced as needed, instead of stuffing everything into a single prompt window. In short, MCP is built for long-lived, context-rich interactions with external systems, unlike the transient function calls of simpler APIs.</p> <h2 id=safety-and-permissions>Safety and Permissions<a class=headerlink href=#safety-and-permissions title="Permanent link">&para;</a></h2> <p>With basic function calling, developers must implement any approval or safety checks themselves in the loop. MCP formalizes some of this. By design, MCP assumes a human or host application is in the loop for potentially risky actions – for example, tools calls are intended to be approved by a user, and the protocol supports permissioning and authentication at a systemic level. The updated MCP spec even includes standardized OAuth 2.1 flows for granting access to protected resources. This level of built-in security and consent is beyond what the raw function-call interfaces provide, making MCP better suited for enterprise and sensitive integrations.</p> <h2 id=multi-model-flexibility>Multi-Model Flexibility<a class=headerlink href=#multi-model-flexibility title="Permanent link">&para;</a></h2> <p>Because MCP decouples tools from any specific model, it gives developers flexibility to switch LLM providers or models without redoing integrations. For example, an application could use Anthropic's Claude today and swap to another MCP-compatible model tomorrow, and all the same MCP servers (tools) would still work. In contrast, OpenAI's function calling is tightly coupled to using OpenAI's models; there's no guarantee those function definitions would port to Google's PaLM or others. MCP's standardized interface acts as a neutral layer between models and tools.</p> <h1 id=interaction-flow-traditional-function-call-vs-mcp>Interaction Flow: Traditional Function Call vs. MCP<a class=headerlink href=#interaction-flow-traditional-function-call-vs-mcp title="Permanent link">&para;</a></h1> <p>To illustrate the difference, let's compare the flow of an AI using an external weather API via traditional function-calling versus using MCP:</p> <h2 id=function-calling-workflow-eg-openai-api>Function-Calling Workflow (e.g. OpenAI API)<a class=headerlink href=#function-calling-workflow-eg-openai-api title="Permanent link">&para;</a></h2> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant LLM_API as LLM (with function-calling)
    participant DevApp as Developer App
    participant WeatherAPI as Weather API
    User-&gt;&gt;LLM_API: "What is the weather in LA tomorrow?"
    note right of LLM_API: LLM decides a function is needed&lt;br/&gt;from provided definitions
    LLM_API--&gt;&gt;DevApp: Function call output (e.g. `get_weather(location="LA")`)
    DevApp-&gt;&gt;WeatherAPI: Invoke weather API (with location="LA")
    WeatherAPI--&gt;&gt;DevApp: Returns weather data
    DevApp--&gt;&gt;LLM_API: Provide function result (weather data)
    LLM_API--&gt;&gt;User: Final answer using the data</code></pre> <p>In the above function-calling flow, the developer had to predefine a get_weather function in the prompt and intercept the model's output to call the API. The interaction with the external service (Weather API) is not standardized – it's custom code in the developer's app, and the connection is not persistent (just a single call). The LLM itself doesn't maintain a direct connection to tools; it relies on the developer to mediate every call.</p> <h2 id=mcp-workflow-clientserver-with-weather-tool>MCP Workflow (Client–Server with Weather Tool)<a class=headerlink href=#mcp-workflow-clientserver-with-weather-tool title="Permanent link">&para;</a></h2> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI_Client as AI App (MCP Client)
    participant MCPServer as Weather MCP Server
    participant WeatherAPI as Weather API
    User-&gt;&gt;AI_Client: "What is the weather in LA tomorrow?"
    AI_Client-&gt;&gt;MCPServer: (via MCP) Request tool "get-forecast" with {"location":"LA"}
    MCPServer-&gt;&gt;WeatherAPI: Fetch forecast for LA
    WeatherAPI--&gt;&gt;MCPServer: Weather data
    MCPServer--&gt;&gt;AI_Client: Result of get-forecast (data or summary)
    AI_Client--&gt;&gt;User: Final answer incorporating weather info</code></pre> <p>In the MCP flow, the AI application (client) has an established connection to a Weather MCP server that exposes a get-forecast tool. The LLM (e.g. Claude or ChatGPT with MCP support) can directly trigger that tool via the MCP client, without the developer writing custom code for this API call. The MCP server handles communicating with the Weather API and returns the result in a standardized format. The AI model receives the weather info through MCP and can respond to the user.</p> <p>Notably, this happens through a consistent protocol – any other MCP-compatible weather service would work the same way. Also, the connection can be two-way: if the Weather server had multiple actions or needed additional queries, the conversation over MCP could continue beyond a one-shot call. This demonstrates how MCP generalizes and extends the idea of function calling into a full client–server integration pattern.</p> <h1 id=core-components-of-mcp>Core Components of MCP<a class=headerlink href=#core-components-of-mcp title="Permanent link">&para;</a></h1> <p>MCP defines a set of core components (primitives) that structure how clients and servers interact and what they can do. There are three server-side primitives (which servers provide) and two client-side primitives (which clients provide). Understanding these components is key to using MCP effectively:</p> <h2 id=server-side-primitives>Server-Side Primitives<a class=headerlink href=#server-side-primitives title="Permanent link">&para;</a></h2> <p>These are capabilities that an MCP Server exposes to add context or functionality for the LLM. An MCP server can implement any or all of these:</p> <h3 id=resources>Resources<a class=headerlink href=#resources title="Permanent link">&para;</a></h3> <p>Resources are pieces of data or content that the server makes available for the AI to read and include in its context. A resource might be a file's contents, a database record, an email, an image, etc. – anything that could be useful as additional information for the model. Each resource is identified by a URI (like file://docs/report.pdf or db://customers/123) and can be fetched via the protocol.</p> <p>Importantly, resources are typically read-only context: the AI doesn't execute them, it uses them as reference material. For example, a "Docs Server" might expose a document's text as a resource so the AI can quote or summarize it. Resources are usually application-controlled, meaning the client or user decides which resources to pull in (to avoid flooding the model with irrelevant data). In practice, an AI interface might let a user pick a file (resource) to share with the assistant. If truly on-demand model-driven access is needed instead, that's where tools come in. Resources help inject structured data into the LLM's prompt context in a standardized way.</p> <h3 id=tools>Tools<a class=headerlink href=#tools title="Permanent link">&para;</a></h3> <p>Tools are executable functions that the server can perform on request. These are analogous to the "functions" in function-calling, but defined in a standard JSON schema format and invoked via MCP endpoints. Tools allow the AI to perform actions or fetch calculated information – for example, run a database query, call an external API, execute a computation, or even control a web browser.</p> <p>Each tool has a name, a description, and a JSON schema for its input parameters. The server lists available tools, and the AI (through the client) can call them by name with the required args. Tools are model-controlled with a human in the loop. This means the design assumes the AI can decide when to use a tool (e.g. the LLM's reasoning says "I should use the database_query tool now"), but the user or client must approve the action for safety.</p> <p>When invoked, the server executes the underlying function and returns the result (or error) to the client. Tools can be simple (e.g. a calculator) or very powerful (e.g. a tool that can send an email or modify data). Unlike resources, tools can have side effects or dynamic outputs – they may change external state or retrieve live data. This makes them essential for building AI agents that act, not just observe.</p> <h3 id=prompts>Prompts<a class=headerlink href=#prompts title="Permanent link">&para;</a></h3> <p>Prompts in MCP are reusable prompt templates or workflows that servers provide. A Prompt primitive is essentially a predefined way to interact with the model, which could involve multiple steps or a structured input. Servers define prompts to standardize common interactions – for example, a prompt template to "Summarize document X in style Y", or a multi-turn workflow to "Debug an error by asking these follow-up questions".</p> <p>Each prompt is identified by a name and can accept input arguments (e.g. the document to summarize, the style to use). The client can query the server for available prompts, which might be presented to the user (for instance, an IDE could show a list of MCP prompt actions like "Explain this code" powered by the server). When a prompt is selected, the server can then guide the interaction: it might inject certain instructions to the model, include relevant resources automatically, or even orchestrate a chain of LLM calls.</p> <p>Prompts are typically user-controlled, meaning the user explicitly triggers those workflows (like choosing a predefined query or action). Under the hood, a prompt might use the other primitives – e.g. it could fetch some resource or call a tool as part of its process – to produce the final result. Prompts let developers encapsulate complex behaviors or multi-step conversations behind a single command, making them easy to reuse.</p> <p>In summary, on the server side: Resources = data context, Tools = actions/functions, and Prompts = preset conversational patterns. These primitives "speak" JSON-RPC – e.g. there are standard methods like resources/list, resources/read, tools/list, tools/call, prompts/list, etc., which the client can call to discover and use these primitives. By separating them, MCP makes clear what the AI is intending to do – whether it's just reading data, executing a function, or following a guided script, each has a defined protocol.</p> <h2 id=client-side-primitives>Client-Side Primitives<a class=headerlink href=#client-side-primitives title="Permanent link">&para;</a></h2> <p>These are features that an MCP Client (host application) provides, which servers can leverage. There are two main client-side primitives:</p> <h3 id=roots>Roots<a class=headerlink href=#roots title="Permanent link">&para;</a></h3> <p>A Root is essentially a boundary or entry point that the client suggests to the server for where to focus. When an MCP connection starts, the client can send one or more "root URIs" to the server. This informs the server about the relevant workspace or scope.</p> <p>For instance, if a developer is working in /home/user/myproject/, the client might set that as a root for a Filesystem server – indicating the server should consider that directory as the project context (and not roam outside it). Or a root could be a specific URL or database name that the server should use as the primary endpoint. Roots thus provide guidance and organization, especially when multiple data sources are in use.</p> <p>They do not hard-enforce access limitations (the server could technically go beyond, unless the server itself restricts it), but they serve as a contract of what the current context is. This helps keep interactions focused and secure – the server knows what subset of data it should operate on, and the client knows the server won't unexpectedly access unrelated data. Not all scenarios require roots, but they are very useful in development tools (for setting project scope) and similar contexts.</p> <h3 id=sampling>Sampling<a class=headerlink href=#sampling title="Permanent link">&para;</a></h3> <p>Sampling is a powerful client-side feature that allows the server to ask the client's LLM to generate a completion. In simpler terms, it lets the server turn around and say: "I need the AI to complete/answer this sub-task for me." This might sound unusual, but it enables advanced workflows like agents that can reason recursively.</p> <p>For example, imagine a complex tool that, in the middle of its function, realizes it needs an LLM's help to parse something or make a decision – the server can send a sampling request to the client, providing a prompt, and the client will invoke the LLM to get a result, then return it to the server. The server can then proceed using that result. All of this happens under the hood of the protocol, with the crucial caveat that the user (or host application) should approve any such additional LLM call.</p> <p>The design is meant for "agents calling themselves," but with human oversight to avoid infinite loops or undesired actions. Sampling requests include a formatted message (or conversation) that the server wants the model to continue, and possibly preferences for which model or how to balance speed vs. accuracy.</p> <p>This feature essentially lets the server compose intelligence: an MCP server could chain multiple LLM calls or do intermediate reasoning by leveraging the client's model. It's an advanced capability (not all clients support it yet, and it should be used judiciously), but it opens the door to sophisticated agent behaviors. With Sampling, MCP isn't just one LLM interacting with tools – it could be tools temporarily invoking the LLM in return, enabling nested AI reasoning.</p> <p>These five components – Resources, Tools, Prompts, Roots, and Sampling – form the core of MCP's design. The protocol is essentially a set of JSON-RPC methods that implement these primitives in a standardized way. By mixing and matching them, one can achieve various integration patterns. For example, a given MCP server might only implement Tools (if it purely offers actions like a Calculator server), or implement Resources + Prompts (if it mainly offers data and some templated queries on that data), etc. Clients similarly may or may not support Sampling or Roots depending on the application. The upshot is that MCP gives a common structure to describe "what an AI can do" in a given context: read this data, use these tools, follow these templates.</p> <h1 id=architecture-clientserver-design>Architecture: Client–Server Design<a class=headerlink href=#architecture-clientserver-design title="Permanent link">&para;</a></h1> <p>At a high level, MCP follows a classic client–server architecture tailored to AI needs. The design involves a Host application that incorporates an MCP client, which connects to one or more external MCP servers. Let's break down the pieces:</p> <h2 id=mcp-host-and-client>MCP Host and Client<a class=headerlink href=#mcp-host-and-client title="Permanent link">&para;</a></h2> <p>The host is the main AI application or agent environment. For example, Claude Desktop (Anthropic's chat app) is an MCP host, as are certain IDE extensions, chat UIs, or agent orchestration frameworks. The host contains the MCP client component, which is responsible for managing the connection to servers and mediating between the LLM and those servers.</p> <p>The client knows how to speak MCP (it implements the protocol on the client side: sending requests like tools/call to servers, and handling incoming requests like sampling/createMessage from servers). It maintains a 1:1 connection with each server it uses. In practice, a host might spawn multiple client connections if it's using multiple servers at once (for example, connect to a GitHub server and a Slack server concurrently).</p> <p>The LLM itself (the model) is usually integrated into the host – for instance, Claude (the model) running locally or via API is what generates the responses, and the MCP client is feeding it information and tool results.</p> <h2 id=mcp-server>MCP Server<a class=headerlink href=#mcp-server title="Permanent link">&para;</a></h2> <p>An MCP server is a lightweight program or process that exposes a specific set of capabilities (the primitives we discussed) via the MCP protocol. Each server typically corresponds to a particular domain or service. For example, one server might interface with a file system (exposing files as resources, and perhaps a prompt for searching files), another might interface with Google Drive, another with GitHub, a database, or a web browser, etc.</p> <p>The server runs separately from the AI model – it could be on the same machine or a remote one – and communicates with the client over a communication channel. Servers declare what capabilities they have during initialization (for instance, "I support resources and tools, but not prompts") so the client knows how to interact.</p> <p>Importantly, servers are stateless with respect to the protocol (they handle requests as they come) but can maintain internal state or connections (e.g. keep a DB session). They do not directly talk to the LLM; instead, all interaction is through the structured protocol messages.</p> <h2 id=communication-transports-json-rpc>Communication (Transports &amp; JSON-RPC)<a class=headerlink href=#communication-transports-json-rpc title="Permanent link">&para;</a></h2> <p>MCP's communication is built on JSON-RPC 2.0 as the message format. This means every action (like listing tools or calling a resource) is a JSON request with a method name and params, and responses are JSON objects with results or errors. The protocol supports both requests (with responses) and notifications (one-way messages) in both directions – so the server can call methods on the client (like sampling/createMessage) and the client can call methods on the server (like tools/call).</p> <p>The underlying transport layer can vary. Two common transport modes are:</p> <p><strong>Stdio (Standard I/O)</strong>: Ideal for local setups, the client can launch the server as a subprocess and communicate via its stdin/stdout streams. This is simple and secure for local integrations – e.g. Claude Desktop can run an MCP server on your machine via stdio.</p> <p><strong>HTTP (with SSE or WebSocket)</strong>: For remote servers, MCP can run over HTTP. Initially, the spec used HTTP + Server-Sent Events (SSE) for server-to-client streaming, but an updated streamable HTTP transport was introduced (Mar 2025) to allow full bidirectional streaming over a single connection. In either case, the client might make HTTP POST requests for each JSON-RPC call and keep a channel open for streaming responses. WebSocket is another possible transport for continuous two-way communication.</p> <p>The transport is abstracted in MCP – developers don't usually worry about it beyond choosing one that fits their deployment. The key is that client and server establish a connection and exchange JSON-RPC messages. The first messages in a session are an initial handshake: the client sends an initialize request (with its MCP version and what features it supports, e.g. "I support roots and sampling") and the server responds with its own capabilities and version. Once initialization is done, they exchange an initialized notification and then the session is ready for general use.</p> <p>Either side can then send requests or notifications according to the protocol. For example, right after init, the client might call prompts/list or tools/list to discover what the server offers. From then on, the LLM (through the client) can start using those tools/resources.</p> <h2 id=lifecycle>Lifecycle<a class=headerlink href=#lifecycle title="Permanent link">&para;</a></h2> <p>The MCP connection stays alive as long as needed. If the user closes the host app or a certain task is done, the client can send a shutdown message or simply disconnect the transport, ending the session. Servers and clients are expected to handle disconnects or errors gracefully (MCP defines standard error codes for invalid requests, internal errors, etc., similar to JSON-RPC's spec).</p> <p>Below is a diagram of the core MCP architecture with its main components and data flow:</p> <pre class=mermaid><code>flowchart LR
    subgraph AI_Host[Host Application]
        direction TB
        AILLM["LLM (AI Assistant)"]
        MCPClient["MCP Client Component"]
        AILLM -- uses --&gt; MCPClient
    end
    subgraph MCP_Server1["MCP Server: Tool/Data Source 1"]
        direction TB
        Tools1["Tools"]
        Resources1["Resources"]
        Prompts1["Prompts"]
    end
    subgraph MCP_Server2["MCP Server: Tool/Data Source 2"]
        direction TB
        Tools2["Tools"]
        Resources2["Resources"]
        %% Prompts2 not implemented in this one for illustration
    end
    %% Host connects to multiple servers
    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server1
    MCPClient -- JSON-RPC over Transport --&gt; MCP_Server2
    Resources1 -- fetch data --&gt; DataSource1["Local/Remote Data\n(e.g. files, DB)"]
    Tools1 -- perform actions --&gt; DataSource1
    Tools2 -- perform actions --&gt; Service2["External Service API"]
    Resources2 -- fetch data --&gt; Service2</code></pre> <p>In this diagram, the Host application (which contains the AI model and the MCP client library) maintains connections to two MCP servers. Server1 might be a local server (e.g. providing file system access – hence its tools/resources tie into local files or a database), and Server2 might be a remote service integration (e.g. a weather API server, which calls an external web service). The MCP client handles the JSON-RPC messaging to each server.</p> <p>When the LLM needs something, it formulates a request that the client sends to the appropriate server; when a server needs something (like performing a sampling request or notifying of new data), it sends that to the client. The primitives (Tools, Resources, Prompts) inside each server define what it can do.</p> <p>For example, if the user asks the AI "Summarize the latest sales data", the AI client might use a Prompts primitive from Server1 that knows how to retrieve sales records (as Resources) and then guide the model to summarize them. Meanwhile, if the user asks "Also, what's the weather in LA?", the AI client can call a Tool on Server2 to get that info. All of this happens through the uniform MCP interface.</p> <p>As the ecosystem grows, we might have dozens of servers (for Git, Jira, Gmail, etc.), but crucially, an AI app doesn't need to have custom code for each – it just speaks MCP to whichever servers are available.</p> <h2 id=standardization>Standardization<a class=headerlink href=#standardization title="Permanent link">&para;</a></h2> <p>Because both client and server follow the MCP spec, any compatible client can talk to any server. This fosters an ecosystem where companies and the community are building a "library" of MCP servers for many common tools. Anthropic has open-sourced reference servers for Google Drive, Slack, Git, GitHub, Postgres, web browser automation, and more. Community contributors have added many others (at the time of writing, hundreds of MCP connectors exist for various services).</p> <p>On the client side, multiple products have added MCP support (Claude's apps, developer IDE plugins, agent frameworks like LangChain, etc.). This architecture and standardization are what turn MCP into that "universal port" for AI – it decouples the integration logic (server side) from the AI agent logic (client side), with a well-defined protocol in between.</p> <h1 id=python-tutorial-building-and-using-mcp-step-by-step>Python Tutorial: Building and Using MCP (Step-by-Step)<a class=headerlink href=#python-tutorial-building-and-using-mcp-step-by-step title="Permanent link">&para;</a></h1> <p>Now that we've covered concepts, let's get hands-on with a Python tutorial using MCP. We'll walk through creating a simple MCP server and demonstrate a client interacting with it. For this tutorial, we'll use the official MCP Python SDK (open-sourced by Anthropic) for convenience. (The SDK abstracts a lot of JSON-RPC boilerplate and lets us focus on defining our tools/resources). If you haven't already, you can install the SDK via pip:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>pip<span class=w> </span>install<span class=w> </span>mcp
</span></code></pre></div> <p>(The package name is mcp. This includes the server framework and some CLI tools.) Let's proceed in steps:</p> <h2 id=1-setting-up-a-basic-mcp-server>1. Setting Up a Basic MCP Server<a class=headerlink href=#1-setting-up-a-basic-mcp-server title="Permanent link">&para;</a></h2> <p>First, we'll create a minimal MCP server script in Python. This server will expose a trivial functionality just to verify everything works. We'll use the SDK's FastMCP class to create a server instance and run it.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># server.py</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=c1># Initialize an MCP server with a name</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=c1># Start the server (using stdio transport by default).</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=c1># This call will block and wait for a client connection.</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>In this snippet, we import FastMCP and instantiate it with a name "Demo Server". The FastMCP class is a high-level server that handles MCP protocol compliance and transport setup for us. By calling mcp.serve(), we tell it to begin listening for an MCP client. By default this uses the standard I/O transport (suitable if this script is launched by a host app). At this point, our server doesn't actually expose any tools or resources yet – it's an empty shell. But it can respond to basic protocol requests like initialize and will report that it has no capabilities.</p> <h2 id=2-exposing-a-tool-function-on-the-server>2. Exposing a Tool (Function) on the Server<a class=headerlink href=#2-exposing-a-tool-function-on-the-server title="Permanent link">&para;</a></h2> <p>Now we'll add a simple Tool to our server. The SDK provides a convenient decorator <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.tool to turn a Python function into an MCP tool. For demonstration, we'll add a basic arithmetic tool and then run the server.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=c1># Define a tool using a decorator. This tool adds two numbers.</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>With the <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.tool() decorator, the SDK automatically registers our add function as a Tool primitive. Under the hood, it will assign it a name (defaulting to the function name "add" here) and generate a JSON schema for the input parameters a and b (both integers, in this case). It also captures the docstring as the tool's description, which is useful for the AI to understand what it does.</p> <p>When this server is running, an MCP client that connects can call tools/list and will see something like:</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=p>{</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=w>  </span><span class=nt>&quot;tools&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=w>    </span><span class=p>{</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a><span class=w>      </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;add&quot;</span><span class=p>,</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=w>      </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;Add two numbers and return the result.&quot;</span><span class=p>,</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=w>      </span><span class=nt>&quot;inputSchema&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=w>        </span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;object&quot;</span><span class=p>,</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=w>        </span><span class=nt>&quot;properties&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=w>          </span><span class=nt>&quot;a&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;integer&quot;</span><span class=p>},</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=w>          </span><span class=nt>&quot;b&quot;</span><span class=p>:</span><span class=w> </span><span class=p>{</span><span class=nt>&quot;type&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;integer&quot;</span><span class=p>}</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=w>        </span><span class=p>}</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=w>      </span><span class=p>}</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a><span class=w>    </span><span class=p>}</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a><span class=w>  </span><span class=p>]</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a><span class=p>}</span>
</span></code></pre></div> <p>This is how the client discovers what functions are available. The inputSchema is derived from our function signature (using type hints, or it would default to accepting arbitrary JSON structure if not annotated). Now the add tool can be invoked via the MCP method tools/call with appropriate parameters.</p> <p>Let's test our server's tool quickly (in a real scenario, we'd connect an actual client, but here we'll simulate what a client would do for illustration). We can write a tiny MCP client snippet or use the SDK's CLI. For simplicity, imagine the client calls:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=c1># Pseudo-client code (for illustration only)</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp</span><span class=w> </span><span class=kn>import</span> <span class=n>Client</span>  <span class=c1># assume the SDK has a Client class for connecting</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=o>.</span><span class=n>connect_stdio</span><span class=p>(</span><span class=s2>&quot;./server.py&quot;</span><span class=p>)</span>   <span class=c1># Launch server.py and connect via stdio</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=n>tools</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>list_tools</span><span class=p>()</span>                    <span class=c1># should list the &quot;add&quot; tool</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>call_tool</span><span class=p>(</span><span class=s2>&quot;add&quot;</span><span class=p>,</span> <span class=p>{</span><span class=s2>&quot;a&quot;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s2>&quot;b&quot;</span><span class=p>:</span> <span class=mi>4</span><span class=p>})</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>  <span class=c1># expected output: 7</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a><span class=n>client</span><span class=o>.</span><span class=n>disconnect</span><span class=p>()</span>
</span></code></pre></div> <p>In reality, one might use the mcp CLI tool: for example, running mcp dev server.py will start the server and open an interactive interface (MCP Inspector) where you can manually invoke tools and see the results. Or, if using Claude Desktop, one could configure it to run this server and then simply ask Claude, "What's 3+4?" – the model could decide to use the add tool and would get the answer 7 from the server.</p> <p>The key takeaway is that adding a tool is as simple as defining a Python function with the SDK; MCP takes care of exposing it in a standardized way.</p> <h2 id=3-exposing-a-resource-data-on-the-server>3. Exposing a Resource (Data) on the Server<a class=headerlink href=#3-exposing-a-resource-data-on-the-server title="Permanent link">&para;</a></h2> <p>Let's extend our server to also provide a Resource. We'll make a resource that gives a friendly greeting. This will demonstrate how resources can supply text for the model's context. We use the <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.resource("uri_pattern") decorator to register a resource handler.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a><span class=c1># Define a resource. The URI pattern {name} means the client can request e.g. &quot;greet://Alice&quot;</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;greet://</span><span class=si>{name}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a><span class=k>def</span><span class=w> </span><span class=nf>get_greeting</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Resource content: returns a greeting for the given name.&quot;&quot;&quot;</span>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Hello, </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>! Welcome to MCP.&quot;</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>Here we used <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.resource("greet://{name}"). This tells the server that it can handle any resource URI of the form greet://<something> – the <something> will be passed as the name argument to our function get_greeting. So if the client later requests the resource greet://Alice, our function returns "Hello, Alice! Welcome to MCP.". The server will transmit that string as the content of the resource. We provided a description in the docstring as well.</p> <p>The client's perspective: if it calls resources/list, it might not list all possible greetings (since {name} is a parameterized resource), but some servers do list resource "directories" or examples. In any case, the client can directly do a resources/read on a specific URI. For example, an AI client could be prompted (via system message) that greet://{name} resources are available for use. If the user asks "Can you greet Alice?", the AI could fetch greet://Alice from the server (the client would call resources/read with uri: "greet://Alice"). The server invokes get_greeting("Alice") and returns the text. The LLM then sees that text and can respond to the user with it.</p> <p>Resources are a nice way to serve static or computed data that the model might include in its answer verbatim. They behave somewhat like HTTP GET endpoints (in REST analogy).</p> <p>Now our Demo Server has both a Tool (add) and a Resource (greet://{name}).</p> <h2 id=4-adding-a-prompt-reusable-template>4. Adding a Prompt (Reusable Template)<a class=headerlink href=#4-adding-a-prompt-reusable-template title="Permanent link">&para;</a></h2> <p>For completeness, let's add a Prompt to our server. Suppose we want a standardized way for the AI to request a calculation and explanation. We'll create a prompt that, given a math problem, instructs the model to solve it step-by-step.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp.prompts</span><span class=w> </span><span class=kn>import</span> <span class=n>base</span>  <span class=c1># utility for prompt message objects</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Demo Server&quot;</span><span class=p>)</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a><span class=k>def</span><span class=w> </span><span class=nf>add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Add two numbers and return the result.&quot;&quot;&quot;</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;greet://</span><span class=si>{name}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a><span class=k>def</span><span class=w> </span><span class=nf>get_greeting</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Resource content: returns a greeting for the given name.&quot;&quot;&quot;</span>
</span><span id=__span-6-14><a id=__codelineno-6-14 name=__codelineno-6-14 href=#__codelineno-6-14></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Hello, </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>! Welcome to MCP.&quot;</span>
</span><span id=__span-6-15><a id=__codelineno-6-15 name=__codelineno-6-15 href=#__codelineno-6-15></a>
</span><span id=__span-6-16><a id=__codelineno-6-16 name=__codelineno-6-16 href=#__codelineno-6-16></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>prompt</span><span class=p>()</span>
</span><span id=__span-6-17><a id=__codelineno-6-17 name=__codelineno-6-17 href=#__codelineno-6-17></a><span class=k>def</span><span class=w> </span><span class=nf>solve_math</span><span class=p>(</span><span class=n>problem</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=n>base</span><span class=o>.</span><span class=n>Message</span><span class=p>]:</span>
</span><span id=__span-6-18><a id=__codelineno-6-18 name=__codelineno-6-18 href=#__codelineno-6-18></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Prompt: guides the model to solve a math problem.&quot;&quot;&quot;</span>
</span><span id=__span-6-19><a id=__codelineno-6-19 name=__codelineno-6-19 href=#__codelineno-6-19></a>    <span class=c1># This returns a conversation (list of messages) as a template</span>
</span><span id=__span-6-20><a id=__codelineno-6-20 name=__codelineno-6-20 href=#__codelineno-6-20></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-6-21><a id=__codelineno-6-21 name=__codelineno-6-21 href=#__codelineno-6-21></a>        <span class=n>base</span><span class=o>.</span><span class=n>SystemMessage</span><span class=p>(</span><span class=s2>&quot;You are a helpful math assistant.&quot;</span><span class=p>),</span>
</span><span id=__span-6-22><a id=__codelineno-6-22 name=__codelineno-6-22 href=#__codelineno-6-22></a>        <span class=n>base</span><span class=o>.</span><span class=n>UserMessage</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Please solve the following problem: </span><span class=si>{</span><span class=n>problem</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>),</span>
</span><span id=__span-6-23><a id=__codelineno-6-23 name=__codelineno-6-23 href=#__codelineno-6-23></a>        <span class=n>base</span><span class=o>.</span><span class=n>AssistantMessage</span><span class=p>(</span><span class=s2>&quot;Sure, let me break it down...&quot;</span><span class=p>),</span>
</span><span id=__span-6-24><a id=__codelineno-6-24 name=__codelineno-6-24 href=#__codelineno-6-24></a>    <span class=p>]</span>
</span><span id=__span-6-25><a id=__codelineno-6-25 name=__codelineno-6-25 href=#__codelineno-6-25></a>
</span><span id=__span-6-26><a id=__codelineno-6-26 name=__codelineno-6-26 href=#__codelineno-6-26></a><span class=n>mcp</span><span class=o>.</span><span class=n>serve</span><span class=p>()</span>
</span></code></pre></div> <p>We used <a class="magiclink magiclink-github magiclink-mention" href=https://github.com/mcp title="GitHub User: mcp">@mcp</a>.prompt() to define solve_math. In this case, our function returns a list of message objects (using the SDK's base.SystemMessage, base.UserMessage, etc., which likely correspond to the roles in a chat prompt). This defines a prompt template where the assistant is primed with a system role and an initial user request, and even an initial assistant response to indicate it will explain step-by-step. The specifics aren't too important – the idea is that the server can package this multi-turn template and offer it as a named prompt.</p> <p>The client will see this if it calls prompts/list:</p> <div class="language-json highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=p>{</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=w>  </span><span class=nt>&quot;prompts&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=w>    </span><span class=p>{</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=w>      </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;solve_math&quot;</span><span class=p>,</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=w>      </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;Prompt: guides the model to solve a math problem.&quot;</span><span class=p>,</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a><span class=w>      </span><span class=nt>&quot;arguments&quot;</span><span class=p>:</span><span class=w> </span><span class=p>[</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class=w>        </span><span class=p>{</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=w>          </span><span class=nt>&quot;name&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;problem&quot;</span><span class=p>,</span>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a><span class=w>          </span><span class=nt>&quot;description&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;&quot;</span><span class=p>,</span>
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a><span class=w>          </span><span class=nt>&quot;required&quot;</span><span class=p>:</span><span class=w> </span><span class=kc>true</span>
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a><span class=w>        </span><span class=p>}</span>
</span><span id=__span-7-12><a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a><span class=w>      </span><span class=p>]</span>
</span><span id=__span-7-13><a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a><span class=w>    </span><span class=p>}</span>
</span><span id=__span-7-14><a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a><span class=w>  </span><span class=p>]</span>
</span><span id=__span-7-15><a id=__codelineno-7-15 name=__codelineno-7-15 href=#__codelineno-7-15></a><span class=p>}</span>
</span></code></pre></div> <p>The client (or user) can then invoke this prompt. For instance, a UI might show an option "Solve a math problem" which triggers prompts/execute (or a similar method) on the server with name: "solve_math" and problem: "2+2*5" as an argument. The server would return the prepared messages; the client would feed those to the LLM, and the LLM would continue the conversation from that context (producing a detailed solution). Essentially, the prompt primitive allows packaging expert instructions or workflows so the model can perform them on demand.</p> <p>We now have a server that demonstrates all three server-side primitives: - A Tool (add) - A Resource (greet://{name}) - A Prompt (solve_math)</p> <h2 id=5-running-the-server-and-client-interaction>5. Running the Server and Client Interaction<a class=headerlink href=#5-running-the-server-and-client-interaction title="Permanent link">&para;</a></h2> <p>To test our MCP server, we need an MCP client. In real-world usage, the "client" might be a GUI application (Claude Desktop) or an agent loop. Here, we can simulate a simple client or use the CLI. For brevity, let's illustrate using the CLI approach to interact with our server:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=c1># In one terminal, run the server (it will wait for client connection)</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>python<span class=w> </span>server.py
</span></code></pre></div> <p>Now, in another terminal, we can use the mcp CLI (installed with the SDK) to connect:</p> <div class="language-bash highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>mcp<span class=w> </span>dev<span class=w> </span>server.py
</span></code></pre></div> <p>This should connect to the running server (or launch it if not already) and drop us into an interactive MCP Inspector. We could then try: - Listing tools: this should show the add tool. - Calling a tool: e.g. call add with {"a":5,"b":7} and expect result 12. - Reading a resource: e.g. request resource URI greet://Alice and see the text. - Executing a prompt: e.g. run prompt solve_math with argument "12*3-4" and watch the assistant's step-by-step solution.</p> <p>If all goes well, the server will log or display calls as it handles them. The SDK likely prints logs for each request. You'd see that our Python functions are triggered accordingly.</p> <p>For a programmatic approach, one could also use the Python SDK's client classes to do the same. For example:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=c1># Using Python to connect via stdio (hypothetical example)</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.client</span><span class=w> </span><span class=kn>import</span> <span class=n>StdioClientTransport</span><span class=p>,</span> <span class=n>Client</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a>
</span><span id=__span-10-4><a id=__codelineno-10-4 name=__codelineno-10-4 href=#__codelineno-10-4></a><span class=c1># Start the server process and connect transport (pseudo-code)</span>
</span><span id=__span-10-5><a id=__codelineno-10-5 name=__codelineno-10-5 href=#__codelineno-10-5></a><span class=n>transport</span> <span class=o>=</span> <span class=n>StdioClientTransport</span><span class=p>(</span><span class=s2>&quot;./server.py&quot;</span><span class=p>)</span>  <span class=c1># launch our server script</span>
</span><span id=__span-10-6><a id=__codelineno-10-6 name=__codelineno-10-6 href=#__codelineno-10-6></a><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=p>(</span><span class=n>transport</span><span class=p>)</span>
</span><span id=__span-10-7><a id=__codelineno-10-7 name=__codelineno-10-7 href=#__codelineno-10-7></a>
</span><span id=__span-10-8><a id=__codelineno-10-8 name=__codelineno-10-8 href=#__codelineno-10-8></a><span class=c1># Initialize the MCP connection</span>
</span><span id=__span-10-9><a id=__codelineno-10-9 name=__codelineno-10-9 href=#__codelineno-10-9></a><span class=n>client</span><span class=o>.</span><span class=n>initialize</span><span class=p>()</span>  <span class=c1># sends initialize handshake</span>
</span><span id=__span-10-10><a id=__codelineno-10-10 name=__codelineno-10-10 href=#__codelineno-10-10></a>
</span><span id=__span-10-11><a id=__codelineno-10-11 name=__codelineno-10-11 href=#__codelineno-10-11></a><span class=c1># List and call the &#39;add&#39; tool</span>
</span><span id=__span-10-12><a id=__codelineno-10-12 name=__codelineno-10-12 href=#__codelineno-10-12></a><span class=n>tools</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&quot;tools/list&quot;</span><span class=p>,</span> <span class=p>{})</span>
</span><span id=__span-10-13><a id=__codelineno-10-13 name=__codelineno-10-13 href=#__codelineno-10-13></a><span class=nb>print</span><span class=p>(</span><span class=n>tools</span><span class=p>)</span>  <span class=c1># should include &#39;add&#39;</span>
</span><span id=__span-10-14><a id=__codelineno-10-14 name=__codelineno-10-14 href=#__codelineno-10-14></a><span class=n>result</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>request</span><span class=p>(</span><span class=s2>&quot;tools/call&quot;</span><span class=p>,</span> <span class=p>{</span><span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=s2>&quot;add&quot;</span><span class=p>,</span> <span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&quot;a&quot;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&quot;b&quot;</span><span class=p>:</span> <span class=mi>3</span><span class=p>}})</span>
</span><span id=__span-10-15><a id=__codelineno-10-15 name=__codelineno-10-15 href=#__codelineno-10-15></a><span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>)</span>  <span class=c1># should output {&#39;result&#39;: 5}</span>
</span><span id=__span-10-16><a id=__codelineno-10-16 name=__codelineno-10-16 href=#__codelineno-10-16></a>
</span><span id=__span-10-17><a id=__codelineno-10-17 name=__codelineno-10-17 href=#__codelineno-10-17></a><span class=n>client</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></code></pre></div> <p>The actual API may differ, but the concept is that the client sends JSON-RPC requests like {"method": "tools/call", "params": {"tool": "add", "params": {"a":2,"b":3}}} and receives a response {"result": 5}. The SDK's Client class would wrap this in a nicer interface.</p> <p>Through this step-by-step exercise, we saw how to implement an MCP server in Python with minimal effort (just defining functions with decorators) and how a client might interact. In a real deployment, you might build much more complex servers – connecting to real databases, handling authentication, streaming large resources, etc. – but the pattern remains the same. Anthropic's SDK and spec provides a lot of guidance and examples for building robust servers (including lifecycle management, dependency injection, error handling, etc., as shown in their docs).</p> <h1 id=advanced-use-cases-and-workflows>Advanced Use Cases and Workflows<a class=headerlink href=#advanced-use-cases-and-workflows title="Permanent link">&para;</a></h1> <p>With the fundamentals in place, let's discuss some advanced use cases of MCP and how it fits into larger AI systems:</p> <h2 id=handling-complex-state-and-workflows>Handling Complex State and Workflows<a class=headerlink href=#handling-complex-state-and-workflows title="Permanent link">&para;</a></h2> <p>One powerful aspect of MCP's design is that a server can maintain and manage complex state over the course of a session. For example, consider an MCP server that interfaces with a user's email. Such a server might need to handle authentication, caching of email headers, incremental fetching of emails, etc.</p> <p>Using MCP, the server could manage an OAuth login flow (especially with the new OAuth 2.1 support in the protocol for secure client-server auth), store the user's token, and reuse it for subsequent requests – all internally. The AI client just calls high-level tools like list_emails or send_email, and the server handles the gritty details with its state.</p> <p>Another scenario is maintaining conversational or task state. Suppose you have an agent working on a coding task using multiple tools: a Git server, a documentation server, etc. The agent might ask the Git server to open a file (as a resource), edit it (via a tool), then ask for a diff. The Git MCP server could keep track of an open repo and the last opened file across those calls, rather than requiring the AI to specify everything each time.</p> <p>This is facilitated by MCP's session – since the connection is persistent, the server can hold onto objects in memory (like a database connection or a file handle) between calls. In our earlier Python example, the SDK even showed a concept of a lifespan context where you can initialize resources at server startup (e.g. connect to a DB) and clean up on shutdown. That means complex state (like a database pool or an in-memory cache) can be established once and reused, improving efficiency and making the server more capable.</p> <p>Chaining and Multi-step workflows: Using prompts and sampling, servers can also coordinate multi-step processes. For instance, a single MCP prompt might involve the server first fetching some data (resource), then formulating a series of messages (prompt template), possibly even calling an internal model (sampling) to pre-analyze something, and finally returning a polished prompt for the client's LLM to answer. This is a form of agent orchestration that can be hidden behind a single MCP call. Essentially, the server can implement a mini-agent or state machine, but expose it as a simple interface to the client. The separation of concerns here is neat: the server handles procedure and state, the client's LLM handles language generation.</p> <h2 id=integration-with-agent-frameworks>Integration with Agent Frameworks<a class=headerlink href=#integration-with-agent-frameworks title="Permanent link">&para;</a></h2> <p>MCP is quickly being adopted by AI agent frameworks and libraries, which is a testament to its flexibility. One notable integration is with LangChain, a popular framework for chaining LLM calls and tools. The LangChain team and community have built adapters to use MCP servers as LangChain tools.</p> <p>For example, rather than writing a custom tool wrapper for every API in LangChain, you can now spin up an MCP server (or use an existing one) and then use a converter that turns all of that server's Tools into LangChain Tool objects. This means LangChain-powered agents can immediately gain access to the huge library of MCP connectors (there are MCP servers for many services).</p> <p>Conversely, tools defined within LangChain could potentially be exposed via an MCP server to other systems. Beyond LangChain, other agent frameworks and IDE assistants are on board. The "fast-agent" project, for instance, is an MCP client that supports all features including Sampling and can coordinate multi-modal inputs.</p> <p>Developer tools like Cursor editor and Continue (VS Code extension) have added MCP support so that code assistants can use external tools uniformly. Even Microsoft's Guidance / Autogen frameworks and OpenAI's new Agents SDK are aligning with these standards.</p> <p>The fact that OpenAI itself announced MCP support in its Agents SDK shows that even proprietary agent ecosystems see value in interoperability. What does this mean for a developer? If you're building an AI agent, you could use MCP as the backbone for tool usage. Instead of hardcoding how to call each API, you let the agent discover available MCP servers and use them. The agent can remain focused on planning and high-level reasoning, delegating actual tool execution to MCP servers.</p> <p>This leads to more modular agent design – you can mix and match capabilities by just running different servers. Want to add calendar access to your agent? Just run a Calendar MCP server and connect it; no need to retrain the model or rewrite logic, because the model will see the new "calendar tool" via MCP and can start using it (assuming it's been instructed appropriately). This plug-and-play nature fits nicely with the vision of agents that can expand their toolsets dynamically.</p> <h2 id=retrieval-augmented-generation-rag-with-mcp>Retrieval-Augmented Generation (RAG) with MCP<a class=headerlink href=#retrieval-augmented-generation-rag-with-mcp title="Permanent link">&para;</a></h2> <p>Retrieval-Augmented Generation refers to the workflow where an LLM fetches relevant external information (from a document corpus or knowledge base) to augment its responses. MCP is an excellent vehicle for implementing RAG in a standardized way. Instead of custom retrieval code, one can create an MCP server that encapsulates the retrieval logic.</p> <p>For example, imagine you have a vector database of documents or an ElasticSearch index. You could build an MCP server KnowledgeBaseServer with a tool search(query: str) -&gt; list[str] (or which returns resource URIs). The AI model, when asked a question, can call this search tool via MCP to get relevant text snippets (as either direct text results or as resource references that it then reads). Those snippets then become part of the context for answering the question. Because MCP can handle streaming and large data via resources, even bigger documents can be fetched in chunks as needed.</p> <p>The advantage is consistency and reuse: any MCP-aware AI app could use that same server for retrieval. In fact, Anthropic's documentation includes a quickstart where Claude uses an MCP server to fetch weather info – which is essentially a form of retrieval (getting factual data) before answering. The pattern extends to any knowledge domain.</p> <p>Let's sketch an advanced RAG flow with MCP in a sequence diagram for clarity:</p> <pre class=mermaid><code>sequenceDiagram
    participant User
    participant AI_Client as AI (MCP Client)
    participant KB_Server as KnowledgeBase MCP Server
    participant DataStore as Document DB / Index
    User-&gt;&gt;AI_Client: "What were the key points of Project X's design?"
    AI_Client-&gt;&gt;KB_Server: tools/call: search({"query": "Project X design key points"})
    KB_Server-&gt;&gt;DataStore: Perform semantic search for "Project X design key points"
    DataStore--&gt;&gt;KB_Server: Returns top relevant doc snippets
    Note over KB_Server: e.g. returns a Resource list or text results
    KB_Server--&gt;&gt;AI_Client: Search results (as resources or text)
    AI_Client-&gt;&gt;AI_Client: (LLM incorporates results into its prompt)
    AI_Client--&gt;&gt;User: "The key design points of Project X are ... (with details from docs)"</code></pre> <p>In this RAG scenario, the heavy lifting of retrieval is done by the KB_Server. The AI client just knows that when it needs info, it can call the search tool. This decoupling means you could swap out the implementation (maybe use a different database or algorithm) by switching the server, without changing the AI's approach. Moreover, because MCP is model-agnostic, even if you used a different LLM tomorrow, it could perform the same retrieval process by using the server.</p> <p>MCP also helps maintain conversation flow in RAG. Traditional retrieval might not keep state of what's already fetched, but an MCP server could track which documents have been shown to the model (to avoid repetition or to follow up with more details as needed). It could also expose each document as a Resource with a URI, so that the model can say "let me read resource doc://123" in parts. All of this can be done with a clear protocol, rather than custom ad-hoc methods.</p> <h3 id=rag-implementation-strategies-with-mcp>RAG Implementation Strategies with MCP<a class=headerlink href=#rag-implementation-strategies-with-mcp title="Permanent link">&para;</a></h3> <p>There are several approaches to implementing RAG with MCP, each with its own advantages:</p> <h4 id=1-tool-based-retrieval>1. Tool-based Retrieval<a class=headerlink href=#1-tool-based-retrieval title="Permanent link">&para;</a></h4> <p>The simplest approach is to expose a search tool that returns relevant content directly. The model asks a question, calls the search tool with appropriate keywords, and gets back content in the tool response:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=k>def</span><span class=w> </span><span class=nf>search_knowledge_base</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>max_results</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>dict</span><span class=p>]:</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Search the knowledge base for documents matching the query.&quot;&quot;&quot;</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>    <span class=c1># Search logic using vector DB, keyword search, etc.</span>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>    <span class=c1># Each result has the document content and metadata</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a>        <span class=p>{</span><span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=s2>&quot;...&quot;</span><span class=p>,</span> <span class=s2>&quot;source&quot;</span><span class=p>:</span> <span class=s2>&quot;doc1.pdf&quot;</span><span class=p>,</span> <span class=s2>&quot;relevance&quot;</span><span class=p>:</span> <span class=mf>0.92</span><span class=p>},</span>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a>        <span class=p>{</span><span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=s2>&quot;...&quot;</span><span class=p>,</span> <span class=s2>&quot;source&quot;</span><span class=p>:</span> <span class=s2>&quot;doc2.pdf&quot;</span><span class=p>,</span> <span class=s2>&quot;relevance&quot;</span><span class=p>:</span> <span class=mf>0.87</span><span class=p>},</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a>        <span class=c1># ...</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a>    <span class=p>]</span>
</span></code></pre></div> <p>This approach is straightforward but may have limitations with token context windows if the search returns large amounts of text.</p> <h4 id=2-resource-based-retrieval>2. Resource-based Retrieval<a class=headerlink href=#2-resource-based-retrieval title="Permanent link">&para;</a></h4> <p>A more flexible approach is to expose documents as resources, so the model can request them by URI:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a><span class=k>def</span><span class=w> </span><span class=nf>search_documents</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>max_results</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>5</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Search documents and return resource URIs of relevant documents.&quot;&quot;&quot;</span>
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>    <span class=c1># Search logic using vector DB, etc.</span>
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>    <span class=c1># Return URIs instead of content</span>
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>    <span class=k>return</span> <span class=p>[</span><span class=s2>&quot;doc://123&quot;</span><span class=p>,</span> <span class=s2>&quot;doc://456&quot;</span><span class=p>,</span> <span class=s2>&quot;doc://789&quot;</span><span class=p>]</span>
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;doc://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a><span class=k>def</span><span class=w> </span><span class=nf>get_document</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get the content of a document by ID.&quot;&quot;&quot;</span>
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>    <span class=c1># Retrieve document with the given ID</span>
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>    <span class=k>return</span> <span class=s2>&quot;Document content goes here...&quot;</span>
</span></code></pre></div> <p>This approach lets the model decide which documents to actually fetch after seeing search results, and it can request document content in chunks if needed.</p> <h4 id=3-hybrid-chunking-and-streaming>3. Hybrid: Chunking and Streaming<a class=headerlink href=#3-hybrid-chunking-and-streaming title="Permanent link">&para;</a></h4> <p>For very large documents, an MCP server can implement chunking and pagination:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;doc://</span><span class=si>{id}</span><span class=s2>/chunk/</span><span class=si>{chunk_number}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=k>def</span><span class=w> </span><span class=nf>get_document_chunk</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>chunk_number</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get a specific chunk of a document.&quot;&quot;&quot;</span>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a>    <span class=c1># Logic to fetch and return only that chunk</span>
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&quot;Content of chunk </span><span class=si>{</span><span class=n>chunk_number</span><span class=si>}</span><span class=s2> from doc </span><span class=si>{</span><span class=nb>id</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a>
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a><span class=k>def</span><span class=w> </span><span class=nf>get_document_metadata</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Get metadata about a document, including total chunks.&quot;&quot;&quot;</span>
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a>        <span class=s2>&quot;title&quot;</span><span class=p>:</span> <span class=s2>&quot;Example Document&quot;</span><span class=p>,</span>
</span><span id=__span-13-12><a id=__codelineno-13-12 name=__codelineno-13-12 href=#__codelineno-13-12></a>        <span class=s2>&quot;total_chunks&quot;</span><span class=p>:</span> <span class=mi>15</span><span class=p>,</span>
</span><span id=__span-13-13><a id=__codelineno-13-13 name=__codelineno-13-13 href=#__codelineno-13-13></a>        <span class=s2>&quot;summary&quot;</span><span class=p>:</span> <span class=s2>&quot;A brief overview of the document...&quot;</span>
</span><span id=__span-13-14><a id=__codelineno-13-14 name=__codelineno-13-14 href=#__codelineno-13-14></a>    <span class=p>}</span>
</span></code></pre></div> <p>This allows the model to navigate large documents efficiently, requesting only the most relevant portions.</p> <h4 id=4-semantic-routing>4. Semantic Routing<a class=headerlink href=#4-semantic-routing title="Permanent link">&para;</a></h4> <p>An advanced MCP server could even implement semantic routing of queries to the right knowledge source:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=k>def</span><span class=w> </span><span class=nf>route_query</span><span class=p>(</span><span class=n>query</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Route a query to the appropriate knowledge source.&quot;&quot;&quot;</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>    <span class=c1># Analyze the query and determine which source to use</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>    <span class=k>if</span> <span class=s2>&quot;financial&quot;</span> <span class=ow>in</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>():</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a>            <span class=s2>&quot;recommended_source&quot;</span><span class=p>:</span> <span class=s2>&quot;financial_db&quot;</span><span class=p>,</span>
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a>            <span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=n>query</span><span class=p>,</span>
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a>            <span class=s2>&quot;confidence&quot;</span><span class=p>:</span> <span class=mf>0.85</span>
</span><span id=__span-14-10><a id=__codelineno-14-10 name=__codelineno-14-10 href=#__codelineno-14-10></a>        <span class=p>}</span>
</span><span id=__span-14-11><a id=__codelineno-14-11 name=__codelineno-14-11 href=#__codelineno-14-11></a>    <span class=k>elif</span> <span class=s2>&quot;technical&quot;</span> <span class=ow>in</span> <span class=n>query</span><span class=o>.</span><span class=n>lower</span><span class=p>():</span>
</span><span id=__span-14-12><a id=__codelineno-14-12 name=__codelineno-14-12 href=#__codelineno-14-12></a>        <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-14-13><a id=__codelineno-14-13 name=__codelineno-14-13 href=#__codelineno-14-13></a>            <span class=s2>&quot;recommended_source&quot;</span><span class=p>:</span> <span class=s2>&quot;technical_docs&quot;</span><span class=p>,</span>
</span><span id=__span-14-14><a id=__codelineno-14-14 name=__codelineno-14-14 href=#__codelineno-14-14></a>            <span class=s2>&quot;query&quot;</span><span class=p>:</span> <span class=n>query</span><span class=p>,</span>
</span><span id=__span-14-15><a id=__codelineno-14-15 name=__codelineno-14-15 href=#__codelineno-14-15></a>            <span class=s2>&quot;confidence&quot;</span><span class=p>:</span> <span class=mf>0.92</span>
</span><span id=__span-14-16><a id=__codelineno-14-16 name=__codelineno-14-16 href=#__codelineno-14-16></a>        <span class=p>}</span>
</span><span id=__span-14-17><a id=__codelineno-14-17 name=__codelineno-14-17 href=#__codelineno-14-17></a>    <span class=c1># ... other routing logic</span>
</span></code></pre></div> <p>This helps the model choose the right information source before retrieval.</p> <p>Each of these patterns can be mixed and matched to create sophisticated RAG systems, all communicating through the standardized MCP interface.</p> <h2 id=beyond-text-multi-modal-and-other-extensions>Beyond Text – Multi-Modal and Other Extensions<a class=headerlink href=#beyond-text-multi-modal-and-other-extensions title="Permanent link">&para;</a></h2> <p>While our focus has been on text-based LLMs, MCP is not limited to text. The protocol can carry binary data (resources can be images or audio clips, with appropriate encoding). The Sampling primitive even has a provision for image generation or interpretation by specifying the content type (text or image) in the request.</p> <p>This means one could have an MCP server for, say, an image database, where images are exposed as resources (with mimeType) and the model could request them, or even an OCR tool as a Tool that returns text from an image. The modular design of MCP allows layering new capabilities.</p> <p>For example, Microsoft's recent contribution of a Playwright MCP server (for web browsing automation) shows how MCP can enable completely new tool modes: that server exposes browser actions like clicking, typing, and navigation as MCP tools. An AI agent using that server can browse the web like a user, but through standardized calls (browser_navigate, browser_click, etc.).</p> <p>This is far more complex than a basic function call, yet MCP handles it smoothly by formalizing those actions as tools with JSON schemas. The server even streams back page content as resources for the AI to read. This kind of advanced use case – essentially turning the AI into an automated agent in a real environment – is made feasible by MCP's architecture. And because it's standardized, improvements like better metadata (tool annotations, as added in MCP's update) benefit all tools across the board.</p> <h3 id=multi-modal-server-examples>Multi-Modal Server Examples<a class=headerlink href=#multi-modal-server-examples title="Permanent link">&para;</a></h3> <p>Here are some examples of what multi-modal MCP servers might look like:</p> <h4 id=1-image-processing-server>1. Image Processing Server<a class=headerlink href=#1-image-processing-server title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a><span class=kn>import</span><span class=w> </span><span class=nn>base64</span>
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a><span class=kn>from</span><span class=w> </span><span class=nn>PIL</span><span class=w> </span><span class=kn>import</span> <span class=n>Image</span>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a><span class=kn>import</span><span class=w> </span><span class=nn>io</span>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Image Processing Server&quot;</span><span class=p>)</span>
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;image://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a><span class=k>def</span><span class=w> </span><span class=nf>get_image</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bytes</span><span class=p>:</span>
</span><span id=__span-15-10><a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return image data as bytes with appropriate MIME type.&quot;&quot;&quot;</span>
</span><span id=__span-15-11><a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a>    <span class=c1># Load image from storage</span>
</span><span id=__span-15-12><a id=__codelineno-15-12 name=__codelineno-15-12 href=#__codelineno-15-12></a>    <span class=n>image_bytes</span> <span class=o>=</span> <span class=n>load_image_from_storage</span><span class=p>(</span><span class=nb>id</span><span class=p>)</span>
</span><span id=__span-15-13><a id=__codelineno-15-13 name=__codelineno-15-13 href=#__codelineno-15-13></a>    <span class=k>return</span> <span class=n>image_bytes</span>  <span class=c1># MCP can handle binary data with proper MIME type</span>
</span><span id=__span-15-14><a id=__codelineno-15-14 name=__codelineno-15-14 href=#__codelineno-15-14></a>
</span><span id=__span-15-15><a id=__codelineno-15-15 name=__codelineno-15-15 href=#__codelineno-15-15></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-15-16><a id=__codelineno-15-16 name=__codelineno-15-16 href=#__codelineno-15-16></a><span class=k>def</span><span class=w> </span><span class=nf>analyze_image</span><span class=p>(</span><span class=n>image_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-15-17><a id=__codelineno-15-17 name=__codelineno-15-17 href=#__codelineno-15-17></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Analyze an image and return metadata and detected objects.&quot;&quot;&quot;</span>
</span><span id=__span-15-18><a id=__codelineno-15-18 name=__codelineno-15-18 href=#__codelineno-15-18></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-15-19><a id=__codelineno-15-19 name=__codelineno-15-19 href=#__codelineno-15-19></a>    <span class=n>image_id</span> <span class=o>=</span> <span class=n>image_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;image://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-15-20><a id=__codelineno-15-20 name=__codelineno-15-20 href=#__codelineno-15-20></a>
</span><span id=__span-15-21><a id=__codelineno-15-21 name=__codelineno-15-21 href=#__codelineno-15-21></a>    <span class=c1># Get image data</span>
</span><span id=__span-15-22><a id=__codelineno-15-22 name=__codelineno-15-22 href=#__codelineno-15-22></a>    <span class=n>image_data</span> <span class=o>=</span> <span class=n>load_image_from_storage</span><span class=p>(</span><span class=n>image_id</span><span class=p>)</span>
</span><span id=__span-15-23><a id=__codelineno-15-23 name=__codelineno-15-23 href=#__codelineno-15-23></a>
</span><span id=__span-15-24><a id=__codelineno-15-24 name=__codelineno-15-24 href=#__codelineno-15-24></a>    <span class=c1># Run image analysis (just mock results for this example)</span>
</span><span id=__span-15-25><a id=__codelineno-15-25 name=__codelineno-15-25 href=#__codelineno-15-25></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-15-26><a id=__codelineno-15-26 name=__codelineno-15-26 href=#__codelineno-15-26></a>        <span class=s2>&quot;resolution&quot;</span><span class=p>:</span> <span class=s2>&quot;1920x1080&quot;</span><span class=p>,</span>
</span><span id=__span-15-27><a id=__codelineno-15-27 name=__codelineno-15-27 href=#__codelineno-15-27></a>        <span class=s2>&quot;detected_objects&quot;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&quot;person&quot;</span><span class=p>,</span> <span class=s2>&quot;car&quot;</span><span class=p>,</span> <span class=s2>&quot;tree&quot;</span><span class=p>],</span>
</span><span id=__span-15-28><a id=__codelineno-15-28 name=__codelineno-15-28 href=#__codelineno-15-28></a>        <span class=s2>&quot;dominant_colors&quot;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&quot;blue&quot;</span><span class=p>,</span> <span class=s2>&quot;green&quot;</span><span class=p>],</span>
</span><span id=__span-15-29><a id=__codelineno-15-29 name=__codelineno-15-29 href=#__codelineno-15-29></a>        <span class=s2>&quot;estimated_style&quot;</span><span class=p>:</span> <span class=s2>&quot;photograph&quot;</span>
</span><span id=__span-15-30><a id=__codelineno-15-30 name=__codelineno-15-30 href=#__codelineno-15-30></a>    <span class=p>}</span>
</span><span id=__span-15-31><a id=__codelineno-15-31 name=__codelineno-15-31 href=#__codelineno-15-31></a>
</span><span id=__span-15-32><a id=__codelineno-15-32 name=__codelineno-15-32 href=#__codelineno-15-32></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-15-33><a id=__codelineno-15-33 name=__codelineno-15-33 href=#__codelineno-15-33></a><span class=k>def</span><span class=w> </span><span class=nf>ocr_image</span><span class=p>(</span><span class=n>image_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-15-34><a id=__codelineno-15-34 name=__codelineno-15-34 href=#__codelineno-15-34></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Extract text from an image using OCR.&quot;&quot;&quot;</span>
</span><span id=__span-15-35><a id=__codelineno-15-35 name=__codelineno-15-35 href=#__codelineno-15-35></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-15-36><a id=__codelineno-15-36 name=__codelineno-15-36 href=#__codelineno-15-36></a>    <span class=n>image_id</span> <span class=o>=</span> <span class=n>image_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;image://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-15-37><a id=__codelineno-15-37 name=__codelineno-15-37 href=#__codelineno-15-37></a>
</span><span id=__span-15-38><a id=__codelineno-15-38 name=__codelineno-15-38 href=#__codelineno-15-38></a>    <span class=c1># Perform OCR on the image</span>
</span><span id=__span-15-39><a id=__codelineno-15-39 name=__codelineno-15-39 href=#__codelineno-15-39></a>    <span class=c1># (in a real implementation, would use an OCR library)</span>
</span><span id=__span-15-40><a id=__codelineno-15-40 name=__codelineno-15-40 href=#__codelineno-15-40></a>    <span class=k>return</span> <span class=s2>&quot;Text extracted from the image would appear here.&quot;</span>
</span></code></pre></div> <h4 id=2-audio-processing-server>2. Audio Processing Server<a class=headerlink href=#2-audio-processing-server title="Permanent link">&para;</a></h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>resource</span><span class=p>(</span><span class=s2>&quot;audio://</span><span class=si>{id}</span><span class=s2>&quot;</span><span class=p>)</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=k>def</span><span class=w> </span><span class=nf>get_audio</span><span class=p>(</span><span class=nb>id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>bytes</span><span class=p>:</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Return audio data as bytes with appropriate MIME type.&quot;&quot;&quot;</span>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a>    <span class=c1># Load audio from storage</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>    <span class=n>audio_bytes</span> <span class=o>=</span> <span class=n>load_audio_from_storage</span><span class=p>(</span><span class=nb>id</span><span class=p>)</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>    <span class=k>return</span> <span class=n>audio_bytes</span>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-16-9><a id=__codelineno-16-9 name=__codelineno-16-9 href=#__codelineno-16-9></a><span class=k>def</span><span class=w> </span><span class=nf>transcribe_audio</span><span class=p>(</span><span class=n>audio_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span><span id=__span-16-10><a id=__codelineno-16-10 name=__codelineno-16-10 href=#__codelineno-16-10></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Transcribe speech in audio to text.&quot;&quot;&quot;</span>
</span><span id=__span-16-11><a id=__codelineno-16-11 name=__codelineno-16-11 href=#__codelineno-16-11></a>    <span class=c1># Extract ID from URI</span>
</span><span id=__span-16-12><a id=__codelineno-16-12 name=__codelineno-16-12 href=#__codelineno-16-12></a>    <span class=n>audio_id</span> <span class=o>=</span> <span class=n>audio_uri</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s2>&quot;audio://&quot;</span><span class=p>,</span> <span class=s2>&quot;&quot;</span><span class=p>)</span>
</span><span id=__span-16-13><a id=__codelineno-16-13 name=__codelineno-16-13 href=#__codelineno-16-13></a>
</span><span id=__span-16-14><a id=__codelineno-16-14 name=__codelineno-16-14 href=#__codelineno-16-14></a>    <span class=c1># Get audio data and transcribe</span>
</span><span id=__span-16-15><a id=__codelineno-16-15 name=__codelineno-16-15 href=#__codelineno-16-15></a>    <span class=c1># (in a real implementation, would use a speech-to-text service)</span>
</span><span id=__span-16-16><a id=__codelineno-16-16 name=__codelineno-16-16 href=#__codelineno-16-16></a>    <span class=k>return</span> <span class=s2>&quot;Transcription of the audio would appear here.&quot;</span>
</span><span id=__span-16-17><a id=__codelineno-16-17 name=__codelineno-16-17 href=#__codelineno-16-17></a>
</span><span id=__span-16-18><a id=__codelineno-16-18 name=__codelineno-16-18 href=#__codelineno-16-18></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-16-19><a id=__codelineno-16-19 name=__codelineno-16-19 href=#__codelineno-16-19></a><span class=k>def</span><span class=w> </span><span class=nf>analyze_audio</span><span class=p>(</span><span class=n>audio_uri</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-16-20><a id=__codelineno-16-20 name=__codelineno-16-20 href=#__codelineno-16-20></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Analyze audio properties and content.&quot;&quot;&quot;</span>
</span><span id=__span-16-21><a id=__codelineno-16-21 name=__codelineno-16-21 href=#__codelineno-16-21></a>    <span class=c1># Implementation details...</span>
</span><span id=__span-16-22><a id=__codelineno-16-22 name=__codelineno-16-22 href=#__codelineno-16-22></a>    <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-16-23><a id=__codelineno-16-23 name=__codelineno-16-23 href=#__codelineno-16-23></a>        <span class=s2>&quot;duration_seconds&quot;</span><span class=p>:</span> <span class=mi>120</span><span class=p>,</span>
</span><span id=__span-16-24><a id=__codelineno-16-24 name=__codelineno-16-24 href=#__codelineno-16-24></a>        <span class=s2>&quot;language_detected&quot;</span><span class=p>:</span> <span class=s2>&quot;English&quot;</span><span class=p>,</span>
</span><span id=__span-16-25><a id=__codelineno-16-25 name=__codelineno-16-25 href=#__codelineno-16-25></a>        <span class=s2>&quot;speakers_detected&quot;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span><span id=__span-16-26><a id=__codelineno-16-26 name=__codelineno-16-26 href=#__codelineno-16-26></a>        <span class=s2>&quot;music_detected&quot;</span><span class=p>:</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-16-27><a id=__codelineno-16-27 name=__codelineno-16-27 href=#__codelineno-16-27></a>        <span class=s2>&quot;audio_quality&quot;</span><span class=p>:</span> <span class=s2>&quot;High&quot;</span>
</span><span id=__span-16-28><a id=__codelineno-16-28 name=__codelineno-16-28 href=#__codelineno-16-28></a>    <span class=p>}</span>
</span></code></pre></div> <p>These examples show how MCP can handle different types of media while maintaining the same protocol structure. The AI can request media as resources and then analyze them using tools, all through the standard MCP interface.</p> <h2 id=workflow-orchestration-and-human-oversight>Workflow Orchestration and Human Oversight<a class=headerlink href=#workflow-orchestration-and-human-oversight title="Permanent link">&para;</a></h2> <p>Finally, it's worth noting how MCP can fit into human-in-the-loop workflows. Because MCP clients (hosts) can intercept every tool invocation or resource request, a developer can implement policies like "ask user for permission before executing destructive tools" or log all data access for auditing. This is critical in enterprise settings.</p> <p>For instance, if an AI agent tries to call a "delete_record" tool on a database server, the MCP client could pop up a confirmation to the user. Or if a sampling request is made by a server (i.e., the server wants the AI to do something), the client can require a user click before letting it proceed. These controls ensure that even as we give AI agents more power via protocols like MCP, we maintain safety and governance.</p> <p>Moreover, MCP's open nature encourages community-driven best practices. Early adopters like Block (Square) highlighted that open protocols enable transparency and collaboration on making AI more helpful and less mechanical. We're likely to see shared schemas for common tasks, security frameworks, and audit tools develop around MCP as it matures.</p> <h3 id=implementing-human-in-the-loop-controls>Implementing Human-in-the-Loop Controls<a class=headerlink href=#implementing-human-in-the-loop-controls title="Permanent link">&para;</a></h3> <p>Let's look at how a client might implement human approval for sensitive operations:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.client</span><span class=w> </span><span class=kn>import</span> <span class=n>MCPClient</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a><span class=k>class</span><span class=w> </span><span class=nc>HumanOversightClient</span><span class=p>(</span><span class=n>MCPClient</span><span class=p>):</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>transport</span><span class=p>,</span> <span class=n>ui_interface</span><span class=p>):</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>transport</span><span class=p>)</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>ui</span> <span class=o>=</span> <span class=n>ui_interface</span>  <span class=c1># Interface to show prompts to the user</span>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a>        <span class=c1># Define which tools require explicit approval</span>
</span><span id=__span-17-9><a id=__codelineno-17-9 name=__codelineno-17-9 href=#__codelineno-17-9></a>        <span class=bp>self</span><span class=o>.</span><span class=n>sensitive_tools</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-17-10><a id=__codelineno-17-10 name=__codelineno-17-10 href=#__codelineno-17-10></a>            <span class=s2>&quot;delete_record&quot;</span><span class=p>,</span>
</span><span id=__span-17-11><a id=__codelineno-17-11 name=__codelineno-17-11 href=#__codelineno-17-11></a>            <span class=s2>&quot;send_email&quot;</span><span class=p>,</span>
</span><span id=__span-17-12><a id=__codelineno-17-12 name=__codelineno-17-12 href=#__codelineno-17-12></a>            <span class=s2>&quot;execute_transaction&quot;</span><span class=p>,</span>
</span><span id=__span-17-13><a id=__codelineno-17-13 name=__codelineno-17-13 href=#__codelineno-17-13></a>            <span class=s2>&quot;modify_permissions&quot;</span>
</span><span id=__span-17-14><a id=__codelineno-17-14 name=__codelineno-17-14 href=#__codelineno-17-14></a>        <span class=p>]</span>
</span><span id=__span-17-15><a id=__codelineno-17-15 name=__codelineno-17-15 href=#__codelineno-17-15></a>
</span><span id=__span-17-16><a id=__codelineno-17-16 name=__codelineno-17-16 href=#__codelineno-17-16></a>        <span class=c1># Track usage and approvals</span>
</span><span id=__span-17-17><a id=__codelineno-17-17 name=__codelineno-17-17 href=#__codelineno-17-17></a>        <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-17-18><a id=__codelineno-17-18 name=__codelineno-17-18 href=#__codelineno-17-18></a>
</span><span id=__span-17-19><a id=__codelineno-17-19 name=__codelineno-17-19 href=#__codelineno-17-19></a>    <span class=k>async</span> <span class=k>def</span><span class=w> </span><span class=nf>call_tool</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>server_id</span><span class=p>,</span> <span class=n>tool_name</span><span class=p>,</span> <span class=n>params</span><span class=p>):</span>
</span><span id=__span-17-20><a id=__codelineno-17-20 name=__codelineno-17-20 href=#__codelineno-17-20></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;Override to add human approval for sensitive tools.&quot;&quot;&quot;</span>
</span><span id=__span-17-21><a id=__codelineno-17-21 name=__codelineno-17-21 href=#__codelineno-17-21></a>        <span class=c1># Log the tool call attempt</span>
</span><span id=__span-17-22><a id=__codelineno-17-22 name=__codelineno-17-22 href=#__codelineno-17-22></a>        <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span><span id=__span-17-23><a id=__codelineno-17-23 name=__codelineno-17-23 href=#__codelineno-17-23></a>            <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>(),</span>
</span><span id=__span-17-24><a id=__codelineno-17-24 name=__codelineno-17-24 href=#__codelineno-17-24></a>            <span class=s2>&quot;action&quot;</span><span class=p>:</span> <span class=s2>&quot;tool_call_attempt&quot;</span><span class=p>,</span>
</span><span id=__span-17-25><a id=__codelineno-17-25 name=__codelineno-17-25 href=#__codelineno-17-25></a>            <span class=s2>&quot;server&quot;</span><span class=p>:</span> <span class=n>server_id</span><span class=p>,</span>
</span><span id=__span-17-26><a id=__codelineno-17-26 name=__codelineno-17-26 href=#__codelineno-17-26></a>            <span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=n>tool_name</span><span class=p>,</span>
</span><span id=__span-17-27><a id=__codelineno-17-27 name=__codelineno-17-27 href=#__codelineno-17-27></a>            <span class=s2>&quot;params&quot;</span><span class=p>:</span> <span class=n>params</span>
</span><span id=__span-17-28><a id=__codelineno-17-28 name=__codelineno-17-28 href=#__codelineno-17-28></a>        <span class=p>})</span>
</span><span id=__span-17-29><a id=__codelineno-17-29 name=__codelineno-17-29 href=#__codelineno-17-29></a>
</span><span id=__span-17-30><a id=__codelineno-17-30 name=__codelineno-17-30 href=#__codelineno-17-30></a>        <span class=c1># Check if this tool requires approval</span>
</span><span id=__span-17-31><a id=__codelineno-17-31 name=__codelineno-17-31 href=#__codelineno-17-31></a>        <span class=k>if</span> <span class=n>tool_name</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>sensitive_tools</span><span class=p>:</span>
</span><span id=__span-17-32><a id=__codelineno-17-32 name=__codelineno-17-32 href=#__codelineno-17-32></a>            <span class=c1># Format the request for human review</span>
</span><span id=__span-17-33><a id=__codelineno-17-33 name=__codelineno-17-33 href=#__codelineno-17-33></a>            <span class=n>approval_text</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;The AI wants to use tool &#39;</span><span class=si>{</span><span class=n>tool_name</span><span class=si>}</span><span class=s2>&#39; with these parameters:</span><span class=se>\n</span><span class=s2>&quot;</span>
</span><span id=__span-17-34><a id=__codelineno-17-34 name=__codelineno-17-34 href=#__codelineno-17-34></a>            <span class=n>approval_text</span> <span class=o>+=</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>indent</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span><span id=__span-17-35><a id=__codelineno-17-35 name=__codelineno-17-35 href=#__codelineno-17-35></a>
</span><span id=__span-17-36><a id=__codelineno-17-36 name=__codelineno-17-36 href=#__codelineno-17-36></a>            <span class=c1># Ask for user approval</span>
</span><span id=__span-17-37><a id=__codelineno-17-37 name=__codelineno-17-37 href=#__codelineno-17-37></a>            <span class=n>approved</span> <span class=o>=</span> <span class=k>await</span> <span class=bp>self</span><span class=o>.</span><span class=n>ui</span><span class=o>.</span><span class=n>ask_for_approval</span><span class=p>(</span><span class=n>approval_text</span><span class=p>)</span>
</span><span id=__span-17-38><a id=__codelineno-17-38 name=__codelineno-17-38 href=#__codelineno-17-38></a>
</span><span id=__span-17-39><a id=__codelineno-17-39 name=__codelineno-17-39 href=#__codelineno-17-39></a>            <span class=c1># Log the decision</span>
</span><span id=__span-17-40><a id=__codelineno-17-40 name=__codelineno-17-40 href=#__codelineno-17-40></a>            <span class=bp>self</span><span class=o>.</span><span class=n>audit_log</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span><span id=__span-17-41><a id=__codelineno-17-41 name=__codelineno-17-41 href=#__codelineno-17-41></a>                <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>(),</span>
</span><span id=__span-17-42><a id=__codelineno-17-42 name=__codelineno-17-42 href=#__codelineno-17-42></a>                <span class=s2>&quot;action&quot;</span><span class=p>:</span> <span class=s2>&quot;approval_decision&quot;</span><span class=p>,</span>
</span><span id=__span-17-43><a id=__codelineno-17-43 name=__codelineno-17-43 href=#__codelineno-17-43></a>                <span class=s2>&quot;tool&quot;</span><span class=p>:</span> <span class=n>tool_name</span><span class=p>,</span>
</span><span id=__span-17-44><a id=__codelineno-17-44 name=__codelineno-17-44 href=#__codelineno-17-44></a>                <span class=s2>&quot;approved&quot;</span><span class=p>:</span> <span class=n>approved</span>
</span><span id=__span-17-45><a id=__codelineno-17-45 name=__codelineno-17-45 href=#__codelineno-17-45></a>            <span class=p>})</span>
</span><span id=__span-17-46><a id=__codelineno-17-46 name=__codelineno-17-46 href=#__codelineno-17-46></a>
</span><span id=__span-17-47><a id=__codelineno-17-47 name=__codelineno-17-47 href=#__codelineno-17-47></a>            <span class=k>if</span> <span class=ow>not</span> <span class=n>approved</span><span class=p>:</span>
</span><span id=__span-17-48><a id=__codelineno-17-48 name=__codelineno-17-48 href=#__codelineno-17-48></a>                <span class=c1># Return a standard rejection response</span>
</span><span id=__span-17-49><a id=__codelineno-17-49 name=__codelineno-17-49 href=#__codelineno-17-49></a>                <span class=k>return</span> <span class=p>{</span>
</span><span id=__span-17-50><a id=__codelineno-17-50 name=__codelineno-17-50 href=#__codelineno-17-50></a>                    <span class=s2>&quot;error&quot;</span><span class=p>:</span> <span class=s2>&quot;User did not approve this operation&quot;</span>
</span><span id=__span-17-51><a id=__codelineno-17-51 name=__codelineno-17-51 href=#__codelineno-17-51></a>                <span class=p>}</span>
</span><span id=__span-17-52><a id=__codelineno-17-52 name=__codelineno-17-52 href=#__codelineno-17-52></a>
</span><span id=__span-17-53><a id=__codelineno-17-53 name=__codelineno-17-53 href=#__codelineno-17-53></a>        <span class=c1># If no approval needed or approval granted, proceed with the call</span>
</span><span id=__span-17-54><a id=__codelineno-17-54 name=__codelineno-17-54 href=#__codelineno-17-54></a>        <span class=k>return</span> <span class=k>await</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>call_tool</span><span class=p>(</span><span class=n>server_id</span><span class=p>,</span> <span class=n>tool_name</span><span class=p>,</span> <span class=n>params</span><span class=p>)</span>
</span></code></pre></div> <p>This example demonstrates how a client can wrap standard MCP methods with approval flows. Similar patterns can be applied to resource access and sampling requests.</p> <h3 id=enterprise-governance-patterns>Enterprise Governance Patterns<a class=headerlink href=#enterprise-governance-patterns title="Permanent link">&para;</a></h3> <p>For enterprise deployments, MCP enables several governance patterns:</p> <ol> <li> <p><strong>Role-Based Access Control (RBAC)</strong>: MCP servers can implement authentication and authorization, mapping user roles to allowed tools and resources.</p> </li> <li> <p><strong>Audit Logging</strong>: All MCP interactions can be logged for compliance and review.</p> </li> <li> <p><strong>Action Policies</strong>: Organizations can define policies about which tools require approval and from whom.</p> </li> <li> <p><strong>Sandboxing</strong>: MCP servers can be deployed in controlled environments with limited access to backend systems.</p> </li> <li> <p><strong>Rate Limiting</strong>: Servers can implement throttling to prevent abuse or unintended resource consumption.</p> </li> </ol> <p>These patterns can be implemented consistently across different MCP servers, creating a standardized governance framework for AI tool use.</p> <h1 id=performance-considerations-and-best-practices>Performance Considerations and Best Practices<a class=headerlink href=#performance-considerations-and-best-practices title="Permanent link">&para;</a></h1> <p>When implementing MCP in production systems, several performance considerations and best practices should be kept in mind:</p> <h2 id=latency-management>Latency Management<a class=headerlink href=#latency-management title="Permanent link">&para;</a></h2> <p>MCP introduces additional communication steps between the AI and external systems. To minimize latency:</p> <ol> <li><strong>Co-locate servers and clients</strong> when possible to reduce network overhead</li> <li><strong>Implement caching</strong> at the server level for frequently accessed resources</li> <li><strong>Use streaming responses</strong> for large data transfers rather than waiting for complete results</li> <li><strong>Balance chunking strategies</strong> - too many small requests can be inefficient, while too few large requests can block the UI</li> </ol> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=c1># Example of an MCP server with caching</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=kn>from</span><span class=w> </span><span class=nn>mcp.server.fastmcp</span><span class=w> </span><span class=kn>import</span> <span class=n>FastMCP</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a><span class=kn>import</span><span class=w> </span><span class=nn>functools</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a><span class=n>mcp</span> <span class=o>=</span> <span class=n>FastMCP</span><span class=p>(</span><span class=s2>&quot;Cached Server&quot;</span><span class=p>)</span>
</span><span id=__span-18-6><a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a>
</span><span id=__span-18-7><a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a><span class=c1># Simple in-memory cache for demonstration</span>
</span><span id=__span-18-8><a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a><span class=n>cache</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-18-9><a id=__codelineno-18-9 name=__codelineno-18-9 href=#__codelineno-18-9></a>
</span><span id=__span-18-10><a id=__codelineno-18-10 name=__codelineno-18-10 href=#__codelineno-18-10></a><span class=k>def</span><span class=w> </span><span class=nf>with_cache</span><span class=p>(</span><span class=n>ttl_seconds</span><span class=o>=</span><span class=mi>300</span><span class=p>):</span>
</span><span id=__span-18-11><a id=__codelineno-18-11 name=__codelineno-18-11 href=#__codelineno-18-11></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Decorator to cache function results.&quot;&quot;&quot;</span>
</span><span id=__span-18-12><a id=__codelineno-18-12 name=__codelineno-18-12 href=#__codelineno-18-12></a>    <span class=k>def</span><span class=w> </span><span class=nf>decorator</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span><span id=__span-18-13><a id=__codelineno-18-13 name=__codelineno-18-13 href=#__codelineno-18-13></a>        <span class=nd>@functools</span><span class=o>.</span><span class=n>wraps</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span><span id=__span-18-14><a id=__codelineno-18-14 name=__codelineno-18-14 href=#__codelineno-18-14></a>        <span class=k>def</span><span class=w> </span><span class=nf>wrapper</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span><span id=__span-18-15><a id=__codelineno-18-15 name=__codelineno-18-15 href=#__codelineno-18-15></a>            <span class=c1># Create a cache key from function name and arguments</span>
</span><span id=__span-18-16><a id=__codelineno-18-16 name=__codelineno-18-16 href=#__codelineno-18-16></a>            <span class=n>key</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>func</span><span class=o>.</span><span class=vm>__name__</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>args</span><span class=p>)</span><span class=si>}</span><span class=s2>:</span><span class=si>{</span><span class=nb>str</span><span class=p>(</span><span class=n>kwargs</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-18-17><a id=__codelineno-18-17 name=__codelineno-18-17 href=#__codelineno-18-17></a>
</span><span id=__span-18-18><a id=__codelineno-18-18 name=__codelineno-18-18 href=#__codelineno-18-18></a>            <span class=c1># Check if result is in cache and not expired</span>
</span><span id=__span-18-19><a id=__codelineno-18-19 name=__codelineno-18-19 href=#__codelineno-18-19></a>            <span class=k>if</span> <span class=n>key</span> <span class=ow>in</span> <span class=n>cache</span> <span class=ow>and</span> <span class=p>(</span><span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>][</span><span class=s1>&#39;time&#39;</span><span class=p>])</span> <span class=o>&lt;</span> <span class=n>ttl_seconds</span><span class=p>:</span>
</span><span id=__span-18-20><a id=__codelineno-18-20 name=__codelineno-18-20 href=#__codelineno-18-20></a>                <span class=k>return</span> <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>][</span><span class=s1>&#39;result&#39;</span><span class=p>]</span>
</span><span id=__span-18-21><a id=__codelineno-18-21 name=__codelineno-18-21 href=#__codelineno-18-21></a>
</span><span id=__span-18-22><a id=__codelineno-18-22 name=__codelineno-18-22 href=#__codelineno-18-22></a>            <span class=c1># Call the function and cache the result</span>
</span><span id=__span-18-23><a id=__codelineno-18-23 name=__codelineno-18-23 href=#__codelineno-18-23></a>            <span class=n>result</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span><span id=__span-18-24><a id=__codelineno-18-24 name=__codelineno-18-24 href=#__codelineno-18-24></a>            <span class=n>cache</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-18-25><a id=__codelineno-18-25 name=__codelineno-18-25 href=#__codelineno-18-25></a>                <span class=s1>&#39;result&#39;</span><span class=p>:</span> <span class=n>result</span><span class=p>,</span>
</span><span id=__span-18-26><a id=__codelineno-18-26 name=__codelineno-18-26 href=#__codelineno-18-26></a>                <span class=s1>&#39;time&#39;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span><span id=__span-18-27><a id=__codelineno-18-27 name=__codelineno-18-27 href=#__codelineno-18-27></a>            <span class=p>}</span>
</span><span id=__span-18-28><a id=__codelineno-18-28 name=__codelineno-18-28 href=#__codelineno-18-28></a>            <span class=k>return</span> <span class=n>result</span>
</span><span id=__span-18-29><a id=__codelineno-18-29 name=__codelineno-18-29 href=#__codelineno-18-29></a>        <span class=k>return</span> <span class=n>wrapper</span>
</span><span id=__span-18-30><a id=__codelineno-18-30 name=__codelineno-18-30 href=#__codelineno-18-30></a>    <span class=k>return</span> <span class=n>decorator</span>
</span><span id=__span-18-31><a id=__codelineno-18-31 name=__codelineno-18-31 href=#__codelineno-18-31></a>
</span><span id=__span-18-32><a id=__codelineno-18-32 name=__codelineno-18-32 href=#__codelineno-18-32></a><span class=nd>@mcp</span><span class=o>.</span><span class=n>tool</span><span class=p>()</span>
</span><span id=__span-18-33><a id=__codelineno-18-33 name=__codelineno-18-33 href=#__codelineno-18-33></a><span class=nd>@with_cache</span><span class=p>(</span><span class=n>ttl_seconds</span><span class=o>=</span><span class=mi>60</span><span class=p>)</span>
</span><span id=__span-18-34><a id=__codelineno-18-34 name=__codelineno-18-34 href=#__codelineno-18-34></a><span class=k>def</span><span class=w> </span><span class=nf>expensive_operation</span><span class=p>(</span><span class=n>param1</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span><span id=__span-18-35><a id=__codelineno-18-35 name=__codelineno-18-35 href=#__codelineno-18-35></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;A computationally expensive operation that benefits from caching.&quot;&quot;&quot;</span>
</span><span id=__span-18-36><a id=__codelineno-18-36 name=__codelineno-18-36 href=#__codelineno-18-36></a>    <span class=c1># Simulate expensive operation</span>
</span><span id=__span-18-37><a id=__codelineno-18-37 name=__codelineno-18-37 href=#__codelineno-18-37></a>    <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># In real code, this would be actual computation</span>
</span><span id=__span-18-38><a id=__codelineno-18-38 name=__codelineno-18-38 href=#__codelineno-18-38></a>    <span class=k>return</span> <span class=p>{</span><span class=s2>&quot;result&quot;</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&quot;Processed </span><span class=si>{</span><span class=n>param1</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>,</span> <span class=s2>&quot;timestamp&quot;</span><span class=p>:</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()}</span>
</span></code></pre></div> <h2 id=security-best-practices>Security Best Practices<a class=headerlink href=#security-best-practices title="Permanent link">&para;</a></h2> <ol> <li><strong>Input validation</strong>: Always validate inputs on the server side to prevent injection attacks</li> <li><strong>Least privilege</strong>: MCP servers should operate with minimal permissions needed</li> <li><strong>Token management</strong>: Securely handle authentication tokens and implement proper renewal</li> <li><strong>Transport security</strong>: Use TLS for HTTP-based transports</li> <li><strong>Rate limiting</strong>: Implement rate limits to prevent abuse</li> </ol> <h2 id=deployment-patterns>Deployment Patterns<a class=headerlink href=#deployment-patterns title="Permanent link">&para;</a></h2> <ol> <li><strong>Sidecar deployment</strong>: Run MCP servers as sidecars alongside existing services</li> <li><strong>Microservice architecture</strong>: Deploy each MCP server as a separate microservice</li> <li><strong>Function-as-a-Service</strong>: Deploy lightweight MCP servers as serverless functions</li> <li><strong>Proxy pattern</strong>: Use a central MCP proxy to manage multiple backend services</li> </ol> <h2 id=development-workflow>Development Workflow<a class=headerlink href=#development-workflow title="Permanent link">&para;</a></h2> <ol> <li><strong>Start with the SDK</strong>: Use Anthropic's MCP SDK for your language to minimize boilerplate</li> <li><strong>Test with the Inspector</strong>: Use the MCP Inspector tool to manually test your server</li> <li><strong>Write integration tests</strong>: Test your MCP server with automated client requests</li> <li><strong>Document capabilities</strong>: Create clear documentation of your server's tools and resources</li> <li><strong>Semantic versioning</strong>: Follow semver when releasing updates to your MCP servers</li> </ol> <h1 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h1> <p>The Model Context Protocol (MCP) is a significant step forward in AI integration. It provides a robust, extensible framework for connecting LLMs to the world around them – to the data and tools that ground their responses in reality. By standardizing how context is provided to models, MCP frees developers from re-inventing the wheel for each integration, and it empowers AI systems with a richer, live model of their environment.</p> <p>We've seen how MCP compares to earlier approaches (offering broader scope, persistent state, and interoperability), learned about its core components (resources, tools, prompts, etc.), and built a simple Python MCP server step-by-step. We also explored advanced patterns like agent frameworks and retrieval augmentation, where MCP serves as the connective tissue for complex AI applications.</p> <p>As of 2025, MCP is still evolving (recent updates added streaming transports, OAuth security, and more), but it's rapidly gaining adoption and community support. Anthropic maintains an open-source repository of MCP SDKs (Python, TypeScript, Java, Kotlin, C#, Rust) and dozens of ready-to-use servers.</p> <p>The vision is that eventually, AI assistants will use MCP to maintain context seamlessly as they hop between tools and data sources, much like a person using a computer – making them far more useful and reliable. By learning and leveraging MCP now, developers can be at the forefront of building context-aware, tool-empowered AI agents that work across platforms. Whether you integrate it into a chatbot, an IDE, or an autonomous agent, MCP provides the foundation to connect AI with the real world in a standardized, powerful way.</p> <h2 id=resources-and-next-steps>Resources and Next Steps<a class=headerlink href=#resources-and-next-steps title="Permanent link">&para;</a></h2> <p>If you'd like to start working with MCP:</p> <ol> <li><strong>Official Documentation</strong>: Visit the Model Context Protocol website for specifications and guides</li> <li><strong>GitHub Repositories</strong>: Check out Anthropic's MCP SDKs and reference implementations</li> <li><strong>Community Servers</strong>: Explore the growing ecosystem of community-contributed MCP servers</li> <li><strong>Join the Discussion</strong>: Participate in the MCP working group to shape the future of the protocol</li> </ol> <p>Sources: The official Anthropic announcement and documentation were used to define MCP and its architecture. InfoQ and VentureBeat articles provided context on MCP's goals and industry adoption. The MCP specification and guides were referenced for technical details on primitives and message flows. Code examples were adapted from Anthropic's open-source Python SDK and quickstart tutorials. These resources, alongside community commentary, offer a comprehensive view of MCP as the emerging standard for AI context integration.</p> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../28/a-visual-guide-to-llm-agents/ class="md-footer__link md-footer__link--prev" aria-label="Previous: A Visual Guide to LLM Agents"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> A Visual Guide to LLM Agents </div> </div> </a> <a href=../model-context-protocol-mcp---a-quick-intro/ class="md-footer__link md-footer__link--next" aria-label="Next: Model Context Protocol (MCP) - A Quick Intro"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Model Context Protocol (MCP) - A Quick Intro </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 PromptX AI </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/PromtEngineer/localGPT target=_blank rel=noopener title="localGPT on GitHub" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=feed_rss_created.xml target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg> </a> <a href=https://twitter.com/engineerrprompt target=_blank rel=noopener title=twitter.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </a> <a href=https://github.com/PromtEngineer target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://www.linkedin.com/in/engineerprompt target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> <a href=https://www.youtube.com/@engineerprompt target=_blank rel=noopener title=www.youtube.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../../../javascripts/mermaid-zoom.js></script> <script src=../../../../../javascripts/mathjax.js></script> <script src=../../../../../javascripts/analytics.js></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>